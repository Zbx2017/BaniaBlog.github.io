<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Pisces","version":"7.7.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="A platform for discussing programming and technology">
<meta property="og:type" content="website">
<meta property="og:title" content="BaniaBlog">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="BaniaBlog">
<meta property="og:description" content="A platform for discussing programming and technology">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Bania">
<meta property="article:tag" content="Python">
<meta property="article:tag" content=" Java">
<meta property="article:tag" content=" Machine Learning">
<meta property="article:tag" content=" Data Science">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false
  };
</script>

  <title>BaniaBlog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="BaniaBlog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">BaniaBlog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">Sharing Technology</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/Zbx2017/Zbx2017.github.io" class="github-corner" title="Bania GitHub" aria-label="Bania GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/24/Two-Layer-Network/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Bania">
      <meta itemprop="description" content="A platform for discussing programming and technology">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BaniaBlog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/24/Two-Layer-Network/" class="post-title-link" itemprop="url">Two-Layer Network</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-02-24 09:49:23 / Modified: 11:35:36" itemprop="dateCreated datePublished" datetime="2020-02-24T09:49:23+08:00">2020-02-24</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>In assignment1, we have implemented a simple example of neural networks(Two-Layer Network), but the problem is that the implementation is not very modular since the loss and gradient are computed in a single monolithic function. That means if we want to create a complex network, we may find it difficult to implement. So the first part of the assignment2 question1 is to make the implementation more modular based on Two-Layer Network.</p>
<p>For each layer we will implement a <code>forward</code> and a <code>backward</code> function. The <code>forward</code> function will receive inputs, weights, and other parameters and will return both an output and a <code>cache</code> object storing data needed for the backward pass, like this:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layer_forward</span><span class="params">(x, w)</span>:</span></span><br><span class="line">  <span class="string">""" Receive inputs x and weights w """</span></span><br><span class="line">  <span class="comment"># Do some computations ...</span></span><br><span class="line">  z = <span class="comment"># ... some intermediate value</span></span><br><span class="line">  <span class="comment"># Do some more computations ...</span></span><br><span class="line">  out = <span class="comment"># the output</span></span><br><span class="line"></span><br><span class="line">  cache = (x, w, z, out) <span class="comment"># Values we need to compute gradients</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure>
<p>The backward pass will receive upstream derivatives and the <code>cache</code> object, and will return gradients with respect to the inputs and weights, like this:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layer_backward</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  Receive derivative of loss with respect to outputs and cache,</span></span><br><span class="line"><span class="string">  and compute derivative with respect to inputs.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="comment"># Unpack cache values</span></span><br><span class="line">  x, w, z, out = cache</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Use values in cache to compute derivatives</span></span><br><span class="line">  dx = <span class="comment"># Derivative of loss with respect to x</span></span><br><span class="line">  dw = <span class="comment"># Derivative of loss with respect to w</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> dx, dw</span><br></pre></td></tr></table></figure>
<p>The implementation will be based on the structure of  Networks. In this case, we have <code>affine----&gt;relu----&gt;affine</code> network, so it is wise to create methods for each layers and activation function. </p>
<p>Open layers.py file and implement the methods below:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">affine_forward</span><span class="params">(x, w, b)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the forward pass for an affine (fully-connected) layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N</span></span><br><span class="line"><span class="string">    examples, where each example x[i] has shape (d_1, ..., d_k). We will</span></span><br><span class="line"><span class="string">    reshape each input into a vector of dimension D = d_1 * ... * d_k, and</span></span><br><span class="line"><span class="string">    then transform it to an output vector of dimension M.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)</span></span><br><span class="line"><span class="string">    - w: A numpy array of weights, of shape (D, M)</span></span><br><span class="line"><span class="string">    - b: A numpy array of biases, of shape (M,)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: output, of shape (N, M)</span></span><br><span class="line"><span class="string">    - cache: (x, w, b)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    out = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the affine forward pass. Store the result in out. You   #</span></span><br><span class="line">    <span class="comment"># will need to reshape the input into rows.                               #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    out = np.reshape(x,(x.shape[<span class="number">0</span>], <span class="number">-1</span>)).dot(w)+b</span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    cache = (x, w, b)</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">affine_backward</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the backward pass for an affine layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivative, of shape (N, M)</span></span><br><span class="line"><span class="string">    - cache: Tuple of:</span></span><br><span class="line"><span class="string">      - x: Input data, of shape (N, d_1, ... d_k)</span></span><br><span class="line"><span class="string">      - w: Weights, of shape (D, M)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)</span></span><br><span class="line"><span class="string">    - dw: Gradient with respect to w, of shape (D, M)</span></span><br><span class="line"><span class="string">    - db: Gradient with respect to b, of shape (M,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    x, w, b = cache</span><br><span class="line">    dx, dw, db = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the affine backward pass.                               #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    dx = np.reshape(dout.dot(w.T),x.shape)</span><br><span class="line">    dw = np.reshape(x,(x.shape[<span class="number">0</span>], <span class="number">-1</span>)).T.dot(dout)</span><br><span class="line">    db = np.sum(dout, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="keyword">return</span> dx, dw, db</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu_forward</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the forward pass for a layer of rectified linear units (ReLUs).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">    - x: Inputs, of any shape</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: Output, of the same shape as x</span></span><br><span class="line"><span class="string">    - cache: x</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    out = <span class="literal">None</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the ReLU forward pass.                                  #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    out = np.maximum(<span class="number">0</span>, x)</span><br><span class="line">    </span><br><span class="line">   </span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #         ###########################################################################</span></span><br><span class="line">    cache = x</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu_backward</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the backward pass for a layer of rectified linear units (ReLUs).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivatives, of any shape</span></span><br><span class="line"><span class="string">    - cache: Input x, of same shape as dout</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - dx: Gradient with respect to x</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dx, x = <span class="literal">None</span>, cache</span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the ReLU backward pass.                                 #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    dout[x&lt;=<span class="number">0</span>]=<span class="number">0</span></span><br><span class="line">    dx = dout </span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure>
<p>Note that we have<code>affine---&gt;relu</code> module, we can combine affine and relu module together to create <code>affine_relu_forward/backward</code>, which will make the implementation easier. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">affine_relu_forward</span><span class="params">(x, w, b)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Convenience layer that perorms an affine transform followed by a ReLU</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: Input to the affine layer</span></span><br><span class="line"><span class="string">    - w, b: Weights for the affine layer</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: Output from the ReLU</span></span><br><span class="line"><span class="string">    - cache: Object to give to the backward pass</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    a, fc_cache = affine_forward(x, w, b)</span><br><span class="line">    out, relu_cache = relu_forward(a)</span><br><span class="line">    cache = (fc_cache, relu_cache)</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">affine_relu_backward</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Backward pass for the affine-relu convenience layer</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    fc_cache, relu_cache = cache</span><br><span class="line">    da = relu_backward(dout, relu_cache)</span><br><span class="line">    dx, dw, db = affine_backward(da, fc_cache)</span><br><span class="line">    <span class="keyword">return</span> dx, dw, db</span><br></pre></td></tr></table></figure>
<p>Then we can test all the methods we have created in FullyConnectedNets.ipynb according to the error. </p>
<p><img src="/2020/02/24/Two-Layer-Network/1582513523961.png" alt="1582513523961"></p>
<p><img src="/2020/02/24/Two-Layer-Network/1582513543672.png" alt="1582513543672"></p>
<p><img src="/2020/02/24/Two-Layer-Network/1582513558613.png" alt="1582513558613"></p>
<p><img src="/2020/02/24/Two-Layer-Network/1582513570779.png" alt="1582513570779"></p>
<p><img src="/2020/02/24/Two-Layer-Network/1582513594658.png" alt="1582513594658"></p>
<p>All the methods have achieve the requirements so that we can use these functions to make our implementation easy to read and modify. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> builtins <span class="keyword">import</span> range</span><br><span class="line"><span class="keyword">from</span> builtins <span class="keyword">import</span> object</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> cs231n.layers <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> cs231n.layer_utils <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TwoLayerNet</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A two-layer fully-connected neural network with ReLU nonlinearity and</span></span><br><span class="line"><span class="string">    softmax loss that uses a modular layer design. We assume an input dimension</span></span><br><span class="line"><span class="string">    of D, a hidden dimension of H, and perform classification over C classes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The architecure should be affine - relu - affine - softmax.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Note that this class does not implement gradient descent; instead, it</span></span><br><span class="line"><span class="string">    will interact with a separate Solver object that is responsible for running</span></span><br><span class="line"><span class="string">    optimization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The learnable parameters of the model are stored in the dictionary</span></span><br><span class="line"><span class="string">    self.params that maps parameter names to numpy arrays.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim=<span class="number">3</span>*<span class="number">32</span>*<span class="number">32</span>, hidden_dim=<span class="number">100</span>, num_classes=<span class="number">10</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 weight_scale=<span class="number">1e-3</span>, reg=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Initialize a new network.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Inputs:</span></span><br><span class="line"><span class="string">        - input_dim: An integer giving the size of the input</span></span><br><span class="line"><span class="string">        - hidden_dim: An integer giving the size of the hidden layer</span></span><br><span class="line"><span class="string">        - num_classes: An integer giving the number of classes to classify</span></span><br><span class="line"><span class="string">        - dropout: Scalar between 0 and 1 giving dropout strength.</span></span><br><span class="line"><span class="string">        - weight_scale: Scalar giving the standard deviation for random</span></span><br><span class="line"><span class="string">          initialization of the weights.</span></span><br><span class="line"><span class="string">        - reg: Scalar giving L2 regularization strength.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.params = &#123;&#125;</span><br><span class="line">        self.reg = reg</span><br><span class="line"></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Initialize the weights and biases of the two-layer net. Weights    #</span></span><br><span class="line">        <span class="comment"># should be initialized from a Gaussian with standard deviation equal to   #</span></span><br><span class="line">        <span class="comment"># weight_scale, and biases should be initialized to zero. All weights and  #</span></span><br><span class="line">        <span class="comment"># biases should be stored in the dictionary self.params, with first layer  #</span></span><br><span class="line">        <span class="comment"># weights and biases using the keys 'W1' and 'b1' and second layer weights #</span></span><br><span class="line">        <span class="comment"># and biases using the keys 'W2' and 'b2'.                                 #</span></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        self.params[<span class="string">'W1'</span>] = weight_scale*np.random.randn(input_dim, hidden_dim)</span><br><span class="line">        self.params[<span class="string">'b1'</span>] = np.zeros(hidden_dim)</span><br><span class="line">        self.params[<span class="string">'W2'</span>] = weight_scale*np.random.randn(hidden_dim, num_classes)</span><br><span class="line">        self.params[<span class="string">'b2'</span>] = np.zeros(num_classes)</span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        <span class="comment">#                             END OF YOUR CODE                             #</span></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Compute loss and gradient for a minibatch of data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Inputs:</span></span><br><span class="line"><span class="string">        - X: Array of input data of shape (N, d_1, ..., d_k)</span></span><br><span class="line"><span class="string">        - y: Array of labels, of shape (N,). y[i] gives the label for X[i].</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">        If y is None, then run a test-time forward pass of the model and return:</span></span><br><span class="line"><span class="string">        - scores: Array of shape (N, C) giving classification scores, where</span></span><br><span class="line"><span class="string">          scores[i, c] is the classification score for X[i] and class c.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        If y is not None, then run a training-time forward and backward pass and</span></span><br><span class="line"><span class="string">        return a tuple of:</span></span><br><span class="line"><span class="string">        - loss: Scalar value giving the loss</span></span><br><span class="line"><span class="string">        - grads: Dictionary with the same keys as self.params, mapping parameter</span></span><br><span class="line"><span class="string">          names to gradients of the loss with respect to those parameters.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        scores = <span class="literal">None</span></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Implement the forward pass for the two-layer net, computing the    #</span></span><br><span class="line">        <span class="comment"># class scores for X and storing them in the scores variable.              #</span></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        W1, b1 = self.params[<span class="string">'W1'</span>], self.params[<span class="string">'b1'</span>]</span><br><span class="line">        W2, b2 = self.params[<span class="string">'W2'</span>], self.params[<span class="string">'b2'</span>]</span><br><span class="line">        N, D = np.reshape(X,(X.shape[<span class="number">0</span>],<span class="number">-1</span>)).shape</span><br><span class="line">        scores,cache1 = affine_relu_forward(X, W1, b1)</span><br><span class="line">        scores,cache2 = affine_forward(scores, W2, b2)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        <span class="comment">#                             END OF YOUR CODE                             #</span></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># If y is None then we are in test mode so just return scores</span></span><br><span class="line">        <span class="keyword">if</span> y <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> scores</span><br><span class="line"></span><br><span class="line">        loss, grads = <span class="number">0</span>, &#123;&#125;</span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Implement the backward pass for the two-layer net. Store the loss  #</span></span><br><span class="line">        <span class="comment"># in the loss variable and gradients in the grads dictionary. Compute data #</span></span><br><span class="line">        <span class="comment"># loss using softmax, and make sure that grads[k] holds the gradients for  #</span></span><br><span class="line">        <span class="comment"># self.params[k]. Don't forget to add L2 regularization!                   #</span></span><br><span class="line">        <span class="comment">#                                                                          #</span></span><br><span class="line">        <span class="comment"># <span class="doctag">NOTE:</span> To ensure that your implementation matches ours and you pass the   #</span></span><br><span class="line">        <span class="comment"># automated tests, make sure that your L2 regularization includes a factor #</span></span><br><span class="line">        <span class="comment"># of 0.5 to simplify the expression for the gradient.                      #</span></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        scores -= np.max(scores, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        pro_scores = np.exp(scores)/ np.sum(np.exp(scores), axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        loss = -np.sum(np.log(pro_scores[np.arange(N),y]))</span><br><span class="line">        </span><br><span class="line">        loss /= N</span><br><span class="line">        loss += <span class="number">0.5</span>*self.reg*np.sum(W1*W1)+ <span class="number">0.5</span>*self.reg*np.sum(W2*W2)</span><br><span class="line">        </span><br><span class="line">        pro_scores[np.arange(N), y] -= <span class="number">1</span></span><br><span class="line">        pro_scores /= N</span><br><span class="line">        </span><br><span class="line">        dhidden, dW2, db2 = affine_backward(pro_scores, cache2)</span><br><span class="line">        _, dW1, db1 = affine_relu_backward(dhidden, cache1)</span><br><span class="line">        </span><br><span class="line">        dW1 += self.reg*W1</span><br><span class="line">        dW2 += self.reg*W2</span><br><span class="line">        </span><br><span class="line">        grads = &#123;<span class="string">'W1'</span>:dW1, <span class="string">'b1'</span>:db1, <span class="string">'W2'</span>:dW2, <span class="string">'b2'</span>:db2&#125;</span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        <span class="comment">#                             END OF YOUR CODE                             #</span></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss, grads</span><br></pre></td></tr></table></figure>
<p>We just use two lines to obtain the scores and the loss, which has greatly simplified the programming.  To see whether we implement the methods correctly we can use the code below to test the error of each parameter. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">231</span>)</span><br><span class="line">N, D, H, C = <span class="number">3</span>, <span class="number">5</span>, <span class="number">50</span>, <span class="number">7</span></span><br><span class="line">X = np.random.randn(N, D)</span><br><span class="line">y = np.random.randint(C, size=N)</span><br><span class="line"></span><br><span class="line">std = <span class="number">1e-3</span></span><br><span class="line">model = TwoLayerNet(input_dim=D, hidden_dim=H, num_classes=C, weight_scale=std)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Testing initialization ... '</span>)</span><br><span class="line">W1_std = abs(model.params[<span class="string">'W1'</span>].std() - std)</span><br><span class="line">b1 = model.params[<span class="string">'b1'</span>]</span><br><span class="line">W2_std = abs(model.params[<span class="string">'W2'</span>].std() - std)</span><br><span class="line">b2 = model.params[<span class="string">'b2'</span>]</span><br><span class="line"><span class="keyword">assert</span> W1_std &lt; std / <span class="number">10</span>, <span class="string">'First layer weights do not seem right'</span></span><br><span class="line"><span class="keyword">assert</span> np.all(b1 == <span class="number">0</span>), <span class="string">'First layer biases do not seem right'</span></span><br><span class="line"><span class="keyword">assert</span> W2_std &lt; std / <span class="number">10</span>, <span class="string">'Second layer weights do not seem right'</span></span><br><span class="line"><span class="keyword">assert</span> np.all(b2 == <span class="number">0</span>), <span class="string">'Second layer biases do not seem right'</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'Testing test-time forward pass ... '</span>)</span><br><span class="line">model.params[<span class="string">'W1'</span>] = np.linspace(<span class="number">-0.7</span>, <span class="number">0.3</span>, num=D*H).reshape(D, H)</span><br><span class="line">model.params[<span class="string">'b1'</span>] = np.linspace(<span class="number">-0.1</span>, <span class="number">0.9</span>, num=H)</span><br><span class="line">model.params[<span class="string">'W2'</span>] = np.linspace(<span class="number">-0.3</span>, <span class="number">0.4</span>, num=H*C).reshape(H, C)</span><br><span class="line">model.params[<span class="string">'b2'</span>] = np.linspace(<span class="number">-0.9</span>, <span class="number">0.1</span>, num=C)</span><br><span class="line">X = np.linspace(<span class="number">-5.5</span>, <span class="number">4.5</span>, num=N*D).reshape(D, N).T</span><br><span class="line">scores = model.loss(X)</span><br><span class="line">correct_scores = np.asarray(</span><br><span class="line">  [[<span class="number">11.53165108</span>,  <span class="number">12.2917344</span>,   <span class="number">13.05181771</span>,  <span class="number">13.81190102</span>,  <span class="number">14.57198434</span>, <span class="number">15.33206765</span>,  <span class="number">16.09215096</span>],</span><br><span class="line">   [<span class="number">12.05769098</span>,  <span class="number">12.74614105</span>,  <span class="number">13.43459113</span>,  <span class="number">14.1230412</span>,   <span class="number">14.81149128</span>, <span class="number">15.49994135</span>,  <span class="number">16.18839143</span>],</span><br><span class="line">   [<span class="number">12.58373087</span>,  <span class="number">13.20054771</span>,  <span class="number">13.81736455</span>,  <span class="number">14.43418138</span>,  <span class="number">15.05099822</span>, <span class="number">15.66781506</span>,  <span class="number">16.2846319</span> ]])</span><br><span class="line">scores_diff = np.abs(scores - correct_scores).sum()</span><br><span class="line"><span class="keyword">assert</span> scores_diff &lt; <span class="number">1e-6</span>, <span class="string">'Problem with test-time forward pass'</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'Testing training loss (no regularization)'</span>)</span><br><span class="line">y = np.asarray([<span class="number">0</span>, <span class="number">5</span>, <span class="number">1</span>])</span><br><span class="line">loss, grads = model.loss(X, y)</span><br><span class="line">correct_loss = <span class="number">3.4702243556</span></span><br><span class="line"><span class="keyword">assert</span> abs(loss - correct_loss) &lt; <span class="number">1e-10</span>, <span class="string">'Problem with training-time loss'</span></span><br><span class="line"></span><br><span class="line">model.reg = <span class="number">1.0</span></span><br><span class="line">loss, grads = model.loss(X, y)</span><br><span class="line">correct_loss = <span class="number">26.5948426952</span></span><br><span class="line"><span class="keyword">assert</span> abs(loss - correct_loss) &lt; <span class="number">1e-10</span>, <span class="string">'Problem with regularization loss'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> reg <span class="keyword">in</span> [<span class="number">0.0</span>, <span class="number">0.7</span>]:</span><br><span class="line">  print(<span class="string">'Running numeric gradient check with reg = '</span>, reg)</span><br><span class="line">  model.reg = reg</span><br><span class="line">  loss, grads = model.loss(X, y)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> name <span class="keyword">in</span> sorted(grads):</span><br><span class="line">    f = <span class="keyword">lambda</span> _: model.loss(X, y)[<span class="number">0</span>]</span><br><span class="line">    grad_num = eval_numerical_gradient(f, model.params[name], verbose=<span class="literal">False</span>)</span><br><span class="line">    print(<span class="string">'%s relative error: %.2e'</span> % (name, rel_error(grad_num, grads[name])))</span><br></pre></td></tr></table></figure>
<p>The error of each parameter should be less than 1e-7.</p>
<p><img src="/2020/02/24/Two-Layer-Network/1582514136392.png" alt="1582514136392"></p>
<p>Finally, we use this Two-Layer Network on the data with <code>Slover</code> which is a modular class that fits the model into it and can train the model, predict the final output etc.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">model = TwoLayerNet(input_dim=<span class="number">3</span>*<span class="number">32</span>*<span class="number">32</span>, hidden_dim=<span class="number">100</span>, num_classes=<span class="number">10</span>,</span><br><span class="line">                 weight_scale=<span class="number">1e-3</span>, reg=<span class="number">0.1</span>)</span><br><span class="line">solver = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##############################################################################</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Use a Solver instance to train a TwoLayerNet that achieves at least  #</span></span><br><span class="line"><span class="comment"># 50% accuracy on the validation set.                                        #</span></span><br><span class="line"><span class="comment">##############################################################################</span></span><br><span class="line">data2 = &#123;</span><br><span class="line">      <span class="string">'X_train'</span>: data[<span class="string">'X_train'</span>],</span><br><span class="line">      <span class="string">'y_train'</span>: data[<span class="string">'y_train'</span>],</span><br><span class="line">      <span class="string">'X_val'</span>: data[<span class="string">'X_val'</span>],</span><br><span class="line">      <span class="string">'y_val'</span>: data[<span class="string">'y_val'</span>],</span><br><span class="line">    &#125;</span><br><span class="line">solver = Solver(model, data2,</span><br><span class="line">                    update_rule=<span class="string">'sgd'</span>,</span><br><span class="line">                    optim_config=&#123;</span><br><span class="line">                      <span class="string">'learning_rate'</span>: <span class="number">1e-3</span>,</span><br><span class="line">                    &#125;,</span><br><span class="line">                    lr_decay=<span class="number">0.95</span>,</span><br><span class="line">                    num_epochs=<span class="number">10</span>, batch_size=<span class="number">100</span>,</span><br><span class="line">                    print_every=<span class="number">100</span>)</span><br><span class="line">solver.train()</span><br><span class="line"><span class="comment">##############################################################################</span></span><br><span class="line"><span class="comment">#                             END OF YOUR CODE                               #</span></span><br><span class="line"><span class="comment">##############################################################################</span></span><br></pre></td></tr></table></figure>
<p>Then, draw the accuracy graph and loss graph to see how they change with the epoch.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run this cell to visualize training loss and train / val accuracy</span></span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">'Training loss'</span>)</span><br><span class="line">plt.plot(solver.loss_history, <span class="string">'o'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Iteration'</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">'Accuracy'</span>)</span><br><span class="line">plt.plot(solver.train_acc_history, <span class="string">'-o'</span>, label=<span class="string">'train'</span>)</span><br><span class="line">plt.plot(solver.val_acc_history, <span class="string">'-o'</span>, label=<span class="string">'val'</span>)</span><br><span class="line">plt.plot([<span class="number">0.5</span>] * len(solver.val_acc_history), <span class="string">'k--'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epoch'</span>)</span><br><span class="line">plt.legend(loc=<span class="string">'lower right'</span>)</span><br><span class="line">plt.gcf().set_size_inches(<span class="number">15</span>, <span class="number">12</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/02/24/Two-Layer-Network/1582514639365.png" alt="1582514639365"></p>
<p><img src="/2020/02/24/Two-Layer-Network/1582514661060.png" alt="1582514661060"></p>
<p>This part of exercise is prepared for the Multilayer Networks in which we have to deal with more complex networks, but the idea is the same that modular programming like above can make it more achievable. </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/24/Fully-Connected-Networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Bania">
      <meta itemprop="description" content="A platform for discussing programming and technology">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BaniaBlog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/24/Fully-Connected-Networks/" class="post-title-link" itemprop="url">Fully-Connected Networks</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-02-24 09:40:42" itemprop="dateCreated datePublished" datetime="2020-02-24T09:40:42+08:00">2020-02-24</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/18/Neural-Net/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Bania">
      <meta itemprop="description" content="A platform for discussing programming and technology">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BaniaBlog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/18/Neural-Net/" class="post-title-link" itemprop="url">Neural Net</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-02-18 08:41:58" itemprop="dateCreated datePublished" datetime="2020-02-18T08:41:58+08:00">2020-02-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-02-24 12:06:19" itemprop="dateModified" datetime="2020-02-24T12:06:19+08:00">2020-02-24</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="Problem-Overview"><a href="#Problem-Overview" class="headerlink" title="Problem Overview"></a>Problem Overview</h4><p>The last classifier in assignment1 is Neural Network. We need to build a two-layers network from scratch, which means we know the function for each layer. But in many cases, if we deal with a complex problem and need a complex model, it is very hard to know exactly the function for each layer and this can make the model unexplainable. That is why sometimes we may prefer to use machine learning algorithms to deal with problems. </p>
<p>Compared to previous classifier, I find it more difficult to understand and the calculation is a little bit more complex. In this exercise, using your pen to do the math on a paper may be clear and comprehensible before you start to write the code. </p>
<p>At the end, you can see that neural network performs well in the classification and has a high validation accuracy. The problem with it, however, is the efficiency because of using two layers linear function and one hidden layer(using ReLU). </p>
<h4 id="Neural-Network"><a href="#Neural-Network" class="headerlink" title="Neural Network"></a>Neural Network</h4><ol>
<li>Model Architectures </li>
</ol>
<p>When you start to write the code, you should keep on mind of the structure of the model. We use two layers &amp; one hidden layer network, so that you can easily know what exactly the structure is. Just like that:</p>
<p><img src="/2020/02/18/Neural-Net/1581992206941.png" alt="1581992206941"> </p>
<h4 id><a href="#" class="headerlink" title=" "></a> </h4><p>For input layer, we have </p>
<script type="math/tex; mode=display">
f_{in}=W_1X+b_1</script><p>and output layer :</p>
<script type="math/tex; mode=display">
f_{out}=W_2X+b_2</script><p>For hidden layer, we use ReLU as activation function.</p>
<script type="math/tex; mode=display">
Re(x)=max(0,x)</script><p>There are also many activation functions, you can choose them to test the result, but ReLU is suitable to most problems. </p>
<p><img src="/2020/02/18/Neural-Net/1581992588211.png" alt="1581992588211"></p>
<ol>
<li>Computational Graph</li>
</ol>
<p>Notice that we can easily get the loss of the networks by calculating the scores first and then use softmax loss to get the loss. Gradient calculation, on the contrary, is more difficult to obtain. If you calculate it directly from input to output, it will be complicated and easy to make mistakes. If the model is very complex , it is unrealistic to do in that way. Thus, we choose to use computational graph &amp; Backpropagation. Let me give you a simple example to help you understand it. </p>
<p>Now, we have a function like that and we want to get the derivative of each parameters. </p>
<p><img src="/2020/02/18/Neural-Net/1581995492794.png" alt="1581995492794"> </p>
<p>Based on the computational graph, we can clearly see the relationships between each parameters, and we can use chain rule to write the derivative of each parameters. </p>
<script type="math/tex; mode=display">
\frac{\partial f}{\partial x} = \frac{\partial f}{\partial q}\frac{\partial q}{\partial x}</script><script type="math/tex; mode=display">
\frac{\partial f}{\partial y} = \frac{\partial f}{\partial q}\frac{\partial q}{\partial y}</script><p><img src="/2020/02/18/Neural-Net/1581996049609.png" alt="1581996049609"></p>
<p><img src="/2020/02/18/Neural-Net/1581996072635.png" alt="1581996072635"></p>
<p>We know basically how it works and now we can get our computational graph based on the model. </p>
<p><img src="/2020/02/18/Neural-Net/1581998109205.png" alt="1581998109205"></p>
<ol>
<li>Backpropagation</li>
</ol>
<p>After we get the computational graph, we can use backpropagation to get the gradients. Because we use softmax loss, we can obtain the gradient of $\frac{\partial L}{\partial y}​$​, then we can use the chain rule to get all the gradients. </p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial W_2} = \frac{\partial L}{\partial y}\frac{\partial y}{\partial W_2} = y_1^{T}\frac{\partial L}{\partial y} \\
\frac{\partial L}{\partial b_2} = \frac{\partial L}{\partial y}\frac{\partial y}{\partial b_2} = \bold{1}_{(1\times N)}\frac{\partial L}{\partial y} \\
\frac{\partial L}{\partial y_1} = \frac{\partial L}{\partial y}\frac{\partial y}{\partial y_1} =\frac{\partial L}{\partial y}W_2^T\\
\frac{\partial y_1}{\partial y_2} = \begin{cases}
1& y_{2_{ij}} >0\\
0& otherwise
\end{cases} \\
\frac{\partial L}{\partial y_2}=\frac{\partial L}{\partial y_1}\frac{\partial y_1}{\partial y_2}=\frac{\partial L}{\partial y_1}, (\frac{\partial L}{\partial y_1}[y_1<=0] = 0)\\
\frac{\partial L}{\partial W_1} = \frac{\partial L}{\partial y_2}\frac{\partial y_2}{\partial W_1} = x^{T}\frac{\partial L}{\partial y_2} \\
\frac{\partial L}{\partial b_1} = \frac{\partial L}{\partial y_2}\frac{\partial y_2}{\partial b_2} = \bold{1}_{(1\times C)}\frac{\partial L}{\partial y_2} \\</script><p>Based on these equations, we can write the code easily without any big mistakes. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> past.builtins <span class="keyword">import</span> xrange</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TwoLayerNet</span><span class="params">(object)</span>:</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">  A two-layer fully-connected neural network. The net has an input dimension of</span></span><br><span class="line"><span class="string">  N, a hidden layer dimension of H, and performs classification over C classes.</span></span><br><span class="line"><span class="string">  We train the network with a softmax loss function and L2 regularization on the</span></span><br><span class="line"><span class="string">  weight matrices. The network uses a ReLU nonlinearity after the first fully</span></span><br><span class="line"><span class="string">  connected layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  In other words, the network has the following architecture:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  input - fully connected layer - ReLU - fully connected layer - softmax</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  The outputs of the second fully-connected layer are the scores for each class.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, output_size, std=<span class="number">1e-4</span>)</span>:</span></span><br><span class="line">              </span><br><span class="line">        </span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">    Initialize the model. Weights are initialized to small random values and</span></span><br><span class="line"><span class="string">    biases are initialized to zero. Weights and biases are stored in the</span></span><br><span class="line"><span class="string">    variable self.params, which is a dictionary with the following keys:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    W1: First layer weights; has shape (D, H)</span></span><br><span class="line"><span class="string">    b1: First layer biases; has shape (H,)</span></span><br><span class="line"><span class="string">    W2: Second layer weights; has shape (H, C)</span></span><br><span class="line"><span class="string">    b2: Second layer biases; has shape (C,)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - input_size: The dimension D of the input data.</span></span><br><span class="line"><span class="string">    - hidden_size: The number of neurons H in the hidden layer.</span></span><br><span class="line"><span class="string">    - output_size: The number of classes C.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.params = &#123;&#125;</span><br><span class="line">        self.params[<span class="string">'W1'</span>] = std * np.random.randn(input_size, hidden_size)</span><br><span class="line">        self.params[<span class="string">'b1'</span>] = np.zeros(hidden_size)</span><br><span class="line">        self.params[<span class="string">'W2'</span>] = std * np.random.randn(hidden_size, output_size)</span><br><span class="line">        self.params[<span class="string">'b2'</span>] = np.zeros(output_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(self, X, y=None, reg=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the loss and gradients for a two layer fully connected neural</span></span><br><span class="line"><span class="string">    network.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - X: Input data of shape (N, D). Each X[i] is a training sample.</span></span><br><span class="line"><span class="string">    - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is</span></span><br><span class="line"><span class="string">      an integer in the range 0 &lt;= y[i] &lt; C. This parameter is optional; if it</span></span><br><span class="line"><span class="string">      is not passed then we only return scores, and if it is passed then we</span></span><br><span class="line"><span class="string">      instead return the loss and gradients.</span></span><br><span class="line"><span class="string">    - reg: Regularization strength.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    If y is None, return a matrix scores of shape (N, C) where scores[i, c] is</span></span><br><span class="line"><span class="string">    the score for class c on input X[i].</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    If y is not None, instead return a tuple of:</span></span><br><span class="line"><span class="string">    - loss: Loss (data loss and regularization loss) for this batch of training</span></span><br><span class="line"><span class="string">      samples.</span></span><br><span class="line"><span class="string">    - grads: Dictionary mapping parameter names to gradients of those parameters</span></span><br><span class="line"><span class="string">      with respect to the loss function; has the same keys as self.params.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">    <span class="comment"># Unpack variables from the params dictionary</span></span><br><span class="line">        W1, b1 = self.params[<span class="string">'W1'</span>], self.params[<span class="string">'b1'</span>]</span><br><span class="line">        W2, b2 = self.params[<span class="string">'W2'</span>], self.params[<span class="string">'b2'</span>]</span><br><span class="line">        N, D = X.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the forward pass</span></span><br><span class="line">        scores = <span class="literal">None</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Perform the forward pass, computing the class scores for the input. #</span></span><br><span class="line">    <span class="comment"># Store the result in the scores variable, which should be an array of      #</span></span><br><span class="line">    <span class="comment"># shape (N, C).                                                             #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">        score_layer1 = X.dot(W1) + b1</span><br><span class="line">        score_ReLU =np.maximum(score_layer1,<span class="number">0</span>)</span><br><span class="line">        scores = score_ReLU.dot(W2) + b2</span><br><span class="line"></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment">#                              END OF YOUR CODE                             #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># If the targets are not given then jump out, we're done</span></span><br><span class="line">        <span class="keyword">if</span> y <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> scores</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the loss</span></span><br><span class="line">        loss = <span class="literal">None</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Finish the forward pass, and compute the loss. This should include  #</span></span><br><span class="line">    <span class="comment"># both the data loss and L2 regularization for W1 and W2. Store the result  #</span></span><br><span class="line">    <span class="comment"># in the variable loss, which should be a scalar. Use the Softmax           #</span></span><br><span class="line">    <span class="comment"># classifier loss.                                                          #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">        </span><br><span class="line">        scores -= np.max(scores, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        pro_scores = np.exp(scores)/ np.sum(np.exp(scores), axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        loss = -np.sum(np.log(pro_scores[np.arange(N),y]))</span><br><span class="line">        </span><br><span class="line">        loss /= N</span><br><span class="line">        loss += reg*np.sum(W1*W1)+ reg*np.sum(W2*W2)</span><br><span class="line">        </span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment">#                              END OF YOUR CODE                             #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass: compute gradients</span></span><br><span class="line">        grads = &#123;&#125;</span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Compute the backward pass, computing the derivatives of the weights #</span></span><br><span class="line">    <span class="comment"># and biases. Store the results in the grads dictionary. For example,       #</span></span><br><span class="line">    <span class="comment"># grads['W1'] should store the gradient on W1, and be a matrix of same size #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">        pro_scores[np.arange(N), y] -= <span class="number">1</span></span><br><span class="line">        pro_scores /= N</span><br><span class="line">        </span><br><span class="line">        dW2 = score_ReLU.T.dot(pro_scores)</span><br><span class="line">        db2 = np.sum(pro_scores, axis=<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        dscore_ReLU = pro_scores.dot(W2.T)</span><br><span class="line">        dscore_ReLU[score_ReLU &lt;= <span class="number">0</span> ] = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        dW1 = X.T.dot(dscore_ReLU)</span><br><span class="line">        db1 = np.sum(dscore_ReLU, axis=<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        dW1 += <span class="number">2</span>*reg*W1</span><br><span class="line">        dW2 += <span class="number">2</span>*reg*W2</span><br><span class="line">        </span><br><span class="line">        grads = &#123;<span class="string">'W1'</span>:dW1, <span class="string">'b1'</span>:db1, <span class="string">'W2'</span>:dW2, <span class="string">'b2'</span>:db2&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment">#                              END OF YOUR CODE                             #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss, grads</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, X, y, X_val, y_val,</span></span></span><br><span class="line"><span class="function"><span class="params">            learning_rate=<span class="number">1e-3</span>, learning_rate_decay=<span class="number">0.95</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">            reg=<span class="number">5e-6</span>, num_iters=<span class="number">100</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">            batch_size=<span class="number">200</span>, verbose=False)</span>:</span></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">    Train this neural network using stochastic gradient descent.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - X: A numpy array of shape (N, D) giving training data.</span></span><br><span class="line"><span class="string">    - y: A numpy array f shape (N,) giving training labels; y[i] = c means that</span></span><br><span class="line"><span class="string">      X[i] has label c, where 0 &lt;= c &lt; C.</span></span><br><span class="line"><span class="string">    - X_val: A numpy array of shape (N_val, D) giving validation data.</span></span><br><span class="line"><span class="string">    - y_val: A numpy array of shape (N_val,) giving validation labels.</span></span><br><span class="line"><span class="string">    - learning_rate: Scalar giving learning rate for optimization.</span></span><br><span class="line"><span class="string">    - learning_rate_decay: Scalar giving factor used to decay the learning rate</span></span><br><span class="line"><span class="string">      after each epoch.</span></span><br><span class="line"><span class="string">    - reg: Scalar giving regularization strength.</span></span><br><span class="line"><span class="string">    - num_iters: Number of steps to take when optimizing.</span></span><br><span class="line"><span class="string">    - batch_size: Number of training examples to use per step.</span></span><br><span class="line"><span class="string">    - verbose: boolean; if true print progress during optimization.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        num_train = X.shape[<span class="number">0</span>]</span><br><span class="line">        iterations_per_epoch = max(num_train / batch_size, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Use SGD to optimize the parameters in self.model</span></span><br><span class="line">        loss_history = []</span><br><span class="line">        train_acc_history = []</span><br><span class="line">        val_acc_history = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> it <span class="keyword">in</span> xrange(num_iters):</span><br><span class="line">            </span><br><span class="line">            X_batch = <span class="literal">None</span></span><br><span class="line">            y_batch = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">      <span class="comment">#########################################################################</span></span><br><span class="line">      <span class="comment"># <span class="doctag">TODO:</span> Create a random minibatch of training data and labels, storing  #</span></span><br><span class="line">      <span class="comment"># them in X_batch and y_batch respectively.                             #</span></span><br><span class="line">      <span class="comment">#########################################################################</span></span><br><span class="line">            choice_index = np.random.choice(num_train, batch_size)</span><br><span class="line">            X_batch = X[choice_index]</span><br><span class="line">            y_batch = y[choice_index]</span><br><span class="line">      </span><br><span class="line">      <span class="comment">#########################################################################</span></span><br><span class="line">      <span class="comment">#                             END OF YOUR CODE                          #</span></span><br><span class="line">      <span class="comment">#########################################################################</span></span><br><span class="line"></span><br><span class="line">      <span class="comment"># Compute loss and gradients using the current minibatch</span></span><br><span class="line">            loss, grads = self.loss(X_batch, y=y_batch, reg=reg)</span><br><span class="line">            loss_history.append(loss)</span><br><span class="line"></span><br><span class="line">      <span class="comment">#########################################################################</span></span><br><span class="line">      <span class="comment"># <span class="doctag">TODO:</span> Use the gradients in the grads dictionary to update the         #</span></span><br><span class="line">      <span class="comment"># parameters of the network (stored in the dictionary self.params)      #</span></span><br><span class="line">      <span class="comment"># using stochastic gradient descent. You'll need to use the gradients   #</span></span><br><span class="line">      <span class="comment"># stored in the grads dictionary defined above.                         #</span></span><br><span class="line">      <span class="comment">#########################################################################</span></span><br><span class="line">            <span class="keyword">for</span> key <span class="keyword">in</span> self.params:</span><br><span class="line">                </span><br><span class="line">            </span><br><span class="line">                self.params[key] -= learning_rate * grads[key]</span><br><span class="line">      </span><br><span class="line">      <span class="comment">#########################################################################</span></span><br><span class="line">      <span class="comment">#                             END OF YOUR CODE                          #</span></span><br><span class="line">      <span class="comment">#########################################################################</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> verbose <span class="keyword">and</span> it % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                </span><br><span class="line">                print(<span class="string">'iteration %d / %d: loss %f'</span> % (it, num_iters, loss))</span><br><span class="line"></span><br><span class="line">      <span class="comment"># Every epoch, check train and val accuracy and decay learning rate.</span></span><br><span class="line">            <span class="keyword">if</span> it % iterations_per_epoch == <span class="number">0</span>:</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># Check accuracy</span></span><br><span class="line">                train_acc = (self.predict(X_batch) == y_batch).mean()</span><br><span class="line">                val_acc = (self.predict(X_val) == y_val).mean()</span><br><span class="line">                train_acc_history.append(train_acc)</span><br><span class="line">                val_acc_history.append(val_acc)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Decay learning rate</span></span><br><span class="line">                learning_rate *= learning_rate_decay</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">      <span class="string">'loss_history'</span>: loss_history,</span><br><span class="line">      <span class="string">'train_acc_history'</span>: train_acc_history,</span><br><span class="line">      <span class="string">'val_acc_history'</span>: val_acc_history,</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        </span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">    Use the trained weights of this two-layer network to predict labels for</span></span><br><span class="line"><span class="string">    data points. For each data point we predict scores for each of the C</span></span><br><span class="line"><span class="string">    classes, and assign each data point to the class with the highest score.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - X: A numpy array of shape (N, D) giving N D-dimensional data points to</span></span><br><span class="line"><span class="string">      classify.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - y_pred: A numpy array of shape (N,) giving predicted labels for each of</span></span><br><span class="line"><span class="string">      the elements of X. For all i, y_pred[i] = c means that X[i] is predicted</span></span><br><span class="line"><span class="string">       to have class c, where 0 &lt;= c &lt; C.</span></span><br><span class="line"><span class="string">       """</span></span><br><span class="line">        y_pred = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement this function; it should be VERY simple!                #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">        y_pred = np.argmax(self.loss(X), axis=<span class="number">1</span>)</span><br><span class="line">  </span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                              END OF YOUR CODE                           #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br></pre></td></tr></table></figure>
<h4 id="Checking"><a href="#Checking" class="headerlink" title="Checking"></a>Checking</h4><p>Now you can go to two_layer_net.ipynb to train your model, but before it you should check the algorithm you create to see whether it is correct. We use a simple dataset to check. </p>
<p><img src="/2020/02/18/Neural-Net/1582003562296.png" alt="1582003562296"></p>
<p><img src="/2020/02/18/Neural-Net/1582003592110.png" alt="1582003592110"></p>
<p><img src="/2020/02/18/Neural-Net/1582003605931.png" alt="1582003605931"></p>
<p><img src="/2020/02/18/Neural-Net/1582003620532.png" alt="1582003620532"></p>
<p><img src="/2020/02/18/Neural-Net/1582003637573.png" alt="1582003637573"></p>
<p>We can see that the loss and gradients we calculate using the algorithm we write have reach the requirements, so that we can go to the next step : training our model.</p>
<h4 id="Training-the-Model"><a href="#Training-the-Model" class="headerlink" title="Training the Model"></a>Training the Model</h4><p>First, we just set those parameters randomly and you can see that the validation accuracy is very low. </p>
<p><img src="/2020/02/18/Neural-Net/1582003790638.png" alt="1582003790638"></p>
<p><img src="/2020/02/18/Neural-Net/1582003850830.png" alt="1582003850830"></p>
<p><img src="/2020/02/18/Neural-Net/1582003872346.png" alt="1582003872346"></p>
<p><strong>What’s wrong?</strong>. Looking at the visualizations above, we see that the loss is decreasing more or less linearly, which seems to suggest that the learning rate may be too low. Moreover, there is no gap between the training and validation accuracy, suggesting that the model we used has low capacity, and that we should increase its size. On the other hand, with a very large model we would expect to see more overfitting, which would manifest itself as a very large gap between the training and validation accuracy.</p>
<p> <strong>Tuning</strong>. Tuning the hyperparameters and developing intuition for how they affect the final performance is a large part of using Neural Networks, so we want  to get a lot of practice. Below,we should experiment with different values of the various hyperparameters, including hidden layer size, learning rate, number of training epochs, and regularization strength. You might also consider tuning the learning rate decay, but you should be able to get good performance using the default value.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">best_net = <span class="literal">None</span> <span class="comment"># store the best model into this </span></span><br><span class="line"></span><br><span class="line"><span class="comment">#################################################################################</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Tune hyperparameters using the validation set. Store your best trained  #</span></span><br><span class="line"><span class="comment"># model in best_net.                                                            #</span></span><br><span class="line"><span class="comment">#                                                                               #</span></span><br><span class="line"><span class="comment"># To help debug your network, it may help to use visualizations similar to the  #</span></span><br><span class="line"><span class="comment"># ones we used above; these visualizations will have significant qualitative    #</span></span><br><span class="line"><span class="comment"># differences from the ones we saw above for the poorly tuned network.          #</span></span><br><span class="line"><span class="comment">#                                                                               #</span></span><br><span class="line"><span class="comment"># Tweaking hyperparameters by hand can be fun, but you might find it useful to  #</span></span><br><span class="line"><span class="comment"># write code to sweep through possible combinations of hyperparameters          #</span></span><br><span class="line"><span class="comment"># automatically like we did on the previous exercises.                          #</span></span><br><span class="line"><span class="comment">#################################################################################</span></span><br><span class="line">learning_rate = [<span class="number">0.9e-3</span>,<span class="number">1e-3</span>,<span class="number">1.1e-3</span>]</span><br><span class="line">hlsize = [<span class="number">80</span>,<span class="number">100</span>,<span class="number">120</span>, <span class="number">150</span>,<span class="number">180</span>]</span><br><span class="line">reg_strengths = [<span class="number">0.1</span>, <span class="number">0.15</span>,<span class="number">0.05</span>]</span><br><span class="line">learning_rate_decay = []</span><br><span class="line">best_val = <span class="number">-1</span></span><br><span class="line">results = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> h_i <span class="keyword">in</span> hlsize:</span><br><span class="line">    <span class="keyword">for</span> lr_i <span class="keyword">in</span> learning_rate:</span><br><span class="line">        <span class="keyword">for</span> rs_i <span class="keyword">in</span> reg_strengths:</span><br><span class="line">            net = TwoLayerNet(input_size, h_i, num_classes)</span><br><span class="line">            net.train(X_train, y_train, X_val, y_val, num_iters=<span class="number">2000</span>, batch_size=<span class="number">200</span>,</span><br><span class="line">                learning_rate=lr_i, learning_rate_decay=<span class="number">0.95</span>,</span><br><span class="line">                reg=rs_i, verbose=<span class="literal">True</span>)</span><br><span class="line">            y_train_pred = net.predict(X_train)</span><br><span class="line">            y_val_pred = net.predict(X_val)</span><br><span class="line">            y_train_acc = np.mean(y_train == y_train_pred)</span><br><span class="line">            y_val_acc = np.mean(y_val == y_val_pred)</span><br><span class="line">            results[(lr_i, rs_i)] = y_train_acc , y_val_acc</span><br><span class="line">            <span class="keyword">if</span> y_val_acc &gt; best_val:</span><br><span class="line">                best_val = y_val_acc</span><br><span class="line">                best_net = net</span><br><span class="line"></span><br><span class="line"><span class="comment">#################################################################################</span></span><br><span class="line"><span class="comment">#                               END OF YOUR CODE                                #</span></span><br><span class="line"><span class="comment">#################################################################################</span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/02/18/Neural-Net/1582003979640.png" alt="1582003979640"></p>
<p>Finally, we get the 0.528 test accuracy. Consequently, the parameters of neural network are very crucial for its performance. Getting the best parameters can take a long time for you should change them every time based on the last performance. When you get the best or near best parameters, the neural network can greatly improve the accuracy.</p>
<p><img src="/2020/02/18/Neural-Net/1582004046620.png" alt="1582004046620"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/16/Softmax/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Bania">
      <meta itemprop="description" content="A platform for discussing programming and technology">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BaniaBlog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/16/Softmax/" class="post-title-link" itemprop="url">Softmax</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-02-16 10:43:59 / Modified: 17:08:55" itemprop="dateCreated datePublished" datetime="2020-02-16T10:43:59+08:00">2020-02-16</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="Problem-Overview"><a href="#Problem-Overview" class="headerlink" title="Problem Overview"></a>Problem Overview</h4><p>In this part, we will try to understand the idea of Softmax classifier and use this algorithm for the picture classification and see the difference between Softmax and SVM. To be more specific, we will solve the problems below:</p>
<ul>
<li>implement a fully-vectorized <strong>loss function</strong> for the Softmax classifier</li>
<li>implement the fully-vectorized expression for its <strong>analytic gradient</strong></li>
<li><strong>check your implementation</strong> with numerical gradient</li>
<li>use a validation set to <strong>tune the learning rate and regularization</strong> strength</li>
<li><strong>optimize</strong> the loss function with <strong>SGD</strong></li>
<li><strong>visualize</strong> the final learned weights</li>
</ul>
<p>Actually, this exercise is quit analogous to the SVM exercise. The most different is the loss function of Softmax. Now, we are going to have a clear idea of Softmax first and then realize the algorithm with the code. </p>
<h4 id="Softmax-Explaintion"><a href="#Softmax-Explaintion" class="headerlink" title="Softmax Explaintion"></a>Softmax Explaintion</h4><p>In SVM, we use hinge loss as its loss function, which treats scores for each class. Now, for Softmax, we not only have to get the scores for each class, but also need to normalize them and finally obtain the probabilities of each class. The loss function we use here is cross-entropy loss:</p>
<p><img src="/2020/02/16/Softmax/1581826848602.png" alt="1581826848602"></p>
<p><img src="/2020/02/16/Softmax/1581826764671.png" alt="1581826764671"></p>
<p>To be honest, the calculation of cross-entropy loss is easier than hinge loss. There are no other limited  conditions and it is more straightforward. But the only thing you should pay attention to is the numerical stability. Notice that the scores for each class may be huge numbers and the exp(f) can be very large so that dividing large numbers can be unstable. When I first did this problem I did not take care of this issue and the loss I got is very large and is not closed to -log(0.1). Then I changed my algorithm by adding a constant C to eliminate the instability.  </p>
<p><img src="/2020/02/16/Softmax/1581827688934.png" alt="1581827688934"></p>
<p>A common choice for C is to set log C = -max(f). This will simply states that we should shift the value inside the vector f so that the highest value is zero, so that the exp(f) will not be very large. </p>
<p>The most troublesome problem is the calculation of dW. I have gone through many troubles because of unfamiliarity with matrix derivative.  Using the chain rule and flexible matrix derivate will simplify the calculation, which makes it easy to code. I have searched other people’s method for this problem and I clearly know how it works. </p>
<p><img src="/2020/02/16/Softmax/1581828245723.png" alt="1581828245723"></p>
<p><img src="/2020/02/16/Softmax/1581828320014.png" alt="1581828320014"></p>
<p><img src="/2020/02/16/Softmax/1581828939035.png" alt="1581828939035"></p>
<p><img src="/2020/02/16/Softmax/1581828963231.png" alt="1581828963231"></p>
<p>Then we can easily obtain dW using this method. Here is the code:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> shuffle</span><br><span class="line"><span class="keyword">from</span> past.builtins <span class="keyword">import</span> xrange</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_naive</span><span class="params">(W, X, y, reg)</span>:</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">  Softmax loss function, naive implementation (with loops)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Inputs have dimension D, there are C classes, and we operate on minibatches</span></span><br><span class="line"><span class="string">  of N examples.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Inputs:</span></span><br><span class="line"><span class="string">  - W: A numpy array of shape (D, C) containing weights.</span></span><br><span class="line"><span class="string">  - X: A numpy array of shape (N, D) containing a minibatch of data.</span></span><br><span class="line"><span class="string">  - y: A numpy array of shape (N,) containing training labels; y[i] = c means</span></span><br><span class="line"><span class="string">    that X[i] has label c, where 0 &lt;= c &lt; C.</span></span><br><span class="line"><span class="string">  - reg: (float) regularization strength</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Returns a tuple of:</span></span><br><span class="line"><span class="string">  - loss as single float</span></span><br><span class="line"><span class="string">  - gradient with respect to weights W; an array of same shape as W</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">  <span class="comment"># Initialize the loss and gradient to zero.</span></span><br><span class="line">    </span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    dW = np.zeros_like(W)</span><br><span class="line"></span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line">  <span class="comment"># <span class="doctag">TODO:</span> Compute the softmax loss and its gradient using explicit loops.     #</span></span><br><span class="line">  <span class="comment"># Store the loss in loss and the gradient in dW. If you are not careful     #</span></span><br><span class="line">  <span class="comment"># here, it is easy to run into numeric instability. Don't forget the        #</span></span><br><span class="line">  <span class="comment"># regularization!                                                           #</span></span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line">    num_classes = W.shape[<span class="number">1</span>]</span><br><span class="line">    num_train = X.shape[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(num_train):</span><br><span class="line">        </span><br><span class="line">        scores = X[i].dot(W)</span><br><span class="line">        scores -= np.max(scores)</span><br><span class="line">        correct_score = scores[y[i]]</span><br><span class="line">        sum_scores = np.sum(np.exp(scores))</span><br><span class="line">        loss += np.log(sum_scores) - correct_score</span><br><span class="line">        dW[:,y[i]] -= X[i].T </span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> xrange(num_classes):</span><br><span class="line">            </span><br><span class="line">            dW[:,j] += (np.exp(scores[j])/sum_scores * X[i]).T</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    loss /= num_train</span><br><span class="line">    loss += reg*np.sum(W*W)</span><br><span class="line">    dW /= num_train</span><br><span class="line">    dW += <span class="number">2</span>*reg*W</span><br><span class="line">            </span><br><span class="line">  </span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line">  <span class="comment">#                          END OF YOUR CODE                                 #</span></span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, dW</span><br><span class="line">    </span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">softmax_loss_vectorized</span><span class="params">(W, X, y, reg)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">  Softmax loss function, vectorized version.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Inputs and outputs are the same as softmax_loss_naive.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">  <span class="comment"># Initialize the loss and gradient to zero.</span></span><br><span class="line">    </span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    dW = np.zeros_like(W)</span><br><span class="line"></span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line">  <span class="comment"># <span class="doctag">TODO:</span> Compute the softmax loss and its gradient using no explicit loops.  #</span></span><br><span class="line">  <span class="comment"># Store the loss in loss and the gradient in dW. If you are not careful     #</span></span><br><span class="line">  <span class="comment"># here, it is easy to run into numeric instability. Don't forget the        #</span></span><br><span class="line">  <span class="comment"># regularization!                                                           #</span></span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line">    num_train = X.shape[<span class="number">0</span>]</span><br><span class="line">    scores = X.dot(W)</span><br><span class="line">    scores -= np.max(scores, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    correct_scores = scores[np.arange(num_train),y]</span><br><span class="line">    pro_scores = np.exp(scores)/ np.sum(np.exp(scores), axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">    loss += -np.sum(np.log(pro_scores[np.arange(num_train),y]))</span><br><span class="line">    </span><br><span class="line">    pro_scores[np.arange(num_train),y] += <span class="number">-1</span> </span><br><span class="line">    dW += X.T.dot(pro_scores)</span><br><span class="line">    </span><br><span class="line">    loss /= num_train</span><br><span class="line">    dW /= num_train</span><br><span class="line">    </span><br><span class="line">    loss += reg*np.sum(W*W)</span><br><span class="line">    dW += <span class="number">2</span>*reg*W</span><br><span class="line">    </span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line">  <span class="comment">#                          END OF YOUR CODE                                 #</span></span><br><span class="line">  <span class="comment">#############################################################################</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, dW</span><br></pre></td></tr></table></figure>
<h4 id="Check-You-Code"><a href="#Check-You-Code" class="headerlink" title="Check You Code"></a>Check You Code</h4><p>Just like the SVM exercise, you can use the method provided to check your code. Before it you have to load the data. As it is the same with SVM, I am not going to show it here and turn to the checking process directly.</p>
<p>Check the loss: the loss you calculate should be close to 2.302585, if not check you algorithm. </p>
<p><img src="/2020/02/16/Softmax/1581829212177.png" alt="1581829212177"></p>
<p>Check the gradient: numerical gradient and analytical gradient should be very close. </p>
<p><img src="/2020/02/16/Softmax/1581829307591.png" alt="1581829307591"></p>
<p>Then you can see the running time of using naive method and vetorized method. </p>
<p><img src="/2020/02/16/Softmax/1581829383085.png" alt="1581829383085"></p>
<h4 id="Cross-Validation-Getting-the-Best-Model"><a href="#Cross-Validation-Getting-the-Best-Model" class="headerlink" title="Cross Validation: Getting the Best Model"></a>Cross Validation: Getting the Best Model</h4><p>Finally, if all processes have been done successfully, which means the algorithm you write is correctly, then you can go to next step: training your model by using cross validation. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Use the validation set to tune hyperparameters (regularization strength and</span></span><br><span class="line"><span class="comment"># learning rate). You should experiment with different ranges for the learning</span></span><br><span class="line"><span class="comment"># rates and regularization strengths; if you are careful you should be able to</span></span><br><span class="line"><span class="comment"># get a classification accuracy of over 0.35 on the validation set.</span></span><br><span class="line"><span class="keyword">from</span> cs231n.classifiers <span class="keyword">import</span> Softmax</span><br><span class="line">results = &#123;&#125;</span><br><span class="line">best_val = <span class="number">-1</span></span><br><span class="line">best_softmax = <span class="literal">None</span></span><br><span class="line">learning_rates = [<span class="number">1e-7</span>, <span class="number">1.2e-7</span>,<span class="number">1.4e-7</span>,<span class="number">1.6e-7</span>,<span class="number">1.8e-7</span>,<span class="number">2.0e-7</span>]</span><br><span class="line">regularization_strengths = [<span class="number">2.5e4</span>, <span class="number">2.7e4</span>,<span class="number">2.9e4</span>,<span class="number">3.1e4</span>,<span class="number">3.3e4</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span>                                                                        #</span></span><br><span class="line"><span class="comment"># Use the validation set to set the learning rate and regularization strength. #</span></span><br><span class="line"><span class="comment"># This should be identical to the validation that you did for the SVM; save    #</span></span><br><span class="line"><span class="comment"># the best trained softmax classifer in best_softmax.                          #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="keyword">for</span> lr_i <span class="keyword">in</span> learning_rates:</span><br><span class="line">    <span class="keyword">for</span> rs_i <span class="keyword">in</span> regularization_strengths:</span><br><span class="line">        softmax = Softmax()</span><br><span class="line">        softmax.train(X_train, y_train, learning_rate=lr_i, reg=rs_i,num_iters=<span class="number">500</span>, verbose=<span class="literal">True</span>)</span><br><span class="line">        y_train_pred = softmax.predict(X_train)</span><br><span class="line">        y_val_pred = softmax.predict(X_val)</span><br><span class="line">        y_train_acc = np.mean(y_train == y_train_pred)</span><br><span class="line">        y_val_acc = np.mean(y_val == y_val_pred)</span><br><span class="line">        results[(lr_i, rs_i)] = y_train_acc , y_val_acc</span><br><span class="line">        <span class="keyword">if</span> y_val_acc &gt; best_val:</span><br><span class="line">            best_val = y_val_acc</span><br><span class="line">            best_softmax =softmax</span><br><span class="line"></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment">#                              END OF YOUR CODE                                #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># Print out results.</span></span><br><span class="line"><span class="keyword">for</span> lr, reg <span class="keyword">in</span> sorted(results):</span><br><span class="line">    train_accuracy, val_accuracy = results[(lr, reg)]</span><br><span class="line">    print(<span class="string">'lr %e reg %e train accuracy: %f val accuracy: %f'</span> % (</span><br><span class="line">                lr, reg, train_accuracy, val_accuracy))</span><br><span class="line">    </span><br><span class="line">print(<span class="string">'best validation accuracy achieved during cross-validation: %f'</span> % best_val)</span><br></pre></td></tr></table></figure>
<p>Then use the best model to predict the testing data. </p>
<p><img src="/2020/02/16/Softmax/1581829600085.png" alt="1581829600085"></p>
<p><img src="/2020/02/16/Softmax/1581829619119.png" alt="1581829619119"></p>
<h4 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h4><p>As we can see, using softmax performs not as well as SVM in this problem. The distinction between them can be shown by a graph below:</p>
<p><img src="/2020/02/16/Softmax/1581829896775.png" alt="1581829896775"></p>
<p>The Softmax classifier is never fully happy with the scores it produces: the correct class could always have a higher probability and the incorrect classes always a lower probability and the loss would always get better. However, the SVM is happy once the margins are satisfied and it does not micromanage the exact scores beyond this constraint.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/14/SVM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Bania">
      <meta itemprop="description" content="A platform for discussing programming and technology">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BaniaBlog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/14/SVM/" class="post-title-link" itemprop="url">SVM</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-02-14 10:39:36 / Modified: 12:55:56" itemprop="dateCreated datePublished" datetime="2020-02-14T10:39:36+08:00">2020-02-14</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="Problem-Overview"><a href="#Problem-Overview" class="headerlink" title="Problem Overview"></a>Problem Overview</h4><p>In assignment1, the second problem is using SVM to classify the pictures. To be more specific, we have to finish questions below:</p>
<ul>
<li>implement a fully-vectorized <strong>loss function</strong> for the SVM</li>
<li>implement the fully-vectorized expression for its <strong>analytic gradient</strong></li>
<li><strong>check your implementation</strong> using numerical gradient</li>
<li>use a validation set to <strong>tune the learning rate and regularization</strong> strength</li>
<li><strong>optimize</strong> the loss function with <strong>SGD</strong></li>
<li><strong>visualize</strong> the final learned weights</li>
</ul>
<p>In this part, the most important is the SVM algorithm. Note that there are up to 10 categories of the pictures, it is unrealistic to use two-classes SVM. In two-classes SVM, we need a line with maximum margin to separate two classes. The basic model is like that: </p>
<p><img src="/2020/02/14/SVM/12.png" alt="12"></p>
<p><img src="/2020/02/14/SVM/1581650269438.png" alt="1581650269438"> </p>
<p>For multiclass classification, we obviously need a different algorithm for SVM. If you have watched the course online, you basically know that Multiclass SVM uses Hinge loss for loss function. Next, I will illustrate it in details and give the code.</p>
<h4 id="Multiclass-Support-Vector-Machine"><a href="#Multiclass-Support-Vector-Machine" class="headerlink" title="Multiclass Support Vector Machine"></a>Multiclass Support Vector Machine</h4><p>Similar to two-classes SVM, the main idea of Multiclass SVM is also to find a hyperplane to separate different categories. Unlike the two-classes SVM(the hyperplane is a line), the Multiclass SVM has a more than two dimensional hyperplane. Thus, the function of the hyperplane is:</p>
<p><img src="/2020/02/14/SVM/1581651442942.png" alt="1581651442942"></p>
<p>where xi and W is more than two dimensional. That is why SVM is also a linear classifier. </p>
<p>The loss function that Multiclass SVM uses is hinge loss. </p>
<p><img src="/2020/02/14/SVM/1581651848316.png" alt="1581651848316"></p>
<p>where Δ is a fixed margin, and it is helpful to decide which is correctly classified. Because we want the score of correctly classified data is greater than those incorrectly classified. In this problem we set Δ to 1.  </p>
<p><img src="/2020/02/14/SVM/1581651765041.png" alt="1581651765041"></p>
<p>To avoid overfitting, we add a regularization loss to the loss function. Then the ultimate loss function is :</p>
<p><img src="/2020/02/14/SVM/1581652469289.png" alt="1581652469289"></p>
<p>For this loss function, we use gradient descent to find the W with the least loss, so that we can obtain the hyperplane. The code of the algorithm is below. The most difficult part for me is the calculation of dW, which needs the knowledge of matrix derivative. For more detail information, you can click the link: <a href="https://en.wikipedia.org/wiki/Matrix_calculus" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Matrix_calculus</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> shuffle</span><br><span class="line"><span class="keyword">from</span> past.builtins <span class="keyword">import</span> xrange</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">svm_loss_naive</span><span class="params">(W, X, y, reg)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Structured SVM loss function, naive implementation (with loops).</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    Inputs have dimension D, there are C classes, and we operate on minibatches</span></span><br><span class="line"><span class="string">    of N examples.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - W: A numpy array of shape (D, C) containing weights.</span></span><br><span class="line"><span class="string">    - X: A numpy array of shape (N, D) containing a minibatch of data.</span></span><br><span class="line"><span class="string">    - y: A numpy array of shape (N,) containing training labels; y[i] = c means</span></span><br><span class="line"><span class="string">      that X[i] has label c, where 0 &lt;= c &lt; C.</span></span><br><span class="line"><span class="string">    - reg: (float) regularization strength</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - loss as single float</span></span><br><span class="line"><span class="string">    - gradient with respect to weights W; an array of same shape as W</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    dW = np.zeros(W.shape)  <span class="comment"># initialize the gradient as zero</span></span><br><span class="line">    <span class="comment"># compute the loss and the gradient</span></span><br><span class="line">    num_classes = W.shape[<span class="number">1</span>]</span><br><span class="line">    num_train = X.shape[<span class="number">0</span>]</span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(num_train):</span><br><span class="line">        scores = X[i].dot(W)</span><br><span class="line">        correct_class_score = scores[y[i]]</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> xrange(num_classes):</span><br><span class="line">            <span class="keyword">if</span> j == y[i]:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            margin = scores[j] - correct_class_score + <span class="number">1</span>  <span class="comment"># note delta = 1</span></span><br><span class="line">            <span class="keyword">if</span> margin &gt; <span class="number">0</span>:</span><br><span class="line">                loss += margin</span><br><span class="line">                dW[:, y[i]] += -X[i].T</span><br><span class="line">                dW[:, j] += X[i].T</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Right now the loss is a sum over all training examples, but we want it</span></span><br><span class="line">    <span class="comment"># to be an average instead so we divide by num_train.</span></span><br><span class="line">    loss /= num_train</span><br><span class="line">    dW /= num_train</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add regularization to the loss.</span></span><br><span class="line">    loss += reg * np.sum(W * W)</span><br><span class="line">    dW += <span class="number">2</span>*reg * W</span><br><span class="line"></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span>                                                                     #</span></span><br><span class="line">    <span class="comment"># Compute the gradient of the loss function and store it dW.                #</span></span><br><span class="line">    <span class="comment"># Rather that first computing the loss and then computing the derivative,   #</span></span><br><span class="line">    <span class="comment"># it may be simpler to compute the derivative at the same time that the     #</span></span><br><span class="line">    <span class="comment"># loss is being computed. As a result you may need to modify some of the    #</span></span><br><span class="line">    <span class="comment"># code above to compute the gradient.                                       #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, dW</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">svm_loss_vectorized</span><span class="params">(W, X, y, reg)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Structured SVM loss function, vectorized implementation.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    Inputs and outputs are the same as svm_loss_naive.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    dW = np.zeros(W.shape)  <span class="comment"># initialize the gradient as zero</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span>                                                                     #</span></span><br><span class="line">    <span class="comment"># Implement a vectorized version of the structured SVM loss, storing the    #</span></span><br><span class="line">    <span class="comment"># result in loss.                                                           #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    scores = X.dot(W)</span><br><span class="line">    num_train = X.shape[<span class="number">0</span>]</span><br><span class="line">    margins = np.maximum(<span class="number">0</span>, (scores - np.reshape(scores[np.arange(num_train), y],(num_train,<span class="number">-1</span>)) + <span class="number">1</span>))</span><br><span class="line">    margins[np.arange(num_train), y] = <span class="number">0</span></span><br><span class="line">    loss += np.sum(margins) / num_train</span><br><span class="line">    loss += reg * np.sum(W * W)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                              #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span>                                                                     #</span></span><br><span class="line">    <span class="comment"># Implement a vectorized version of the gradient for the structured SVM     #</span></span><br><span class="line">    <span class="comment"># loss, storing the result in dW.                                           #</span></span><br><span class="line">    <span class="comment">#                                                                           #</span></span><br><span class="line">    <span class="comment"># Hint: Instead of computing the gradient from scratch, it may be easier    #</span></span><br><span class="line">    <span class="comment"># to reuse some of the intermediate values that you used to compute the     #</span></span><br><span class="line">    <span class="comment"># loss.                                                                     #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    margins[margins &gt; <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">    num_minus = np.sum(margins, axis=<span class="number">1</span>)</span><br><span class="line">    margins[np.arange(num_train), y] = -num_minus</span><br><span class="line">    dW = X.T.dot(margins) / num_train + <span class="number">2</span> * reg * W</span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                              #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, dW</span><br></pre></td></tr></table></figure>
<p>After finish the code of SVM, you can turn to svm.ipynb file to test your algorithm and do the further exercises. </p>
<h4 id="Checking-your-Code"><a href="#Checking-your-Code" class="headerlink" title="Checking your Code"></a>Checking your Code</h4><p>The idea of debugging the code is based on numerical estimate and analytical estimate. Before you start the word, you should load the data and preprocess it. </p>
<p><img src="/2020/02/14/SVM/1581653437812.png" alt="1581653437812"></p>
<p><img src="/2020/02/14/SVM/1581653462776.png" alt="1581653462776"></p>
<p><img src="/2020/02/14/SVM/1581653476956.png" alt="1581653476956"></p>
<p><img src="/2020/02/14/SVM/Users\zhong201707030308\AppData\Roaming\Typora\typora-user-images\1581653495267.png" alt="1581653495267"></p>
<p><img src="/2020/02/14/SVM/1581653546323.png" alt="1581653546323"></p>
<p><img src="/2020/02/14/SVM/1581653560001.png" alt="1581653560001"></p>
<p><img src="/2020/02/14/SVM/1581653577111.png" alt="1581653577111"></p>
<p>Then you can use the algorithm to calculate the loss and dW. As the graph shown below, the error between numerical estimate and analytical estimate is very low, so we can believe that our algorithm is ok. </p>
<p><img src="/2020/02/14/SVM/1581653628627.png" alt="1581653628627"></p>
<p><img src="/2020/02/14/SVM/1581653643802.png" alt="1581653643802"></p>
<p><img src="/2020/02/14/SVM/1581653654946.png" alt="1581653654946"></p>
<p>Remember that we use two way to calculate loss and dW : with loops and without loops. It is obvious the latter is more efficient. We can check it by observing the running time. </p>
<p><img src="/2020/02/14/SVM/1581654019461.png" alt="1581654019461"></p>
<p><img src="/2020/02/14/SVM/1581654031173.png" alt="1581654031173"></p>
<h4 id="Training-the-Model"><a href="#Training-the-Model" class="headerlink" title="Training the Model"></a>Training the Model</h4><p>The training dataset is so large(49000), so if we use the normal way to calculate loss and dW, it will take a long time. The strategy we use here is SGD( Randomly select a small part of the data to get the loss and dW.) which is more effective. You need to modify some code in linear_classifier.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> cs231n.classifiers.linear_svm <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> cs231n.classifiers.softmax <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> past.builtins <span class="keyword">import</span> xrange</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearClassifier</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.W = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, X, y, learning_rate=<span class="number">1e-3</span>, reg=<span class="number">1e-5</span>, num_iters=<span class="number">100</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">              batch_size=<span class="number">200</span>, verbose=False)</span>:</span></span><br><span class="line">        </span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Train this linear classifier using stochastic gradient descent.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        Inputs:</span></span><br><span class="line"><span class="string">        - X: A numpy array of shape (N, D) containing training data; there are N</span></span><br><span class="line"><span class="string">          training samples each of dimension D.</span></span><br><span class="line"><span class="string">        - y: A numpy array of shape (N,) containing training labels; y[i] = c</span></span><br><span class="line"><span class="string">          means that X[i] has label 0 &lt;= c &lt; C for C classes.</span></span><br><span class="line"><span class="string">        - learning_rate: (float) learning rate for optimization.</span></span><br><span class="line"><span class="string">        - reg: (float) regularization strength.</span></span><br><span class="line"><span class="string">        - num_iters: (integer) number of steps to take when optimizing</span></span><br><span class="line"><span class="string">        - batch_size: (integer) number of training examples to use at each step.</span></span><br><span class="line"><span class="string">        - verbose: (boolean) If true, print progress during optimization.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        Outputs:</span></span><br><span class="line"><span class="string">        A list containing the value of the loss function at each training iteration.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        num_train, dim = X.shape</span><br><span class="line">        num_classes = np.max(y) + <span class="number">1</span>  <span class="comment"># assume y takes values 0...K-1 where K is number of classes</span></span><br><span class="line">        <span class="keyword">if</span> self.W <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># lazily initialize W</span></span><br><span class="line">            self.W = <span class="number">0.001</span> * np.random.randn(dim, num_classes)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Run stochastic gradient descent to optimize W</span></span><br><span class="line">        loss_history = []</span><br><span class="line">        <span class="keyword">for</span> it <span class="keyword">in</span> xrange(num_iters):</span><br><span class="line">            </span><br><span class="line">            X_batch = <span class="literal">None</span></span><br><span class="line">            y_batch = <span class="literal">None</span></span><br><span class="line">            </span><br><span class="line"></span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line">            <span class="comment"># <span class="doctag">TODO:</span>                                                                 #</span></span><br><span class="line">            <span class="comment"># Sample batch_size elements from the training data and their           #</span></span><br><span class="line">            <span class="comment"># corresponding labels to use in this round of gradient descent.        #</span></span><br><span class="line">            <span class="comment"># Store the data in X_batch and their corresponding labels in           #</span></span><br><span class="line">            <span class="comment"># y_batch; after sampling X_batch should have shape (dim, batch_size)   #</span></span><br><span class="line">            <span class="comment"># and y_batch should have shape (batch_size,)                           #</span></span><br><span class="line">            <span class="comment">#                                                                       #</span></span><br><span class="line">            <span class="comment"># Hint: Use np.random.choice to generate indices. Sampling with         #</span></span><br><span class="line">            <span class="comment"># replacement is faster than sampling without replacement.              #</span></span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line">            index_choice = np.random.choice(num_train, batch_size)</span><br><span class="line">            X_batch = X[index_choice]</span><br><span class="line">            y_batch = y[index_choice]</span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line">            <span class="comment">#                       END OF YOUR CODE                                #</span></span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># evaluate loss and gradient</span></span><br><span class="line">            loss, grad = self.loss(X_batch, y_batch, reg)</span><br><span class="line">            loss_history.append(loss)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># perform parameter update</span></span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line">            <span class="comment"># <span class="doctag">TODO:</span>                                                                 #</span></span><br><span class="line">            <span class="comment"># Update the weights using the gradient and the learning rate.          #</span></span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line">            self.W -= learning_rate*grad </span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line">            <span class="comment">#                       END OF YOUR CODE                                #</span></span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> verbose <span class="keyword">and</span> it % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                print(<span class="string">'iteration %d / %d: loss %f'</span> % (it, num_iters, loss))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss_history</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Use the trained weights of this linear classifier to predict labels for</span></span><br><span class="line"><span class="string">        data points.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        Inputs:</span></span><br><span class="line"><span class="string">        - X: A numpy array of shape (N, D) containing training data; there are N</span></span><br><span class="line"><span class="string">          training samples each of dimension D.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">        - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional</span></span><br><span class="line"><span class="string">          array of length N, and each element is an integer giving the predicted</span></span><br><span class="line"><span class="string">          class.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        y_pred = np.zeros(X.shape[<span class="number">0</span>])</span><br><span class="line">        <span class="comment">###########################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span>                                                                   #</span></span><br><span class="line">        <span class="comment"># Implement this method. Store the predicted labels in y_pred.            #</span></span><br><span class="line">        <span class="comment">###########################################################################</span></span><br><span class="line">        scores = X.dot(self.W)</span><br><span class="line">        y_pred = np.argmax(scores, axis=<span class="number">1</span>)</span><br><span class="line">        <span class="comment">###########################################################################</span></span><br><span class="line">        <span class="comment">#                           END OF YOUR CODE                              #</span></span><br><span class="line">        <span class="comment">###########################################################################</span></span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(self, X_batch, y_batch, reg)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Compute the loss function and its derivative. </span></span><br><span class="line"><span class="string">        Subclasses will override this.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        Inputs:</span></span><br><span class="line"><span class="string">        - X_batch: A numpy array of shape (N, D) containing a minibatch of N</span></span><br><span class="line"><span class="string">          data points; each point has dimension D.</span></span><br><span class="line"><span class="string">        - y_batch: A numpy array of shape (N,) containing labels for the minibatch.</span></span><br><span class="line"><span class="string">        - reg: (float) regularization strength.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        Returns: A tuple containing:</span></span><br><span class="line"><span class="string">        - loss as a single float</span></span><br><span class="line"><span class="string">        - gradient with respect to self.W; an array of the same shape as W</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearSVM</span><span class="params">(LinearClassifier)</span>:</span></span><br><span class="line">    <span class="string">""" A subclass that uses the Multiclass SVM loss function """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(self, X_batch, y_batch, reg)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> svm_loss_vectorized(self.W, X_batch, y_batch, reg)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Softmax</span><span class="params">(LinearClassifier)</span>:</span></span><br><span class="line">    <span class="string">""" A subclass that uses the Softmax + Cross-entropy loss function """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(self, X_batch, y_batch, reg)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> softmax_loss_vectorized(self.W, X_batch, y_batch, reg)</span><br></pre></td></tr></table></figure>
<p> Then, you can see the time it take to run the algorithm and the change of the loss. </p>
<p><img src="/2020/02/14/SVM/1581654617210.png" alt="1581654617210"></p>
<p><img src="/2020/02/14/SVM/1581654631673.png" alt="1581654631673"></p>
<p>Use the model to predict the data. The accuracy of validation is 0.38 which is higher than knn.</p>
<p><img src="/2020/02/14/SVM/1581655038395.png" alt="1581655038395"></p>
<p>The accuracy is related to learning rate and the regularization rate. Then we use cross validate to find the best learning rate and regularization rate. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Use the validation set to tune hyperparameters (regularization strength and</span></span><br><span class="line"><span class="comment"># learning rate). You should experiment with different ranges for the learning</span></span><br><span class="line"><span class="comment"># rates and regularization strengths; if you are careful you should be able to</span></span><br><span class="line"><span class="comment"># get a classification accuracy of about 0.4 on the validation set.</span></span><br><span class="line">learning_rates = [<span class="number">1.31e-7</span>,<span class="number">1.33e-7</span>,<span class="number">1.35e-7</span>,<span class="number">1.37e-7</span>,<span class="number">1.38e-7</span>,<span class="number">1.4e-7</span>]</span><br><span class="line">regularization_strengths = [<span class="number">2.6e4</span>, <span class="number">2.63e4</span>,<span class="number">2.66e4</span>,<span class="number">2.69e4</span>,<span class="number">2.72e4</span>,<span class="number">2.75e4</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># results is dictionary mapping tuples of the form</span></span><br><span class="line"><span class="comment"># (learning_rate, regularization_strength) to tuples of the form</span></span><br><span class="line"><span class="comment"># (training_accuracy, validation_accuracy). The accuracy is simply the fraction</span></span><br><span class="line"><span class="comment"># of data points that are correctly classified.</span></span><br><span class="line">results = &#123;&#125;</span><br><span class="line">best_val = <span class="number">-1</span>   <span class="comment"># The highest validation accuracy that we have seen so far.</span></span><br><span class="line">best_svm = <span class="literal">None</span> <span class="comment"># The LinearSVM object that achieved the highest validation rate.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span>                                                                        #</span></span><br><span class="line"><span class="comment"># Write code that chooses the best hyperparameters by tuning on the validation #</span></span><br><span class="line"><span class="comment"># set. For each combination of hyperparameters, train a linear SVM on the      #</span></span><br><span class="line"><span class="comment"># training set, compute its accuracy on the training and validation sets, and  #</span></span><br><span class="line"><span class="comment"># store these numbers in the results dictionary. In addition, store the best   #</span></span><br><span class="line"><span class="comment"># validation accuracy in best_val and the LinearSVM object that achieves this  #</span></span><br><span class="line"><span class="comment"># accuracy in best_svm.                                                        #</span></span><br><span class="line"><span class="comment">#                                                                              #</span></span><br><span class="line"><span class="comment"># Hint: You should use a small value for num_iters as you develop your         #</span></span><br><span class="line"><span class="comment"># validation code so that the SVMs don't take much time to train; once you are #</span></span><br><span class="line"><span class="comment"># confident that your validation code works, you should rerun the validation   #</span></span><br><span class="line"><span class="comment"># code with a larger value for num_iters.                                      #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> lr_i <span class="keyword">in</span> learning_rates:</span><br><span class="line">    <span class="keyword">for</span> rs_i <span class="keyword">in</span> regularization_strengths:</span><br><span class="line">        svm = LinearSVM()</span><br><span class="line">        svm.train(X_train, y_train, learning_rate=lr_i, reg=rs_i,num_iters=<span class="number">1500</span>, verbose=<span class="literal">True</span>)</span><br><span class="line">        y_train_pred = svm.predict(X_train)</span><br><span class="line">        y_val_pred = svm.predict(X_val)</span><br><span class="line">        y_train_acc = np.mean(y_train == y_train_pred)</span><br><span class="line">        y_val_acc = np.mean(y_val == y_val_pred)</span><br><span class="line">        results[(lr_i, rs_i)] = y_train_acc , y_val_acc</span><br><span class="line">        <span class="keyword">if</span> y_val_acc &gt; best_val:</span><br><span class="line">            best_val = y_val_acc</span><br><span class="line">            best_svm = svm</span><br><span class="line"></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment">#                              END OF YOUR CODE                                #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># Print out results.</span></span><br><span class="line"><span class="keyword">for</span> lr, reg <span class="keyword">in</span> sorted(results):</span><br><span class="line">    train_accuracy, val_accuracy = results[(lr, reg)]</span><br><span class="line">    print(<span class="string">'lr %e reg %e train accuracy: %f val accuracy: %f'</span> % (</span><br><span class="line">                lr, reg, train_accuracy, val_accuracy))</span><br><span class="line">    </span><br><span class="line">print(<span class="string">'best validation accuracy achieved during cross-validation: %f'</span> % best_val)</span><br></pre></td></tr></table></figure>
<p>We can get the best validate accuracy is 0.394, then use the best model for testing and the accuracy is 0.371</p>
<p><img src="/2020/02/14/SVM/1581655455574.png" alt="1581655455574"></p>
<p><img src="/2020/02/14/SVM/1581655532270.png" alt="1581655532270"></p>
<p><img src="/2020/02/14/SVM/1581655545859.png" alt="1581655545859"></p>
<p><img src="/2020/02/14/SVM/1581655565773.png" alt="1581655565773"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/13/KNN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Bania">
      <meta itemprop="description" content="A platform for discussing programming and technology">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BaniaBlog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/13/KNN/" class="post-title-link" itemprop="url">KNN</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-02-13 14:57:08 / Modified: 16:44:00" itemprop="dateCreated datePublished" datetime="2020-02-13T14:57:08+08:00">2020-02-13</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h4><h5 id="KNN-Classifier"><a href="#KNN-Classifier" class="headerlink" title="KNN Classifier"></a>KNN Classifier</h5><p>In assignment1 the first question is K-Nearest Neighbor Classifier. You should open the classifiers file and write your code in the k_nearest_neighbor.py. </p>
<p>The first thing you should keep in mind is the idea of KNN. Let me give a brief of it. In our dataset, we often split the data into training data and testing data. In testing data, we do not know the value of the target variable(the variable we need to predict), while in training data, the target variable is given and we use this data to train our model. For KNN, we also need the training data to predict the target variable in the testing data. How to achieve this? We can understand it by the name of KNN. For example, now we have a testing data point and we need to know which categories it belongs to. And then we find k points which are near the testing point most. The way we decide the k points is by measuring the distance between the testing point and all training data points. After we get these k points and they all have their own categories, we can see the most common category in these points and the testing point belongs to this category. </p>
<p>The idea of KNN is quite straightforward. In conclusion, we just need three steps:</p>
<ol>
<li>Measuring the distance between testing data and training data.</li>
<li>For every testing data point, selecting nearsest k points. </li>
<li>In those k points, find the most common value of target variable and use it to predict the value of target variable for testing data.</li>
</ol>
<p>Knowing the idea of KNN, now we can code in the file. You can see that there are some instructions in the file, and you just need to follow them. </p>
<p>These code are already written for you and you just need to understand them. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> past.builtins <span class="keyword">import</span> xrange</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KNearestNeighbor</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="string">""" a kNN classifier with L2 distance """</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">     <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Train the classifier. For k-nearest neighbors this is just </span></span><br><span class="line"><span class="string">    memorizing the training data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - X: A numpy array of shape (num_train, D) containing the training data</span></span><br><span class="line"><span class="string">    consisting of num_train samples each of dimension D.</span></span><br><span class="line"><span class="string">    - y: A numpy array of shape (N,) containing the training labels, where</span></span><br><span class="line"><span class="string">      y[i] is the label for X[i].</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    self.X_train = X</span><br><span class="line">    self.y_train = y</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X, k=<span class="number">1</span>, num_loops=<span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Predict labels for test data using this classifier.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - X: A numpy array of shape (num_test, D) containing test data consisting</span></span><br><span class="line"><span class="string">         of num_test samples each of dimension D.</span></span><br><span class="line"><span class="string">    - k: The number of nearest neighbors that vote for the predicted labels.</span></span><br><span class="line"><span class="string">    - num_loops: Determines which implementation to use to compute distances</span></span><br><span class="line"><span class="string">      between training points and testing points.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - y: A numpy array of shape (num_test,) containing predicted labels for the</span></span><br><span class="line"><span class="string">      test data, where y[i] is the predicted label for the test point X[i].  </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> num_loops == <span class="number">0</span>:</span><br><span class="line">      dists = self.compute_distances_no_loops(X)</span><br><span class="line">    <span class="keyword">elif</span> num_loops == <span class="number">1</span>:</span><br><span class="line">      dists = self.compute_distances_one_loop(X)</span><br><span class="line">    <span class="keyword">elif</span> num_loops == <span class="number">2</span>:</span><br><span class="line">      dists = self.compute_distances_two_loops(X)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">raise</span> ValueError(<span class="string">'Invalid value %d for num_loops'</span> % num_loops)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self.predict_labels(dists, k=k)</span><br></pre></td></tr></table></figure>
<p>Here is the first step: measuring the distance. We have three algorithms to do this. Different algorithms have different performance and you can see it in the next session. </p>
<p>The first algorithm is simple but consume more time.  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_distances_two_loops</span><span class="params">(self, X)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the distance between each test point in X and each training point</span></span><br><span class="line"><span class="string">    in self.X_train using a nested loop over both the training data and the </span></span><br><span class="line"><span class="string">    test data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - X: A numpy array of shape (num_test, D) containing test data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - dists: A numpy array of shape (num_test, num_train) where dists[i, j]</span></span><br><span class="line"><span class="string">      is the Euclidean distance between the ith test point and the jth training</span></span><br><span class="line"><span class="string">      point.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    num_test = X.shape[<span class="number">0</span>]</span><br><span class="line">    num_train = self.X_train.shape[<span class="number">0</span>]</span><br><span class="line">    dists = np.zeros((num_test, num_train))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(num_test):</span><br><span class="line">      <span class="keyword">for</span> j <span class="keyword">in</span> xrange(num_train): </span><br><span class="line">        <span class="comment"># calculate the distance</span></span><br><span class="line">        dists[i][j] = np.sum((X[i] - self.X_train[j])**<span class="number">2</span>)**<span class="number">0.5</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        np.sum() method is used to add an matrix, axis is one of the peremeter in this           method. In this case, the X[i] - self.X_train[j] has one axis, and we do not need         to give the axis peremeter or if you like you can set it to 0.</span></span><br><span class="line"><span class="string">        We can also see that in many numpy method there will be axis peremeter.Different         dataset has different dimension and axis is the one you want to delete by using           the method. </span></span><br><span class="line"><span class="string">        For example, by using np.sum(One dimensional dataset, axis=0) it will give you a         point. np.sum(two dimensional dataset, axis=0) will add each row and return a one         dimensional data, while axis=1 will add each column. </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment">#####################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span>                                                             #</span></span><br><span class="line">        <span class="comment"># Compute the l2 distance between the ith test point and the jth    #</span></span><br><span class="line">        <span class="comment"># training point, and store the result in dists[i, j]. You should   #</span></span><br><span class="line">        <span class="comment"># not use a loop over dimension.                                    #</span></span><br><span class="line">        <span class="comment">#####################################################################</span></span><br><span class="line">        <span class="comment">#         pass</span></span><br><span class="line">        <span class="comment">#####################################################################</span></span><br><span class="line">        <span class="comment">#                       END OF YOUR CODE                            #</span></span><br><span class="line">        <span class="comment">#####################################################################</span></span><br><span class="line">    <span class="keyword">return</span> dists</span><br></pre></td></tr></table></figure>
<p>The second algorithm uses one loop.  The most important method is tile().</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_distances_one_loop</span><span class="params">(self, X)</span>:</span></span><br><span class="line">   <span class="string">"""</span></span><br><span class="line"><span class="string">   Compute the distance between each test point in X and each training point</span></span><br><span class="line"><span class="string">   in self.X_train using a single loop over the test data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">   Input / Output: Same as compute_distances_two_loops</span></span><br><span class="line"><span class="string">   """</span></span><br><span class="line">   num_test = X.shape[<span class="number">0</span>]</span><br><span class="line">   num_train = self.X_train.shape[<span class="number">0</span>]</span><br><span class="line">   dists = np.zeros((num_test, num_train))</span><br><span class="line">   <span class="keyword">for</span> i <span class="keyword">in</span> xrange(num_test):</span><br><span class="line">       diff = np.tile(X[i], (num_train, <span class="number">1</span>)) - self.X_train</span><br><span class="line">       dists[i] = np.sum(diff**<span class="number">2</span>, axis=<span class="number">1</span>)**<span class="number">0.5</span></span><br><span class="line">       <span class="string">"""</span></span><br><span class="line"><span class="string">       np.tile() can duplicate an array in different size. The second peremeter(rows,           columns) indicate the array should be duplicated rows times in the row                   direction and columns times in the column direction. </span></span><br><span class="line"><span class="string">       In this case, we duplicate the X[i] num_train times so the size of the X[i] is           the same with X_train, then do the substraction.</span></span><br><span class="line"><span class="string">       """</span></span><br><span class="line">     <span class="comment">#######################################################################</span></span><br><span class="line">     <span class="comment"># <span class="doctag">TODO:</span>                                                               #</span></span><br><span class="line">     <span class="comment"># Compute the l2 distance between the ith test point and all training #</span></span><br><span class="line">     <span class="comment"># points, and store the result in dists[i, :].                        #</span></span><br><span class="line">     <span class="comment">#######################################################################</span></span><br><span class="line">   </span><br><span class="line">     <span class="comment">#       pass</span></span><br><span class="line">     <span class="comment">#######################################################################</span></span><br><span class="line">     <span class="comment">#                         END OF YOUR CODE                            #</span></span><br><span class="line">     <span class="comment">#######################################################################</span></span><br><span class="line">   <span class="keyword">return</span> dists</span><br></pre></td></tr></table></figure>
<p>The last algorithm is the most effective one because it uses no loops. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_distances_no_loops</span><span class="params">(self, X)</span>:</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  Compute the distance between each test point in X and each training point</span></span><br><span class="line"><span class="string">  in self.X_train using no explicit loops.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Input / Output: Same as compute_distances_two_loops</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  num_test = X.shape[<span class="number">0</span>]</span><br><span class="line">  num_train = self.X_train.shape[<span class="number">0</span>]</span><br><span class="line">  dists = np.zeros((num_test, num_train)) </span><br><span class="line">  <span class="comment"># the main idea is (a-b)**2 = a**2 +b**2 -2ab</span></span><br><span class="line">  multi_matrix = np.dot(X, self.X_train.T)</span><br><span class="line">  X_sum = np.sum(X**<span class="number">2</span>, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">  <span class="comment"># using sum will return a vector so we set keepdims to True and it will not change       the shape of the data.</span></span><br><span class="line">  X_squared = np.tile(X_sum,  multi_matrix.shape[<span class="number">1</span>])</span><br><span class="line">  X_train_sum = np.sum(self.X_train.T**<span class="number">2</span>, axis=<span class="number">0</span>)</span><br><span class="line">  X_train_squared = np.tile(X_train_sum, (multi_matrix.shape[<span class="number">0</span>], <span class="number">1</span>))</span><br><span class="line">  dists = (X_squared + X_train_squared - <span class="number">2</span>*multi_matrix)**<span class="number">0.5</span></span><br><span class="line">  <span class="comment">#########################################################################</span></span><br><span class="line">  <span class="comment"># <span class="doctag">TODO:</span>                                                                 #</span></span><br><span class="line">  <span class="comment"># Compute the l2 distance between all test points and all training      #</span></span><br><span class="line">  <span class="comment"># points without using any explicit loops, and store the result in      #</span></span><br><span class="line">  <span class="comment"># dists.                                                                #</span></span><br><span class="line">  <span class="comment">#                                                                       #</span></span><br><span class="line">  <span class="comment"># You should implement this function using only basic array operations; #</span></span><br><span class="line">  <span class="comment"># in particular you should not use functions from scipy.                #</span></span><br><span class="line">  <span class="comment">#                                                                       #</span></span><br><span class="line">  <span class="comment"># HINT: Try to formulate the l2 distance using matrix multiplication    #</span></span><br><span class="line">  <span class="comment">#       and two broadcast sums.                                         #</span></span><br><span class="line">  <span class="comment">#########################################################################</span></span><br><span class="line">  <span class="comment">#     pass</span></span><br><span class="line">  <span class="comment">#########################################################################</span></span><br><span class="line">  <span class="comment">#                         END OF YOUR CODE                              #</span></span><br><span class="line">  <span class="comment">#########################################################################</span></span><br><span class="line">  <span class="keyword">return</span> dists</span><br></pre></td></tr></table></figure>
<p>Then, we can test those three algorithms to see whether they can get the same result. </p>
<p><img src="/2020/02/13/KNN/1581326111097.png" alt="1581326111097"></p>
<p>Now we will turn to step2 and step3. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">predict_labels</span><span class="params">(self, dists, k=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Given a matrix of distances between test points and training points,</span></span><br><span class="line"><span class="string">    predict a label for each test point.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dists: A numpy array of shape (num_test, num_train) where dists[i, j]</span></span><br><span class="line"><span class="string">      gives the distance betwen the ith test point and the jth training point.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - y: A numpy array of shape (num_test,) containing predicted labels for the</span></span><br><span class="line"><span class="string">      test data, where y[i] is the predicted label for the test point X[i].  </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    num_test = dists.shape[<span class="number">0</span>]</span><br><span class="line">    y_pred = np.zeros(num_test)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(num_test):</span><br><span class="line">      <span class="comment"># A list of length k storing the labels of the k nearest neighbors to</span></span><br><span class="line">      <span class="comment"># the ith test point.</span></span><br><span class="line">        closest_y = []</span><br><span class="line">      <span class="comment">#########################################################################</span></span><br><span class="line">      <span class="comment"># <span class="doctag">TODO:</span>                                                                 #</span></span><br><span class="line">      <span class="comment"># Use the distance matrix to find the k nearest neighbors of the ith    #</span></span><br><span class="line">      <span class="comment"># testing point, and use self.y_train to find the labels of these       #</span></span><br><span class="line">      <span class="comment"># neighbors. Store these labels in closest_y.                           #</span></span><br><span class="line">      <span class="comment"># Hint: Look up the function numpy.argsort.                             #</span></span><br><span class="line">      <span class="comment">#########################################################################</span></span><br><span class="line">      </span><br><span class="line">        labels_index = np.argsort(dists[i]) </span><br><span class="line">        closest_y = np.array(self.y_train)[labels_index[:k]]</span><br><span class="line">        </span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        np.argsort() can return the index of the value from low to high. Then we can get         the k points. </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"><span class="comment">#       pass</span></span><br><span class="line">      <span class="comment">#########################################################################</span></span><br><span class="line">      <span class="comment"># <span class="doctag">TODO:</span>                                                                 #</span></span><br><span class="line">      <span class="comment"># Now that you have found the labels of the k nearest neighbors, you    #</span></span><br><span class="line">      <span class="comment"># need to find the most common label in the list closest_y of labels.   #</span></span><br><span class="line">      <span class="comment"># Store this label in y_pred[i]. Break ties by choosing the smaller     #</span></span><br><span class="line">      <span class="comment"># label.                                                                #</span></span><br><span class="line">      <span class="comment">#########################################################################</span></span><br><span class="line">       </span><br><span class="line"><span class="comment">#         from collections import Counter</span></span><br><span class="line"><span class="comment">#         labels_counter = Counter(closest_y)</span></span><br><span class="line"><span class="comment">#         y_pred[i] = labels_counter.most_common()[0][0]</span></span><br><span class="line">       <span class="comment"># The above is one way to get the most common value in closest_y. And I have the          #more simple way below.  </span></span><br><span class="line">        y_pred[i] = np.argmax(np.bincount(closest_y.tolist()))</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        np.bincount() returns the times of different value appears in closest_y.</span></span><br><span class="line"><span class="string">        np.argmax() returns the most common value in closest_y</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line"><span class="comment">#       pass</span></span><br><span class="line">      <span class="comment">#########################################################################</span></span><br><span class="line">      <span class="comment">#                           END OF YOUR CODE                            # </span></span><br><span class="line">      <span class="comment">#########################################################################</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> y_pred</span><br></pre></td></tr></table></figure>
<p>We can use a simple example to test the model. The result is true, which means now we can use it to classify our data.</p>
<p><img src="/2020/02/13/KNN/1581327098979.png" alt="1581327098979"></p>
<h5 id="Training-and-Testing"><a href="#Training-and-Testing" class="headerlink" title="Training and Testing"></a>Training and Testing</h5><p>In this session, I will show you the whole process of training and testing the model and get the predictions. First, you have to open the knn.ipynb file and you can see you are given some instructions. </p>
<p>1) Load the dataset. </p>
<p><img src="/2020/02/13/KNN/1581327308499.png" alt="1581327308499"></p>
<p><img src="/2020/02/13/KNN/1581327332838.png" alt="1581327332838"></p>
<p>You can find the shape of X_train, y_train, X_test and y_test. load_CIFAR10() is a method that can split the data into training data and testing data. </p>
<p>Then using method below can visualize the dataset. </p>
<p><img src="/2020/02/13/KNN/1581327868618.png" alt="1581327868618"></p>
<p><img src="/2020/02/13/KNN/1581327880973.png" alt="1581327880973"></p>
<p>For the original dataset is too large, we just choose some of the data to analyse. </p>
<p><img src="/2020/02/13/KNN/1581327940045.png" alt="1581327940045"></p>
<p>Then import the knn to train.</p>
<p><img src="/2020/02/13/KNN/1581328056138.png" alt="1581328056138"></p>
<p><img src="/2020/02/13/KNN/1581328071246.png" alt="1581328071246"></p>
<p>We can draw the graph above to see the distance between testing data and training data. Then we can use the model to predict and get the accuracy. We can see that the accuracy is 0.274. It is very low, because we classify the data just by the RGB value of the picture. </p>
<p><img src="/2020/02/13/KNN/1581328131076.png" alt="1581328131076"></p>
<p><img src="/2020/02/13/KNN/1581328226483.png" alt="1581328226483"></p>
<p>When we set k to 1, the accuracy slightly increases. And then we see whether we can get the same distance matrix by using three different algorithms. </p>
<p><img src="/2020/02/13/KNN/1581328356671.png" alt="1581328356671"></p>
<p><img src="/2020/02/13/KNN/1581328379895.png" alt="1581328379895"></p>
<p>Then look at the running time. It is obvious that no-loops is the most effective. </p>
<p><img src="/2020/02/13/KNN/1581328411502.png" alt="1581328411502"></p>
<p>You may notice that when we use k=1 and k=5, the accuracy is different, so can we find the best k value which has the highest accuracy? For this, we can use cross validation. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">num_folds = <span class="number">5</span></span><br><span class="line">k_choices = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">15</span>, <span class="number">20</span>, <span class="number">50</span>, <span class="number">100</span>]</span><br><span class="line"></span><br><span class="line">X_train_folds = []</span><br><span class="line">y_train_folds = []</span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span>                                                                        #</span></span><br><span class="line"><span class="comment"># Split up the training data into folds. After splitting, X_train_folds and    #</span></span><br><span class="line"><span class="comment"># y_train_folds should each be lists of length num_folds, where                #</span></span><br><span class="line"><span class="comment"># y_train_folds[i] is the label vector for the points in X_train_folds[i].     #</span></span><br><span class="line"><span class="comment"># Hint: Look up the numpy array_split function.                                #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line">X_train_folds = np.array_split(X_train, num_folds)</span><br><span class="line">y_train_folds = np.array_split(y_train, num_folds)</span><br><span class="line"><span class="comment"># pass</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment">#                                 END OF YOUR CODE                             #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># A dictionary holding the accuracies for different values of k that we find</span></span><br><span class="line"><span class="comment"># when running cross-validation. After running cross-validation,</span></span><br><span class="line"><span class="comment"># k_to_accuracies[k] should be a list of length num_folds giving the different</span></span><br><span class="line"><span class="comment"># accuracy values that we found when using that value of k.</span></span><br><span class="line">k_to_accuracies = &#123;&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span>                                                                        #</span></span><br><span class="line"><span class="comment"># Perform k-fold cross validation to find the best value of k. For each        #</span></span><br><span class="line"><span class="comment"># possible value of k, run the k-nearest-neighbor algorithm num_folds times,   #</span></span><br><span class="line"><span class="comment"># where in each case you use all but one of the folds as training data and the #</span></span><br><span class="line"><span class="comment"># last fold as a validation set. Store the accuracies for all fold and all     #</span></span><br><span class="line"><span class="comment"># values of k in the k_to_accuracies dictionary.                               #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="keyword">for</span> k_value <span class="keyword">in</span> k_choices:</span><br><span class="line">    k_to_accuracies[k_value] = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_folds):</span><br><span class="line">        <span class="comment"># didn't work</span></span><br><span class="line"><span class="comment">#         X_train_cv = np.delete(X_train_folds, i).tolist()</span></span><br><span class="line"><span class="comment">#         y_train_cv = np.delete(y_train_folds, i).tolist()        </span></span><br><span class="line"><span class="comment">#         X_train_cv = np.delete(X_train_folds, i,         axis=0).reshape(X_train_folds[i].shape[0],X_train_folds[i].shape[1])</span></span><br><span class="line">        X_train_temp = np.delete(X_train_folds, i, axis=<span class="number">0</span>)</span><br><span class="line">        X_train_cv = np.concatenate(X_train_temp)</span><br><span class="line">        y_train_temp = np.delete(y_train_folds, i, axis=<span class="number">0</span>)</span><br><span class="line">        y_train_cv = np.concatenate(y_train_temp)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        classifier.train(X_train_cv, y_train_cv)</span><br><span class="line">        dist = classifier.compute_distances_no_loops(X_train_folds[i])</span><br><span class="line">        y_test_predictions = classifier.predict_labels(dist, k_value)</span><br><span class="line">        num_corrects = np.sum(y_test_predictions == y_train_folds[i])</span><br><span class="line">        accuracy = float(num_corrects) / y_train_folds[i].shape[<span class="number">0</span>]</span><br><span class="line">        k_to_accuracies[k_value].append(accuracy)</span><br><span class="line"><span class="comment"># pass</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment">#                                 END OF YOUR CODE                             #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Print out the computed accuracies</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> sorted(k_to_accuracies):</span><br><span class="line">    <span class="keyword">for</span> accuracy <span class="keyword">in</span> k_to_accuracies[k]:</span><br><span class="line">        print(<span class="string">'k = %d, accuracy = %f'</span> % (k, accuracy))</span><br></pre></td></tr></table></figure>
<p>You can draw the graph to see which k is the best. </p>
<p><img src="/2020/02/13/KNN/1581329141634.png" alt="1581329141634"></p>
<p><img src="/2020/02/13/KNN/1581329151487.png" alt="1581329151487"></p>
<p><img src="/2020/02/13/KNN/1581329185444.png" alt="1581329185444"></p>
<p>We can see that 8 is the best value for k and we use it to get the accuracy: 0.274.</p>
<h5 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h5><p>KNN is an easy algorithm to classify different categories just by calculating the distance between testing data and training data. There are three ways to calculae the distance and the no-loops way is the most efficient. But it is not a very effective way to correctly classify different categories. The accuracy is not very high as we can see from the result. </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/13/Setup/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Bania">
      <meta itemprop="description" content="A platform for discussing programming and technology">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BaniaBlog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/13/Setup/" class="post-title-link" itemprop="url">CS231n assignment1 Environment Setup</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-02-13 13:56:34 / Modified: 16:44:18" itemprop="dateCreated datePublished" datetime="2020-02-13T13:56:34+08:00">2020-02-13</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="Environment-setup"><a href="#Environment-setup" class="headerlink" title="Environment setup"></a>Environment setup</h3><p>In this session, I will illustrate how to setup an environment for the CS231n assignment1. It is a little difference from the way we carry out our projects before. I know that many people would use Pycharm to do python programming, which is convenient and easy. However, the requirement of the environment for assigment1 is based on ubuntu system and ipython. If you do not have ubuntu system, you can download it from its website or if you are using window system, you can get it from microsoft store.  The one I use is Ubuntu 18.04 LTS. I assume that you have downloaded ubuntu system and use python3.x, and now I will show you the whole environment setup for the assignment. </p>
<p>In the website: <a href="http://cs231n.github.io/assignments2017/assignment1/" target="_blank" rel="noopener">http://cs231n.github.io/assignments2017/assignment1/</a> , you can see that there are some instructions of the environment setup. </p>
<ol>
<li><p>Get the code as a zip file in the website. Click here and you can download the file. Please remember the path of the file.</p>
<p><img src="/2020/02/13/Setup/1581303993815.png" alt="1581303993815"></p>
</li>
<li><p>In your ubuntu setup a virtual environment. </p>
<p>1) Enter your file.</p>
</li>
</ol>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cd assignment1          <span class="comment"># this is the path of your file</span></span><br></pre></td></tr></table></figure>
<p>If your file is stored on your windows system, you should remember to add <strong>/mnt/</strong> before the path  eg. <strong>/mnt/c/Users/zhong201707030308/Desktop/assignment1</strong> </p>
<p>​     2) Install vituralenv if you do not have it.</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo pip <span class="keyword">install</span> virtualenv      <span class="comment"># This may already be installed</span></span><br></pre></td></tr></table></figure>
<p>​    3)Create a virtual environment.</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">virtualenv -p python3 .env       <span class="comment"># Create a virtual environment (python3)</span></span><br></pre></td></tr></table></figure>
<p>   4)Activate your virtual environment.</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">source .env/bin/activate         <span class="comment"># Activate the virtual environment</span></span><br></pre></td></tr></table></figure>
<p>Now that you have created your virtual environment, in this environment you can use python3. Note that you do not have any necessary packages now, so you need to download them. </p>
<ol>
<li>Download all the necessary packages. </li>
</ol>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip <span class="keyword">install</span> -r requirements.txt  <span class="comment"># Install dependencies</span></span><br></pre></td></tr></table></figure>
<p>The requirements.txt has listed all the libraries you may have to use.  You may also face the same problem as I did when installing the libraries. The first one may be downloading numpy==1.10.4. The easiest way to deal with it is downloading the latest version. It is quite tricky sometimes when you are installing some packages. In this case, the only thing you can do is to search on the internet. The second problem is site==0.0.1. You will be prompted that the compute can find any version of site, but you do not need to worry about it. You can just ignore this package which does not matter the programming. </p>
<p>After that ,you have installed some necessary packages, but if you need other libraries you can just install them by yourself. Next, I will show you other way to install these packages. </p>
<ol>
<li><p>Start programming. </p>
<p>1）Download dataset.</p>
</li>
</ol>
<p>In the assignment1 file enter the path:  <strong>cs231n/datasets</strong>, and then use <strong>./get_datasets.sh</strong> to get the dataset.</p>
<figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cd cs231n/datasets</span><br><span class="line"></span><br><span class="line">./get_datasets.sh</span><br></pre></td></tr></table></figure>
<p>When you have implemented the command above, you will find that in the <strong>datasets/cifar-10-batches-py</strong> file there are many datasets. You can click readme.html to see the descriptions of the data.</p>
<p><img src="/2020/02/13/Setup/1581306454897.png" alt="1581306454897"></p>
<p>​      2) Start your jupyter notebook.</p>
<p>Use the command : <strong>jupyter notebook</strong> and you can start it. In this case, you can get the url and then open your notebook on the website. </p>
<p><img src="/2020/02/13/Setup/1581306823481.png" alt="1581306823481"></p>
<p><img src="/2020/02/13/Setup/1581306996635.png" alt="1581306996635"></p>
<p>If you want to create a .py file and want to run it on the notebook, you should click <strong>New</strong> on the top right  corner and then choose python3. </p>
<p><img src="/2020/02/13/Setup/1581307248848.png" alt="1581307248848"></p>
<p>Then you can program your code and run it. </p>
<p><img src="/2020/02/13/Setup/1581307297557.png" alt="1581307297557"></p>
<p>However, you may face the last and troublesome problem that is you can not import the packages which you have installed before. That is because the python3 jupyter notebook uses is not the same with the python3 that you have created in the virtual environment. You need to change the path of it. </p>
<p>First, check the path of python3 you use in the virtual environment. </p>
<p><img src="/2020/02/13/Setup/1581307763017.png" alt="1581307763017"></p>
<p>The path is where your python.exe exists. </p>
<p>Second, check the path of python3 the jupyter notebook uses. </p>
<p><img src="/2020/02/13/Setup/1581307874063.png" alt="1581307874063"></p>
<p>The first time I run this code, I got <strong>“/usr/bin/python3”</strong>. You can notice that the two paths are different. Now, you should modify the path of the jupyter notebook. </p>
<p><img src="/2020/02/13/Setup/1581308104047.png" alt="1581308104047"></p>
<p>Then enter the path shown above in your ubuntu and modify the kernel.json .</p>
<p><img src="/2020/02/13/Setup/1581308180368.png" alt="1581308180368"></p>
<p><img src="/2020/02/13/Setup/1581308206901.png" alt="1581308206901"></p>
<p>Change the path to your python’s path, then you can import all those packages successfully. That means you can enjoy the coding. </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

  </div>

  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Bania</p>
  <div class="site-description" itemprop="description">A platform for discussing programming and technology</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">7</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Bania</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.7.1
  </div>

<div>
<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<span id="busuanzi_container_site_pv" style='display:none'>
  Total visits: <span id="busuanzi_value_site_pv"></span> times
  <span class="post-meta-divider">|</span>
</span>
<span id="busuanzi_container_site_uv" style='display:none'>
  <span id="busuanzi_value_site_uv"></span>people have viewed my bolg.
</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
        loader: {
          load: ['[tex]/mhchem']
        },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
