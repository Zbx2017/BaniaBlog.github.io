<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Pisces","version":"7.7.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="A platform for discussing programming and technology">
<meta property="og:type" content="website">
<meta property="og:title" content="BaniaBlog">
<meta property="og:url" content="http://yoursite.com/index.html">
<meta property="og:site_name" content="BaniaBlog">
<meta property="og:description" content="A platform for discussing programming and technology">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="Bania">
<meta property="article:tag" content="Python">
<meta property="article:tag" content=" Java">
<meta property="article:tag" content=" Machine Learning">
<meta property="article:tag" content=" Data Science">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: true,
    isPost: false
  };
</script>

  <title>BaniaBlog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="BaniaBlog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">BaniaBlog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">Sharing Technology</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/Zbx2017/Zbx2017.github.io" class="github-corner" title="Bania GitHub" aria-label="Bania GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
      
  
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/08/11/Priority%20queue/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Bania">
      <meta itemprop="description" content="A platform for discussing programming and technology">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BaniaBlog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/08/11/Priority%20queue/" class="post-title-link" itemprop="url">Priority queue</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-08-11 17:39:15" itemprop="dateCreated datePublished" datetime="2020-08-11T17:39:15+08:00">2020-08-11</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="Priority-queue-优先队列"><a href="#Priority-queue-优先队列" class="headerlink" title="Priority queue(优先队列)"></a>Priority queue(优先队列)</h3><p>队列是一种满足先进先出原则的数据结构，在Python中利用列表就可以简单地实现一个队列。首先需要定义一个列表，接在在列表中加入元素，最后利用pop方法将最先加入的元素取出，即可实现一个队列。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">queue = []</span><br><span class="line">queue.append(<span class="number">2</span>)</span><br><span class="line">queue.append(<span class="number">3</span>)</span><br><span class="line">queue.append(<span class="number">1</span>)</span><br><span class="line">queue.pop(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output: 2</span></span><br></pre></td></tr></table></figure>
<p>队列的应用很多，比如在BFS算法中就使用到了队列的数据结构。但在有些场合中，我们不需要按照时间的先后顺序来取出元素，而是根据优先等级来取元素，这时就需要优先队列这种数据结构。</p>
<p>简单地说，优先队列也会像队列一样保存一些元素，但取出元素的方式会按照元素的等级进行，一般有最大优先队列和最小优先队列之分，前者每次取出的是最大等级的元素，后者取出的是最小等级的元素。按照这种说法，我们可以将上面的代码简单修改一下就实现了优先队列的功能。</p>
<p>比如我们按照客户的VIP等级对客户提供服务，级别高的先服务。首先把客户和其VIP等级添加进队列里，接着将列表降序排序，输出第一个元素即是VIP等级最高的客户。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">pq = []</span><br><span class="line">pq.append((<span class="number">2</span>,<span class="string">"Tom"</span>))</span><br><span class="line">pq.append((<span class="number">3</span>,<span class="string">"Tina"</span>))</span><br><span class="line">pq.append((<span class="number">1</span>,<span class="string">"Tony"</span>))</span><br><span class="line">pq.sort(reverse=<span class="literal">True</span>)</span><br><span class="line">pq.pop(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># output: (3,"Tina")</span></span><br></pre></td></tr></table></figure>
<p>利用列表实现的优先队列虽然简单，但其扩展性较差，比如我们希望输出客户这个对象，而不是将加入列表里面的内容全部输出，这时的列表将不能满足这个功能。同时调用<code>pop</code>方法，会改变剩下元素的索引，其时间复杂度为O(n)。那么有什么方法可以减少时间复杂度，同时提供优先队列的扩展性呢？</p>
<p>上面代码中，当我们取出最高等级的元素时，所用的时间复杂度为O(n)，用何种数据结构可以使得取出最大值或者最小值的时间复杂度小于O(n)呢？二叉堆可以做到，在<code>Heapsort</code>中，我们可以看到对于最大堆，取出最大元素只要取出第一个元素即可，同时调用<code>Max-heapify</code>方法使树的结构满足堆的特性。该方法的时间复杂度为O(logn)，因此完成取出最大值的时间复杂度为O(logn)，要小于线性时间。</p>
<p>下面我将介绍如何利用二叉堆来设计一个优先队列，我将以最小优先队列为例进行介绍。</p>
<p>最主要需要三个方法：</p>
<ul>
<li><code>Insert</code>: 插入元素</li>
<li><code>Extract_Min</code>: 取出最小等级的元素</li>
<li><code>Decrease_Key</code>: 减小某一元素的等级，减小之后需要交换元素以满足堆的特性。</li>
</ul>
<p>Python代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PriorityQueue</span>:</span></span><br><span class="line">    <span class="string">"""Min-heap-based priority queue, using 1-based indexing. Adapted from CLRS.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    Augmented to include a map of keys to their indices in the heap so that</span></span><br><span class="line"><span class="string">    key lookup is constant time and decrease_key(key) is O(log n) time.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Initializes the priority queue."""</span></span><br><span class="line">        self.heap = [<span class="literal">None</span>] <span class="comment"># To make the index 1-based.</span></span><br><span class="line">        self.key_index = &#123;&#125; <span class="comment"># key to index mapping.</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.heap) - <span class="number">1</span></span><br><span class="line">       </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, i)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.heap[i]</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__setitem__</span><span class="params">(self, i, key)</span>:</span></span><br><span class="line">        self.heap[i] = key</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">decrease_key</span><span class="params">(self, key)</span>:</span></span><br><span class="line">        <span class="string">"""Decreases the value of the key if it is in the priority queue and </span></span><br><span class="line"><span class="string">        maintains the heap property."""</span></span><br><span class="line">        index = self.key_index[key]</span><br><span class="line">        <span class="keyword">if</span> index:</span><br><span class="line">            self._decrease_key(index, key)</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">insert</span><span class="params">(self, key)</span>:</span></span><br><span class="line">        <span class="string">"""Inserts a key into the priority queue."""</span></span><br><span class="line">        self.heap.append(key)</span><br><span class="line">        self.key_index[key] = len(self)</span><br><span class="line">        self._decrease_key(len(self), key)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">extract_min</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">"""Removes and returns the minimum key."""</span></span><br><span class="line">        <span class="keyword">if</span> len(self) &lt; <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        self._swap(<span class="number">1</span>, len(self))</span><br><span class="line">        min = self.heap.pop()</span><br><span class="line">        <span class="keyword">del</span> self.key_index[min]</span><br><span class="line">        self._min_heapify(<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> min</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_decrease_key</span><span class="params">(self, i, key)</span>:</span></span><br><span class="line">        <span class="string">"""Decreases key at a give index.</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            i: index of the key.</span></span><br><span class="line"><span class="string">            key: key with decreased value.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">while</span> i &gt; <span class="number">1</span>:</span><br><span class="line">            parent = i // <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> self[parent] &gt; key:</span><br><span class="line">                self._swap(i, parent)</span><br><span class="line">                i = parent</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_min_heapify</span><span class="params">(self, i)</span>:</span></span><br><span class="line">        <span class="string">"""Restores the heap property from index i downwards."""</span></span><br><span class="line">        l = <span class="number">2</span> * i</span><br><span class="line">        r = <span class="number">2</span> * i + <span class="number">1</span></span><br><span class="line">        smallest = i</span><br><span class="line">        <span class="keyword">if</span> l &lt;= len(self) <span class="keyword">and</span> self[l] &lt; self[i]:</span><br><span class="line">            smallest = l</span><br><span class="line">        <span class="keyword">if</span> r &lt;= len(self) <span class="keyword">and</span> self[r] &lt; self[smallest]:</span><br><span class="line">            smallest = r</span><br><span class="line">        <span class="keyword">if</span> smallest != i:</span><br><span class="line">            self._swap(i, smallest)</span><br><span class="line">            self._min_heapify(smallest)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">_swap</span><span class="params">(self, i, j)</span>:</span></span><br><span class="line">        <span class="comment"># Swaps the key at indices i and j and updates the key to index map.</span></span><br><span class="line">        self.heap[i], self.heap[j] = self.heap[j], self.heap[i]</span><br><span class="line">        self.key_index[self.heap[i]], self.key_index[self.heap[j]] = i, j</span><br></pre></td></tr></table></figure>
<p> 以下面这个例子说明下上述代码如何使用。现在我需要给车的价值排个序，每次取出价值最低的车。为了方便，创建一个Car类。在上面代码中，我们是按照key的大小构造了二叉堆。按照这个意思我们应该把车的价值用insert方法加入到队列中，但是那样就无法获取其对应的车的信息。为此我们将每个车的对象加入到优先队列，按照它们的价值构造二叉堆，故要在Car类中定义比较大小的方法，如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Car</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, value)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.value = value</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__lt__</span><span class="params">(self, other)</span>:</span></span><br><span class="line">        <span class="comment"># :nodoc: Delegate comparison to distance.</span></span><br><span class="line">        <span class="keyword">return</span> (self.value &lt; other.value <span class="keyword">or</span></span><br><span class="line">                (self.value == other.value <span class="keyword">and</span></span><br><span class="line">                 id(self.name) &lt; id(other.name)))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__le__</span><span class="params">(self, other)</span>:</span></span><br><span class="line">        <span class="comment"># :nodoc: Delegate comparison to distance.</span></span><br><span class="line">        <span class="keyword">return</span> (self.value &lt; other.value <span class="keyword">or</span></span><br><span class="line">                (self.value == other.value <span class="keyword">and</span></span><br><span class="line">                 id(self.name) &lt;= id(other.name)))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__gt__</span><span class="params">(self, other)</span>:</span></span><br><span class="line">        <span class="comment"># :nodoc: Delegate comparison to distance.</span></span><br><span class="line">        <span class="keyword">return</span> (self.value &gt; other.value <span class="keyword">or</span></span><br><span class="line">                (self.value == other.value <span class="keyword">and</span></span><br><span class="line">                 id(self.name) &gt; id(other.name)))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__ge__</span><span class="params">(self, other)</span>:</span></span><br><span class="line">        <span class="comment"># :nodoc: Delegate comparison to distance.</span></span><br><span class="line">        <span class="keyword">return</span> (self.value &gt; other.value <span class="keyword">or</span></span><br><span class="line">                (self.value == other.value <span class="keyword">and</span></span><br><span class="line">                 id(self.name) &gt;= id(other.name)))</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">   car1 = Car(<span class="string">"BMW"</span>, <span class="number">45</span>)</span><br><span class="line">   car2 = Car(<span class="string">"Maybach"</span>, <span class="number">145</span>)</span><br><span class="line">   car3 = Car(<span class="string">"Bugatti"</span>, <span class="number">85</span>)</span><br><span class="line">   car4 = Car(<span class="string">"Cadillac"</span>, <span class="number">78</span>)</span><br><span class="line">   car5 = Car(<span class="string">"Maserati"</span>, <span class="number">85</span>)</span><br><span class="line">   pq = PriorityQueue()</span><br><span class="line">   pq.insert(car1)</span><br><span class="line">   pq.insert(car2)</span><br><span class="line">   pq.insert(car3)</span><br><span class="line">   pq.insert(car4)</span><br><span class="line">   pq.insert(car5)</span><br><span class="line">   print(<span class="string">"队列大小：&#123;0&#125;"</span>.format(len(pq)))</span><br><span class="line">   print(pq.extract_min().name)</span><br><span class="line">   </span><br><span class="line">   <span class="comment"># output:</span></span><br><span class="line">   <span class="comment"># 队列大小：5</span></span><br><span class="line"><span class="comment"># BMW</span></span><br></pre></td></tr></table></figure>
<p>BMW的价值最小，利用<code>extract_min</code>方法取出了car1对象，其<code>name</code>是BMW。</p>
<p>其实在python中已经有相关的模块可实现上述的优先队列的功能，如<code>heapq</code>模块，但只提供实现最小优先队列功能。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> heapq</span><br><span class="line"></span><br><span class="line">q = []</span><br><span class="line"></span><br><span class="line">heapq.heappush(q, (<span class="number">2</span>, <span class="string">'code'</span>))</span><br><span class="line">heapq.heappush(q, (<span class="number">1</span>, <span class="string">'eat'</span>))</span><br><span class="line">heapq.heappush(q, (<span class="number">3</span>, <span class="string">'sleep'</span>))</span><br><span class="line"></span><br><span class="line">print(heapq.heappop(q))</span><br><span class="line"></span><br><span class="line"><span class="comment"># output：(1, 'eat')</span></span><br></pre></td></tr></table></figure>
<p>还有queue中的<code>PriorityQueue</code>，其时间复杂度与heapq的一样，区别在于<code>PriorityQueue</code>是同步的，提供了锁语义来支持多个并发的生产者和消费者。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> queue <span class="keyword">import</span> PriorityQueue</span><br><span class="line"></span><br><span class="line">q = PriorityQueue()</span><br><span class="line"></span><br><span class="line">q.put((<span class="number">2</span>, <span class="string">'code'</span>))</span><br><span class="line">q.put((<span class="number">1</span>, <span class="string">'eat'</span>))</span><br><span class="line">q.put((<span class="number">3</span>, <span class="string">'sleep'</span>))</span><br><span class="line"></span><br><span class="line">print(q.get())</span><br><span class="line"><span class="comment"># output：(1, 'eat')</span></span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/08/09/Heapsort/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Bania">
      <meta itemprop="description" content="A platform for discussing programming and technology">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BaniaBlog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/08/09/Heapsort/" class="post-title-link" itemprop="url">Heapsort</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-08-09 17:23:36" itemprop="dateCreated datePublished" datetime="2020-08-09T17:23:36+08:00">2020-08-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-08-10 13:15:47" itemprop="dateModified" datetime="2020-08-10T13:15:47+08:00">2020-08-10</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="堆排序（Heapsort）"><a href="#堆排序（Heapsort）" class="headerlink" title="堆排序（Heapsort）"></a>堆排序（Heapsort）</h3><h5 id="Heap-property"><a href="#Heap-property" class="headerlink" title="Heap property"></a>Heap property</h5><p>堆数据结构是数组对象，它可以用近似完全二叉树进行表示。例如下面数组A，可以表示成a图的二叉树形式。</p>
<p><img src="/2020/08/09/Heapsort/heap.png" alt="img"></p>
<p>设一节点的索引为i，从上图可以看到，其父节点的索引和子节点的索引与该节点的索引有如下关系：</p>
<script type="math/tex; mode=display">
parent(i) = i//2  \quad(// 表示取整)\\</script><script type="math/tex; mode=display">
left(i) = 2i\\</script><script type="math/tex; mode=display">
right(i) = 2i + 1\\</script><p>例如，元素14的索引为2，其父节点索引为2//2 = 1，左子节点索引为2x2 = 4, 右子节点为2x2+1 = 5</p>
<p>堆有最大堆（max heaps）和最小堆（min heaps）结构，两种结构分别满足不同的特性。对于max heaps，除根节点外，所有节点的父节点的值要大于或等于该节点的值。也即：</p>
<script type="math/tex; mode=display">
A[parent(i)] \ge A[i]</script><p>对于min heaps，其特性刚好相反：</p>
<script type="math/tex; mode=display">
A[parent(i)] \le A[i]</script><p>但是任意给出一个数组，不一定满足堆的上述特性，因此需要相应的操作来保持堆的特性，下面以max heaps为例，讲述堆的相关操作。</p>
<h5 id="Max-heapify"><a href="#Max-heapify" class="headerlink" title="Max heapify"></a>Max heapify</h5><p>max heapify是用来保持堆的特性，对某一节点，判断该节点是否满足堆的特性，如果不满足则将该节点移动合适的位置。</p>
<p><img src="/2020/08/09/Heapsort/heapify.png" alt="img"></p>
<p>如上图，元素4不满足最大堆的特性因为它要比14和7都要小，因此4需要和14和7中最大的一个元素互换位置，以保证父节点的值大于等于子节点的值。互换位置后再次判断是否满足特性，这时仍然不满足，因此需要再次移动直到图c的位置。</p>
<p>因为当移动节点后仍然是进行相同的操作：判断是否满足特性，不满足则移动。因此可以用递归的方式进行编写代码，程序的时间复杂度为O(logn)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">heapify</span><span class="params">(arr, n, i)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    arr:数组</span></span><br><span class="line"><span class="string">    n：数组的长度</span></span><br><span class="line"><span class="string">    i: 当前节点的索引</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    r = <span class="number">2</span>*i + <span class="number">1</span></span><br><span class="line">    l = <span class="number">2</span>*i</span><br><span class="line">    <span class="comment"># 判断左子节点是否大于当前节点</span></span><br><span class="line">    <span class="keyword">if</span> l &lt; n <span class="keyword">and</span> arr[l] &gt; arr[i]:</span><br><span class="line">        largest = l</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        largest = i</span><br><span class="line">    <span class="comment"># 判断右子节点是否大于当前节点 </span></span><br><span class="line">    <span class="keyword">if</span> r &lt; n <span class="keyword">and</span> arr[r] &gt; arr[largest]:</span><br><span class="line">        largest = r</span><br><span class="line">    <span class="comment"># 不满足特性，交换节点位置，交换后再次进行heapify</span></span><br><span class="line">    <span class="keyword">if</span> largest <span class="keyword">is</span> <span class="keyword">not</span> i:</span><br><span class="line">        arr[i], arr[largest] = arr[largest], arr[i]</span><br><span class="line">        heapify(arr, n, largest)</span><br></pre></td></tr></table></figure>
<p>有了上述操作后，我们就可以根据数组来建立max heaps.</p>
<h5 id="Build-max-heaps"><a href="#Build-max-heaps" class="headerlink" title="Build max -heaps"></a>Build max -heaps</h5><p>堆的本质还是一个数组，Max-Heapify只是改变了元素的位置，使其满足最大堆的特性，因此需要对每一个元素检查其是否满足特性。但是A[n/2 + 1,…,n]这部分的元素都是子节点，因为2x(n/2 + 1)&gt;n，超出了索引范围，为此我们只需考虑前半部分的元素，其时间复杂度可以很容易得分析出为O(nlogn)。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_max_heaps</span><span class="params">(arr, n)</span>:</span></span><br><span class="line">    i = n//<span class="number">2</span></span><br><span class="line">    <span class="keyword">while</span> i &gt;= <span class="number">0</span>:</span><br><span class="line">        heapify(arr, n, i)</span><br><span class="line">        i -= <span class="number">1</span></span><br></pre></td></tr></table></figure>
<h5 id="Heapsort-algorithm"><a href="#Heapsort-algorithm" class="headerlink" title="Heapsort algorithm"></a>Heapsort algorithm</h5><p>建立好的最大堆如图c，第一个元素是数组中的最大的元素，而索引靠后的元素是数组中较小的元素。如果我们需要升序排序，那么每一次可以将第一个元素和最后一个元素交换，改变数组的元素排列。交换后利用max-heapify，保持最大堆特性，这时第一个元素是前n-1个元素中最大的元素，将其放在n-1的位置处，以此类推即可完成排序。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">heapSort</span><span class="params">(arr)</span>:</span></span><br><span class="line">    n = len(arr)</span><br><span class="line">	build_max_heaps(arr, n)</span><br><span class="line">    <span class="keyword">for</span> k <span class="keyword">in</span> range(n<span class="number">-1</span>, <span class="number">0</span>, <span class="number">-1</span>):</span><br><span class="line">        arr[k], arr[<span class="number">0</span>] = arr[<span class="number">0</span>], arr[k]</span><br><span class="line">        heapify(arr, k, <span class="number">0</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">arr = [<span class="number">13</span>, <span class="number">11</span>, <span class="number">12</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">7</span>]</span><br><span class="line">heapSort(arr)</span><br><span class="line">n = len(arr)</span><br><span class="line">print(<span class="string">"Sorted array is"</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(n):</span><br><span class="line">    print(<span class="string">" &#123;&#125;"</span>.format(arr[i]), end=<span class="string">""</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># output: </span></span><br><span class="line"><span class="comment"># Sorted array is</span></span><br><span class="line"><span class="comment"># 5 6 7 11 12 13</span></span><br></pre></td></tr></table></figure>
<p>整个程序的时间复杂度为O(nlogn)</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/08/09/%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E7%AE%97%E6%B3%95-1/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Bania">
      <meta itemprop="description" content="A platform for discussing programming and technology">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BaniaBlog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/08/09/%E6%9C%80%E7%9F%AD%E8%B7%AF%E5%BE%84%E7%AE%97%E6%B3%95-1/" class="post-title-link" itemprop="url">最短路径算法-1</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-08-09 17:23:36" itemprop="dateCreated datePublished" datetime="2020-08-09T17:23:36+08:00">2020-08-09</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-08-10 10:04:35" itemprop="dateModified" datetime="2020-08-10T10:04:35+08:00">2020-08-10</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="最短路径算法-1"><a href="#最短路径算法-1" class="headerlink" title="最短路径算法-1"></a>最短路径算法-1</h3><p>BFS和DFS都可以用来找两点之间的最短路径，这里我将介绍其他几种最短路径的算法。</p>
<h5 id="爬山法（Hill-Climbing）"><a href="#爬山法（Hill-Climbing）" class="headerlink" title="爬山法（Hill-Climbing）"></a>爬山法（Hill-Climbing）</h5><p>在图的表示中，我介绍了如何自定义类来表示图，其中就有启发式距离的表示。爬山法是在DFS上基于启发式距离的一种算法。有点类似于贪婪算法，每一次选择离目标顶点最近的顶点进行遍历，因此只需对DFS的代码增加选取离目标最近顶点即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">hill_climbing</span><span class="params">(graph, start, goal)</span>:</span></span><br><span class="line">    pathlist=[(start,)]</span><br><span class="line">    <span class="keyword">if</span> start == goal:</span><br><span class="line">        <span class="keyword">return</span> [start]</span><br><span class="line">    <span class="keyword">while</span> len(pathlist) &gt; <span class="number">0</span>:</span><br><span class="line">        curr_path = pathlist.pop(<span class="number">0</span>)</span><br><span class="line">        curr_node = curr_path[<span class="number">-1</span>]</span><br><span class="line">        new_nodes = graph.get_connected_nodes(curr_node)</span><br><span class="line">        <span class="keyword">if</span> len(curr_path) &gt; <span class="number">1</span>:</span><br><span class="line">            new_nodes = [node <span class="keyword">for</span> node <span class="keyword">in</span> new_nodes <span class="keyword">if</span> node <span class="keyword">not</span> <span class="keyword">in</span> curr_path]</span><br><span class="line">        <span class="keyword">if</span> goal <span class="keyword">in</span> new_nodes:</span><br><span class="line">            goal_path = curr_path + (goal, )</span><br><span class="line">            <span class="keyword">return</span> list(goal_path)</span><br><span class="line">        <span class="comment"># 根据启发式距离对顶点进行排序</span></span><br><span class="line">        dis = [(graph.get_heuristic(node, goal), node) <span class="keyword">for</span> node <span class="keyword">in</span> new_nodes]</span><br><span class="line">        dis = sorted(dis)</span><br><span class="line">        <span class="comment"># 取距离最近的顶点</span></span><br><span class="line">        new_nodes = [node[<span class="number">1</span>] <span class="keyword">for</span> node <span class="keyword">in</span> dis]</span><br><span class="line">        new_paths = [curr_path + (node,) <span class="keyword">for</span> node <span class="keyword">in</span> new_nodes]</span><br><span class="line">        new_paths.extend(pathlist)</span><br><span class="line">        pathlist = new_paths</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> []</span><br></pre></td></tr></table></figure>
<p>爬山法运行起来要比DFS效率要高，因为省去了一些顶点，但是缺点在于容易陷入局部最优，而不能找到全局最优的路径。</p>
<h5 id="集束搜索（Beam-Search）"><a href="#集束搜索（Beam-Search）" class="headerlink" title="集束搜索（Beam Search）"></a>集束搜索（Beam Search）</h5><p>前面的爬山法是在DFS算法基础上改进的，而集束搜索是在BFS算法基础上改进的。BFS会遍历每一层级的所有顶点，而集束搜索会选择前k个离目标顶点的启发式距离最近的顶点进行遍历，k称为集束宽度（beam width）。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">beam_search</span><span class="params">(graph, start, goal, beam_width)</span>:</span></span><br><span class="line">    pathlist = [(start,)]</span><br><span class="line">    <span class="keyword">if</span> start == goal:</span><br><span class="line">        <span class="keyword">return</span> [start]</span><br><span class="line">    <span class="keyword">while</span> len(pathlist) &gt; <span class="number">0</span>:</span><br><span class="line">        new_paths = []</span><br><span class="line">        <span class="comment"># 取前beam_width个路径</span></span><br><span class="line">        pathlist = pathlist[:beam_width]</span><br><span class="line">        <span class="keyword">while</span> len(pathlist) &gt; <span class="number">0</span>:</span><br><span class="line">            curr_path = pathlist.pop(<span class="number">0</span>)</span><br><span class="line">            curr_node = curr_path[<span class="number">-1</span>]</span><br><span class="line">            new_nodes = graph.get_connected_nodes(curr_node)</span><br><span class="line">            <span class="keyword">if</span> len(curr_path) &gt; <span class="number">1</span>:</span><br><span class="line">                new_nodes = [node <span class="keyword">for</span> node <span class="keyword">in</span> new_nodes <span class="keyword">if</span> node <span class="keyword">not</span> <span class="keyword">in</span> curr_path]</span><br><span class="line">            <span class="keyword">if</span> goal <span class="keyword">in</span> new_nodes:</span><br><span class="line">                goal_path = curr_path + (goal,)</span><br><span class="line">                <span class="keyword">return</span> list(goal_path)</span><br><span class="line">            new_paths += [curr_path + (node,) <span class="keyword">for</span> node <span class="keyword">in</span> new_nodes]</span><br><span class="line">        pathlist.extend(new_paths)</span><br><span class="line">        <span class="comment"># sort pathlist</span></span><br><span class="line">        dis = [(graph.get_heuristic(path[<span class="number">-1</span>], goal), path) <span class="keyword">for</span> path <span class="keyword">in</span> pathlist]</span><br><span class="line">        dis = sorted(dis)</span><br><span class="line">        pathlist = [path[<span class="number">1</span>] <span class="keyword">for</span> path <span class="keyword">in</span> dis]</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> []</span><br></pre></td></tr></table></figure>
<h5 id="分支界定法（branch-and-bound）"><a href="#分支界定法（branch-and-bound）" class="headerlink" title="分支界定法（branch and bound）"></a>分支界定法（branch and bound）</h5><p>前面的两种算法都是基于启发式距离，而路径最短往往说的是实际的路径长度，分支界定法就是基于实际的路径长度，每次选路径最短进行遍历。因此需要一个方法来统计到目前顶点的累积路径长度：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">path_length</span><span class="params">(graph, node_names)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    node_names:遍历过的顶点</span></span><br><span class="line"><span class="string">    return:返回累积路径长度</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    length = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> i &lt; len(node_names) - <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">if</span> graph.are_connected(node_names[i], node_names[i+<span class="number">1</span>]):</span><br><span class="line">            length += graph.get_edge(node_names[i], node_names[i+<span class="number">1</span>]).length</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> length</span><br></pre></td></tr></table></figure>
<p>同时，只需将爬山法的对启发式距离排序的程序改为对所有路径按累积路径长度进行排序即可，代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">branch_and_bound</span><span class="params">(graph, start, goal)</span>:</span></span><br><span class="line">    pathlist = [(start,)]</span><br><span class="line">    <span class="keyword">if</span> start == goal:</span><br><span class="line">        <span class="keyword">return</span> [start]</span><br><span class="line">    goal_path = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">while</span> len(pathlist) &gt; <span class="number">0</span>:</span><br><span class="line">        curr_path = pathlist.pop(<span class="number">0</span>)</span><br><span class="line">        curr_node = curr_path[<span class="number">-1</span>]</span><br><span class="line">        new_nodes = graph.get_connected_nodes(curr_node)</span><br><span class="line">        <span class="keyword">if</span> len(curr_path) &gt; <span class="number">1</span>:</span><br><span class="line">            new_nodes = [node <span class="keyword">for</span> node <span class="keyword">in</span> new_nodes <span class="keyword">if</span> node <span class="keyword">not</span> <span class="keyword">in</span> curr_path]</span><br><span class="line">        <span class="keyword">if</span> goal <span class="keyword">in</span> new_nodes <span class="keyword">and</span> goal_path:</span><br><span class="line">            new_goal_path = curr_path + (goal,)</span><br><span class="line">            <span class="keyword">if</span> path_length(graph, new_goal_path) &lt; path_length(graph, goal_path):</span><br><span class="line">                goal_path = new_goal_path</span><br><span class="line">        <span class="keyword">elif</span> goal <span class="keyword">in</span> new_nodes <span class="keyword">and</span> <span class="keyword">not</span> goal_path:</span><br><span class="line">            goal_path = curr_path + (goal,)</span><br><span class="line">        new_paths = [curr_path + (node,) <span class="keyword">for</span> node <span class="keyword">in</span> new_nodes]</span><br><span class="line">        new_paths.extend(pathlist)</span><br><span class="line">        pathlist = new_paths</span><br><span class="line">        <span class="comment"># 按路径长度对所有路径进行排序</span></span><br><span class="line">        dis = [(path_length(graph, path), path) <span class="keyword">for</span> path <span class="keyword">in</span> pathlist]</span><br><span class="line">        dis = sorted(dis)</span><br><span class="line">        <span class="comment"># 取最短的路径</span></span><br><span class="line">        pathlist = [path[<span class="number">1</span>] <span class="keyword">for</span> path <span class="keyword">in</span> dis]</span><br><span class="line">    <span class="keyword">if</span> goal_path:</span><br><span class="line">        <span class="keyword">return</span> list(goal_path)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> []</span><br></pre></td></tr></table></figure>
<h5 id="A-算法"><a href="#A-算法" class="headerlink" title="A* 算法"></a>A* 算法</h5><p>A*算法是同时基于累积路径长度和启发式距离的算法，对于程序的编写，我们只需在上述代码中增加启发式距离即可：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">a_star</span><span class="params">(graph, start, goal)</span>:</span></span><br><span class="line">    pathlist = [(start,)]</span><br><span class="line">    <span class="keyword">if</span> start == goal:</span><br><span class="line">        <span class="keyword">return</span> [start]</span><br><span class="line">    goal_path = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">while</span> len(pathlist) &gt; <span class="number">0</span> <span class="keyword">and</span> <span class="keyword">not</span> goal_path:</span><br><span class="line">        curr_path = pathlist.pop(<span class="number">0</span>)</span><br><span class="line">        curr_node = curr_path[<span class="number">-1</span>]</span><br><span class="line">        new_nodes = graph.get_connected_nodes(curr_node)</span><br><span class="line">        <span class="keyword">if</span> len(curr_path) &gt; <span class="number">1</span>:</span><br><span class="line">            new_nodes = [node <span class="keyword">for</span> node <span class="keyword">in</span> new_nodes <span class="keyword">if</span> node <span class="keyword">not</span> <span class="keyword">in</span> curr_path]</span><br><span class="line">        <span class="keyword">if</span> goal <span class="keyword">in</span> new_nodes:</span><br><span class="line">            goal_path = curr_path + (goal,)</span><br><span class="line">        new_paths = [curr_path + (node,) <span class="keyword">for</span> node <span class="keyword">in</span> new_nodes]</span><br><span class="line">        new_paths.extend(pathlist)</span><br><span class="line">        pathlist = new_paths</span><br><span class="line">        <span class="comment"># 按 累计长度+启发式距离 对所有路径进行排序</span></span><br><span class="line">        dis = [(path_length(graph, path)+graph.get_heuristic(path[<span class="number">-1</span>], goal),</span><br><span class="line">                path) <span class="keyword">for</span> path <span class="keyword">in</span> pathlist]</span><br><span class="line">        dis = sorted(dis)</span><br><span class="line">        pathlist = [path[<span class="number">1</span>] <span class="keyword">for</span> path <span class="keyword">in</span> dis]</span><br><span class="line">    <span class="keyword">if</span> goal_path:</span><br><span class="line">        <span class="keyword">return</span> list(goal_path)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> []</span><br></pre></td></tr></table></figure>
<p>A<em> 算法是其中效率比较高的算法，但其运行的好坏，在于启发式距离设定的好坏。如果get_heuristic(node, goal)始终小于等于节点node到goal的实际距离，则A</em>算法保证一定能够找到最短路径。但是当get_heuristic(node, goal)的值越小，算法将遍历越多的节点，也就导致算法越慢。</p>
<p>如果get_heuristic(node, goal)大于节点node到goal的实际距离，则A*算法不能保证找到最短路径，不过此时会很快。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/08/09/DFS/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Bania">
      <meta itemprop="description" content="A platform for discussing programming and technology">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BaniaBlog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/08/09/DFS/" class="post-title-link" itemprop="url">DFS</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-08-09 16:28:51" itemprop="dateCreated datePublished" datetime="2020-08-09T16:28:51+08:00">2020-08-09</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="DFS-深度优先搜索"><a href="#DFS-深度优先搜索" class="headerlink" title="DFS(深度优先搜索)"></a>DFS(深度优先搜索)</h3><p>DFS也是一种常见的图的搜索算法，与BFS不同，DFS不按层级遍历，而是从一条路径遍历到末尾，再返回原来的路径，选择另一路径继续相同的操作，直到全部遍历完成，或到达停止的条件，这一过程类似于走迷宫一样。因此我们还可以用递归的方法实现DFS算法。</p>
<p>在BFS算法中，我们借助了队列的数据结构，遵循先进先出原则，从而实现对每一层级的遍历。与之相反，在DFS，我们需要借助堆栈的数据结构，遵循后进先出原则，从而实现深度优先。</p>
<p>非递归方法的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dfs_1</span><span class="params">(graph, start)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    return:返回依次序遍历的顶点</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    visited, stack = set(), [start]</span><br><span class="line">    <span class="keyword">while</span> stack:</span><br><span class="line">        vertex = stack.pop()</span><br><span class="line">        <span class="keyword">if</span> vertex <span class="keyword">not</span> <span class="keyword">in</span> visited:</span><br><span class="line">            visited.add(vertex)</span><br><span class="line">            stack.extend(set(graph.get_connected_nodes(vertex)) - visited)</span><br><span class="line">    <span class="keyword">return</span> visited</span><br></pre></td></tr></table></figure>
<p>递归方法的代码如下：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dfs_rec</span><span class="params">(graph, node, visited=None)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    return:返回依次序遍历的顶点</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> visited <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        visited = []</span><br><span class="line">    <span class="keyword">if</span> node <span class="keyword">not</span> <span class="keyword">in</span> visited:</span><br><span class="line">        visited.append(node)</span><br><span class="line">        <span class="keyword">for</span> neighbour <span class="keyword">in</span> graph.get_connected_nodes(node):</span><br><span class="line">            dfs_rec(graph, neighbour, visited)</span><br><span class="line">    <span class="keyword">return</span> visited</span><br></pre></td></tr></table></figure>
<p>如果需要寻找两个顶点间的最短路径，可以采用类似在BFS中找最短路径的方法，代码结构基本一致，其中的细节需要做如下改动：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dfs_path</span><span class="params">(graph, start, goal)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param graph:</span></span><br><span class="line"><span class="string">    :param start:</span></span><br><span class="line"><span class="string">    :param goal:</span></span><br><span class="line"><span class="string">    :return: return the path from start to goal, when the goal is found, the</span></span><br><span class="line"><span class="string">    programme will stop.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    pathlist=[(start,)]</span><br><span class="line">    <span class="keyword">if</span> start == goal:</span><br><span class="line">        <span class="keyword">return</span> [start]</span><br><span class="line">    <span class="keyword">while</span> len(pathlist) &gt; <span class="number">0</span>:</span><br><span class="line">        curr_path = pathlist.pop(<span class="number">0</span>)</span><br><span class="line">        curr_node = curr_path[<span class="number">-1</span>]</span><br><span class="line">        new_nodes = graph.get_connected_nodes(curr_node)</span><br><span class="line">        <span class="keyword">if</span> len(curr_path) &gt; <span class="number">1</span>:</span><br><span class="line">            new_nodes = [node <span class="keyword">for</span> node <span class="keyword">in</span> new_nodes <span class="keyword">if</span> node <span class="keyword">not</span> <span class="keyword">in</span> curr_path]</span><br><span class="line">        <span class="keyword">if</span> goal <span class="keyword">in</span> new_nodes:</span><br><span class="line">            goal_path = curr_path + (goal, )</span><br><span class="line">            <span class="keyword">return</span> goal_path</span><br><span class="line">        new_paths = [curr_path + (node, ) <span class="keyword">for</span> node <span class="keyword">in</span> new_nodes]</span><br><span class="line">        new_paths.extend(pathlist)</span><br><span class="line">        pathlist = new_paths</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> []</span><br></pre></td></tr></table></figure>
<p>同时，如果需要找到所有的路径，基本思路与BFS的类似：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dfs_all_path</span><span class="params">(graph, start, goal)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    :param graph:</span></span><br><span class="line"><span class="string">    :param start:</span></span><br><span class="line"><span class="string">    :param goal:</span></span><br><span class="line"><span class="string">    :return: return the list of all the paths from start to goal</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    stack = [(start, [start])]</span><br><span class="line">    <span class="keyword">while</span> stack:</span><br><span class="line">        (vertex, path) = stack.pop()</span><br><span class="line">        <span class="keyword">for</span> next <span class="keyword">in</span> set(graph.get_connected_nodes(vertex)) - set(path):</span><br><span class="line">            <span class="keyword">if</span> next == goal:</span><br><span class="line">                <span class="keyword">yield</span> path + [next]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                stack.append((next, path + [next]))</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/08/09/BFS/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Bania">
      <meta itemprop="description" content="A platform for discussing programming and technology">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BaniaBlog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/08/09/BFS/" class="post-title-link" itemprop="url">BFS</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-08-09 11:53:30" itemprop="dateCreated datePublished" datetime="2020-08-09T11:53:30+08:00">2020-08-09</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h3 id="BFS-广度优先搜索"><a href="#BFS-广度优先搜索" class="headerlink" title="BFS(广度优先搜索)"></a>BFS(广度优先搜索)</h3><p>图的应用中，最重要的一部分就是图的搜索和遍历，图的搜索算法有很多，而BFS是其中一种比较简单的搜索算法。算法核心在于，从初始顶点开始，一层一层遍历搜索顶点，直到所有顶点搜索完毕，如下图所示：</p>
<p><img src="/2020/08/09/BFS/BFS.png" alt="img"></p>
<p>实现这个算法，我们首先定义level字典，保存每个vertex的层级信息，frontier列表用来保存上一层级遍历过的vertex，parent字典用来保存每个vertex的上一个vertex。为此代码的思路如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">初始化：level，frontier，parent 保存初始顶点</span><br><span class="line">遍历frontier的所有顶点：</span><br><span class="line">	定义next列表，保存下一次需要遍历的顶点</span><br><span class="line">	遍历所有与frontier中顶点相连的其他顶点：</span><br><span class="line">		如果不在level中即没有遍历：</span><br><span class="line">			更新level，parent，next</span><br><span class="line">	frontier &#x3D; next</span><br><span class="line">	层级加一</span><br></pre></td></tr></table></figure>
<p>因此我们的代码如下</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bfs_1</span><span class="params">(graph, start)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    return: 被遍历的顶点次序和level字典</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    level = &#123;start: <span class="number">0</span>&#125;</span><br><span class="line">    parent = &#123;start: <span class="literal">None</span>&#125;</span><br><span class="line">    i = <span class="number">1</span></span><br><span class="line">    frontier = [start]</span><br><span class="line">    node_visited = [start]</span><br><span class="line">    <span class="keyword">while</span> frontier:</span><br><span class="line">        next = []</span><br><span class="line">        <span class="keyword">for</span> u <span class="keyword">in</span> frontier:</span><br><span class="line">            <span class="keyword">for</span> v <span class="keyword">in</span> graph.get_connected_nodes(u):</span><br><span class="line">                <span class="keyword">if</span> v <span class="keyword">not</span> <span class="keyword">in</span> level:</span><br><span class="line">                    level[v] = i</span><br><span class="line">                    parent[v] = u</span><br><span class="line">                    next.append(v)</span><br><span class="line">                    node_visited.append(v)</span><br><span class="line">        frontier = next</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> node_visited, level</span><br></pre></td></tr></table></figure>
<p>根据BFS的核心思想，我们可以对代码进行不同版本的修改，比如利用队列的特性，可以将上述代码简化，结果返回遍历顶点的次序。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bfs_2</span><span class="params">(graph, start)</span>:</span></span><br><span class="line">    explore = []</span><br><span class="line">    queue = [start]</span><br><span class="line">    <span class="keyword">while</span> queue:</span><br><span class="line">        node = queue.pop(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span> node <span class="keyword">not</span> <span class="keyword">in</span> explore:</span><br><span class="line">            explore.append(node)</span><br><span class="line">            <span class="keyword">for</span> neighbor <span class="keyword">in</span> graph.get_connected_nodes(node):</span><br><span class="line">                queue.append(neighbor)</span><br><span class="line">    <span class="keyword">return</span> explore</span><br></pre></td></tr></table></figure>
<p>如果需要用BFS找到两点之间的最短路径，可以将第一版的代码修改如下：</p>
<p>在for循环中增加是否找到目标顶点的判断条件，如果是则结束遍历，reach_goal标志设为true，最后利用parent字典，反向遍历找到start到goal的路径。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bfs_path</span><span class="params">(graph, start, goal)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> start == goal:</span><br><span class="line">        <span class="keyword">return</span> [start]</span><br><span class="line">    level = &#123;start: <span class="number">0</span>&#125;</span><br><span class="line">    parent = &#123;start: <span class="literal">None</span>&#125;</span><br><span class="line">    i = <span class="number">1</span></span><br><span class="line">    frontier = [start]</span><br><span class="line">    node_visited = [start]</span><br><span class="line">    reach_goal = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">while</span> frontier:</span><br><span class="line">        next = []</span><br><span class="line">        <span class="keyword">for</span> u <span class="keyword">in</span> frontier:</span><br><span class="line">            <span class="keyword">if</span> goal <span class="keyword">in</span> graph.get_connected_nodes(u):</span><br><span class="line">                parent[goal] = u</span><br><span class="line">                reach_goal = <span class="literal">True</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">for</span> v <span class="keyword">in</span> graph.get_connected_nodes(u):</span><br><span class="line">                <span class="keyword">if</span> v <span class="keyword">not</span> <span class="keyword">in</span> level:</span><br><span class="line">                    level[v] = i</span><br><span class="line">                    parent[v] = u</span><br><span class="line">                    next.append(v)</span><br><span class="line">                    node_visited.append(v)</span><br><span class="line">        frontier = next</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> reach_goal:</span><br><span class="line">        pathlist = []</span><br><span class="line">        temp = goal</span><br><span class="line">        <span class="keyword">while</span> temp:</span><br><span class="line">            pathlist.append(temp)</span><br><span class="line">            temp = parent[temp]</span><br><span class="line">        pathlist.reverse()</span><br><span class="line">        <span class="keyword">return</span> pathlist</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> []</span><br></pre></td></tr></table></figure>
<p>第二种方法没有用parent字典，它是保存当前的路径，从而无需对parent字典反向遍历。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bfs_path_2</span><span class="params">(graph, start, goal)</span>:</span></span><br><span class="line">    pathlist = [(start,)]</span><br><span class="line">    <span class="keyword">if</span> start == goal:</span><br><span class="line">        <span class="keyword">return</span> [start]</span><br><span class="line">    <span class="keyword">while</span> len(pathlist) &gt; <span class="number">0</span>:</span><br><span class="line">        new_paths = []</span><br><span class="line">        <span class="keyword">while</span> len(pathlist) &gt; <span class="number">0</span>:</span><br><span class="line">            curr_path = pathlist.pop(<span class="number">0</span>)</span><br><span class="line">            curr_node = curr_path[<span class="number">-1</span>]</span><br><span class="line">            new_nodes = graph.get_connected_nodes(curr_node)</span><br><span class="line">            <span class="comment"># 更新顶点信息</span></span><br><span class="line">            <span class="keyword">if</span> len(curr_path) &gt; <span class="number">1</span>:</span><br><span class="line">                new_nodes = [node <span class="keyword">for</span> node <span class="keyword">in</span> new_nodes <span class="keyword">if</span> node <span class="keyword">not</span> <span class="keyword">in</span> curr_path]</span><br><span class="line">            <span class="keyword">if</span> goal <span class="keyword">in</span> new_nodes:</span><br><span class="line">                goal_path = curr_path + (goal,)</span><br><span class="line">                <span class="keyword">return</span> goal_path</span><br><span class="line">            <span class="comment"># 更新路径信息</span></span><br><span class="line">            new_paths += [curr_path + (node,) <span class="keyword">for</span> node <span class="keyword">in</span> new_nodes]</span><br><span class="line">        pathlist.extend(new_paths)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> []</span><br></pre></td></tr></table></figure>
<p>如果需要找到从初始顶点到目标顶点的所有路径，我们只需修改找到目标顶点的判断条件即可，使程序继续执行，这里我们可以利用yield关键字，同时利用集合的差集运算，去掉已经遍历过的顶点。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bfs_allpaths</span><span class="params">(graph, start, goal)</span>:</span></span><br><span class="line">    queue = [(start, [start])]</span><br><span class="line">    <span class="keyword">while</span> queue:</span><br><span class="line">        (vertex, path) = queue.pop(<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">for</span> next <span class="keyword">in</span> set(graph.get_connected_nodes(vertex)) - set(path):</span><br><span class="line">            <span class="keyword">if</span> next == goal:</span><br><span class="line">                <span class="keyword">yield</span> path + [next]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                queue.append((next, path + [next]))</span><br></pre></td></tr></table></figure>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/08/09/Graph/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Bania">
      <meta itemprop="description" content="A platform for discussing programming and technology">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BaniaBlog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/08/09/Graph/" class="post-title-link" itemprop="url">Graph</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-08-09 11:02:21 / Modified: 11:02:22" itemprop="dateCreated datePublished" datetime="2020-08-09T11:02:21+08:00">2020-08-09</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="图（Graph）的表示"><a href="#图（Graph）的表示" class="headerlink" title="图（Graph）的表示"></a>图（Graph）的表示</h4><h5 id="1-图的概念"><a href="#1-图的概念" class="headerlink" title="1.图的概念"></a>1.图的概念</h5><p>图是一种重要的数据结构，在解决实际问题中也经常用到这种数据结构，其基本表示为G=(V, E)，V(vectex)是图的顶点，E(edge)表示图的边。我们的地图在计算机中就可以表示成一个Graph，不同的标志性地点为vectex，地点之间的路表示为edge，这样就可以方便操作实际中的地图去解决一些复杂的问题，比如找两个地点之间的最短路径，旅行商问题等等。</p>
<p>图分为有向图和无向图，有向图指的是各个顶点之间有一定的方向，一个顶点到另一个顶点需要按照给定的方向进行移动；而无向图指的是顶点之间没有固定方向。如下图所示，第一幅图为无向图，第二幅图为有向图。在有向图中，顶点1指向顶点2，说明1可以移向2，但是2不能移向1。</p>
<p><img src="/2020/08/09/Graph/有向图.png" alt="img"></p>
<p><img src="/2020/08/09/Graph/无向图.png" alt="img"></p>
<h5 id="2-图的表示"><a href="#2-图的表示" class="headerlink" title="2.图的表示"></a>2.图的表示</h5><p>图的表示就是表示清楚顶点信息和边的信息。有两种表示方法，邻接链表和邻接矩阵。如上面两张图可分别表示如下：</p>
<p><img src="/2020/08/09/Graph/图的表示1.png" alt="img"></p>
<p><img src="/2020/08/09/Graph/图的表示2.png" alt="img"></p>
<p>在计算机中，我们可以使用一些数据结构来简化图的表达，比如在Python中，我们可以用下面代码来表示一个图：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">graph = &#123;<span class="string">'A'</span>: set([<span class="string">'B'</span>, <span class="string">'C'</span>]),</span><br><span class="line">         <span class="string">'B'</span>: set([<span class="string">'A'</span>, <span class="string">'D'</span>, <span class="string">'E'</span>]),</span><br><span class="line">         <span class="string">'C'</span>: set([<span class="string">'A'</span>, <span class="string">'F'</span>]),</span><br><span class="line">         <span class="string">'D'</span>: set([<span class="string">'B'</span>]),</span><br><span class="line">         <span class="string">'E'</span>: set([<span class="string">'B'</span>, <span class="string">'F'</span>]),</span><br><span class="line">         <span class="string">'F'</span>: set([<span class="string">'C'</span>, <span class="string">'E'</span>])&#125;</span><br></pre></td></tr></table></figure>
<p>graph是一个字典，其中keys是所有的vertices，values是与该顶点连接的其他顶点。如果我们需要获取所有的顶点，那么可以利用字典的keys()方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph.keys()</span><br><span class="line"></span><br><span class="line">output: dict_keys([<span class="string">'A'</span>, <span class="string">'B'</span>, <span class="string">'C'</span>, <span class="string">'D'</span>, <span class="string">'E'</span>, <span class="string">'F'</span>])</span><br></pre></td></tr></table></figure>
<p>获取与某个顶点相连的其他顶点，就可直接利用字典的取值方法：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">graph[<span class="string">'A'</span>]</span><br><span class="line"></span><br><span class="line">output: &#123;<span class="string">'B'</span>, <span class="string">'C'</span>&#125;</span><br></pre></td></tr></table></figure>
<p>但是如果图中的每一个边有权，我们称之为有权图时，利用上述的表示方法不能很方便地得到权重信息，为此我们可以封装一个类，定义获取边，权重等相关信息的函数。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Fall 2012 6.034 Lab 2: Search</span></span><br><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> reduce</span><br><span class="line"></span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    set()</span><br><span class="line"><span class="keyword">except</span> NameError:</span><br><span class="line">    <span class="keyword">from</span> sets <span class="keyword">import</span> Set <span class="keyword">as</span> set, ImmutableSet <span class="keyword">as</span> frozenset</span><br><span class="line"></span><br><span class="line">NAME=<span class="string">"NAME"</span></span><br><span class="line">NODE1=<span class="string">"NODE1"</span></span><br><span class="line">NODE2=<span class="string">"NODE2"</span></span><br><span class="line">VAL=<span class="string">"LENGTH"</span></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Edge</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, name, node1, node2, length)</span>:</span></span><br><span class="line">        self.name = name</span><br><span class="line">        self.node1 = node1</span><br><span class="line">        self.node2 = node2</span><br><span class="line">        self.length = length</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__repr__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">'Edge '</span> + self.name + \</span><br><span class="line">               <span class="string">' from '</span> + self.node1 + <span class="string">' to '</span> + self.node2 + \</span><br><span class="line">               <span class="string">' with length '</span> + str(self.length)</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Graph</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, nodes=None, edgesdict=None, heuristic=None,</span></span></span><br><span class="line"><span class="function"><span class="params">                 edges=None)</span>:</span></span><br><span class="line">        <span class="string">'''specify EITHER edgesdict OR edges'''</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> edges:</span><br><span class="line">            self.edges = edges</span><br><span class="line">        <span class="keyword">elif</span> edgesdict:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                self.edges = [Edge(e[<span class="string">'NAME'</span>], e[<span class="string">'NODE1'</span>], e[<span class="string">'NODE2'</span>], e[<span class="string">'LENGTH'</span>])\</span><br><span class="line">                              <span class="keyword">for</span> e <span class="keyword">in</span> edgesdict]</span><br><span class="line">            <span class="keyword">except</span> KeyError:</span><br><span class="line">                self.edges = [Edge(e[<span class="string">'name'</span>], e[<span class="string">'node1'</span>], e[<span class="string">'node2'</span>], e[<span class="string">'length'</span>])\</span><br><span class="line">                              <span class="keyword">for</span> e <span class="keyword">in</span> edgesdict]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.edges = []</span><br><span class="line">        self.nodes = nodes</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> nodes:</span><br><span class="line">            self.nodes = list(set([edge.node1 <span class="keyword">for</span> edge <span class="keyword">in</span> self.edges] + </span><br><span class="line">                                  [edge.node2 <span class="keyword">for</span> edge <span class="keyword">in</span> self.edges]))</span><br><span class="line">        <span class="comment"># heuristic is a dictionary where heuristic[G][S] is the</span></span><br><span class="line">        <span class="comment">#  heuristic distance from S to G</span></span><br><span class="line">        self.heuristic = heuristic</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> heuristic:</span><br><span class="line">            self.heuristic = &#123;&#125;</span><br><span class="line">        self.validate()</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">validate</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> name <span class="keyword">in</span> self.nodes:</span><br><span class="line">            <span class="keyword">assert</span> isinstance(name, str), str(type(name))+<span class="string">": "</span>+str(name)</span><br><span class="line">        <span class="keyword">assert</span> len(self.nodes) == len(set(self.nodes)), <span class="string">"no duplicate nodes"</span></span><br><span class="line">        edgenames = [edge.name <span class="keyword">for</span> edge <span class="keyword">in</span> self.edges]</span><br><span class="line">        <span class="keyword">assert</span> len(edgenames) == len(set(edgenames)), <span class="string">"no duplicate edges"</span></span><br><span class="line">        <span class="keyword">for</span> edge <span class="keyword">in</span> self.edges:</span><br><span class="line">            <span class="keyword">assert</span> isinstance(edge.name, str), type(edge.name)</span><br><span class="line">            <span class="keyword">assert</span> edge.node1 <span class="keyword">in</span> self.nodes</span><br><span class="line">            <span class="keyword">assert</span> edge.node2 <span class="keyword">in</span> self.nodes</span><br><span class="line">            <span class="keyword">assert</span> edge.length &gt; <span class="number">0</span>, <span class="string">"positive edges only today"</span></span><br><span class="line">        <span class="keyword">for</span> start <span class="keyword">in</span> self.nodes:</span><br><span class="line">            <span class="keyword">for</span> end <span class="keyword">in</span> self.nodes:</span><br><span class="line">                <span class="keyword">assert</span> self.get_heuristic(start,end) &gt;= <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_connected_nodes</span><span class="params">(self, node)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        gets a list of all node id values connected to a given node.</span></span><br><span class="line"><span class="string">        'node' should be a node name, not a dictionary.</span></span><br><span class="line"><span class="string">        The return value is a list of node names.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">assert</span> node <span class="keyword">in</span> self.nodes, <span class="string">"No node "</span>+str(node)+<span class="string">" in graph "</span>+str(self)</span><br><span class="line">        result = [x.node2 <span class="keyword">for</span> x <span class="keyword">in</span> self.edges <span class="keyword">if</span> x.node1 == node]</span><br><span class="line">        result += [x.node1 <span class="keyword">for</span> x <span class="keyword">in</span> self.edges <span class="keyword">if</span> x.node2 == node]</span><br><span class="line">        <span class="keyword">return</span> sorted(result)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_edge</span><span class="params">(self, node1, node2)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        checks the list of edges and returns an edge if</span></span><br><span class="line"><span class="string">        both connected nodes are part of the edge, or 'None' otherwise.</span></span><br><span class="line"><span class="string">        'node1' and 'node2' are names of nodes, not 'NODE' dictionaries.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">assert</span> node1 <span class="keyword">in</span> self.nodes, <span class="string">"No node "</span>+str(node1)+<span class="string">" in graph "</span>+str(self)</span><br><span class="line">        <span class="keyword">assert</span> node2 <span class="keyword">in</span> self.nodes, <span class="string">"No node "</span>+str(node2)+<span class="string">" in graph "</span>+str(self)</span><br><span class="line">        node_names = ( node1, node2 )</span><br><span class="line">        <span class="keyword">for</span> edge <span class="keyword">in</span> self.edges:</span><br><span class="line">            <span class="keyword">if</span> ((edge.node1, edge.node2) == node_names <span class="keyword">or</span> </span><br><span class="line">                (edge.node2, edge.node1) == node_names):</span><br><span class="line">                <span class="keyword">return</span> edge</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">are_connected</span><span class="params">(self, node1, node2)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        checks if two edges are connected.</span></span><br><span class="line"><span class="string">        'node1' and 'node2' are names of nodes, not 'NODE' dictionaries.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> bool(self.get_edge(node1, node2) )</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">get_heuristic</span><span class="params">(self, start, goal)</span>:</span></span><br><span class="line">        <span class="string">""" Return the value of the heuristic from the start to the goal"""</span></span><br><span class="line">        <span class="keyword">assert</span> start <span class="keyword">in</span> self.nodes, <span class="string">"No node "</span>+str(start)+<span class="string">" in graph "</span>+str(self)</span><br><span class="line">        <span class="keyword">assert</span> goal <span class="keyword">in</span> self.nodes, <span class="string">"No node "</span>+str(goal)+<span class="string">" in graph "</span>+str(self)</span><br><span class="line">        <span class="keyword">if</span> goal <span class="keyword">in</span> self.heuristic:</span><br><span class="line">            <span class="keyword">if</span> start <span class="keyword">in</span> self.heuristic[goal]:</span><br><span class="line">                <span class="keyword">return</span> self.heuristic[goal][start]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="number">0</span> <span class="comment"># we have checked that everything is positive</span></span><br><span class="line">        <span class="keyword">else</span>: </span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span> <span class="comment"># we have checked that everything is positive</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">is_valid_path</span><span class="params">(self, path)</span>:</span></span><br><span class="line">        <span class="function"><span class="keyword">def</span> <span class="title">is_valid_path_reducer</span><span class="params">(elt_a, elt_b)</span>:</span></span><br><span class="line">            <span class="keyword">if</span> elt_a == <span class="literal">False</span> <span class="keyword">or</span> <span class="keyword">not</span> self.are_connected(elt_a, elt_b):</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> elt_b</span><br><span class="line">        <span class="keyword">return</span> (reduce(is_valid_path_reducer, path) != <span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">add_edge</span><span class="params">(self, node1, node2, length, name=None)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> node1 <span class="keyword">not</span> <span class="keyword">in</span> self.nodes:</span><br><span class="line">            self.nodes.append(node1)</span><br><span class="line">        <span class="keyword">if</span> node2 <span class="keyword">not</span> <span class="keyword">in</span> self.nodes:</span><br><span class="line">            self.nodes.append(node2)</span><br><span class="line">        <span class="keyword">if</span> name == <span class="literal">None</span>:</span><br><span class="line">            name = (<span class="string">"%s %s"</span> % (node1, node2))</span><br><span class="line">        self.edges.append(Edge(name, node1, node2, length))</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">set_heuristic</span><span class="params">(self, start, goal, value)</span>:</span></span><br><span class="line">        <span class="keyword">if</span> goal <span class="keyword">not</span> <span class="keyword">in</span> self.heuristic:</span><br><span class="line">            self.heuristic[goal] = &#123;&#125;</span><br><span class="line">        self.heuristic[goal][start] = value</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__str__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> <span class="string">"Graph: \n  edges="</span>+str(self.edges)+<span class="string">"\n  heuristic="</span>+str(self.heuristic)</span><br></pre></td></tr></table></figure>
<p>根据init方法的定义，一个图可以如下定义：</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">AGRAPH = Graph(nodes = [<span class="string">'S'</span>, <span class="string">'A'</span>, <span class="string">'B'</span>, <span class="string">'C'</span>, <span class="string">'G'</span>],</span><br><span class="line">               edgesdict = [&#123;<span class="string">'NAME'</span>: <span class="string">'eSA'</span>, <span class="string">'LENGTH'</span>: <span class="number">3</span>, <span class="string">'NODE1'</span>: <span class="string">'S'</span>, <span class="string">'NODE2'</span>: <span class="string">'A'</span>&#125;,</span><br><span class="line">                            &#123;<span class="string">'NAME'</span>: <span class="string">'eSB'</span>, <span class="string">'LENGTH'</span>: <span class="number">1</span>, <span class="string">'NODE1'</span>: <span class="string">'S'</span>, <span class="string">'NODE2'</span>: <span class="string">'B'</span>&#125;,</span><br><span class="line">                            &#123;<span class="string">'NAME'</span>: <span class="string">'eAB'</span>, <span class="string">'LENGTH'</span>: <span class="number">1</span>, <span class="string">'NODE1'</span>: <span class="string">'A'</span>, <span class="string">'NODE2'</span>: <span class="string">'B'</span>&#125;,</span><br><span class="line">                            &#123;<span class="string">'NAME'</span>: <span class="string">'eAC'</span>, <span class="string">'LENGTH'</span>: <span class="number">1</span>, <span class="string">'NODE1'</span>: <span class="string">'A'</span>, <span class="string">'NODE2'</span>: <span class="string">'C'</span>&#125;,</span><br><span class="line">                            &#123;<span class="string">'NAME'</span>: <span class="string">'eCG'</span>, <span class="string">'LENGTH'</span>: <span class="number">10</span>, <span class="string">'NODE1'</span>: <span class="string">'C'</span>, <span class="string">'NODE2'</span>: <span class="string">'G'</span>&#125;],</span><br><span class="line">               heuristic = &#123;<span class="string">'G'</span>:&#123;<span class="string">'S'</span>: <span class="number">12</span>,</span><br><span class="line">                                 <span class="string">'A'</span>: <span class="number">9</span>,</span><br><span class="line">                                 <span class="string">'B'</span>: <span class="number">12</span>,</span><br><span class="line">                                 <span class="string">'C'</span>: <span class="number">8</span>,</span><br><span class="line">                                 <span class="string">'G'</span>: <span class="number">0</span>&#125;&#125;)</span><br></pre></td></tr></table></figure>
<p>heuristic是启发式的距离，在A*算法中会使用到这个属性，这里不做过多的描述。我们只需明白图的表示可以很灵活多样，但关键在于把需要的信息表达清楚即可，特别是顶点和边的信息。对于不同问题，我们可以使用不同的数据结构和算法对图进行表示。</p>
<h5 id="参考资料："><a href="#参考资料：" class="headerlink" title="参考资料："></a>参考资料：</h5><p>1.算法导论第三版</p>
<p>2.MIT Artificial Intelligence open course Assignment lab2 code </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/02/Dropout/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Bania">
      <meta itemprop="description" content="A platform for discussing programming and technology">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BaniaBlog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/02/Dropout/" class="post-title-link" itemprop="url">Dropout</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-03-02 10:33:41 / Modified: 11:45:39" itemprop="dateCreated datePublished" datetime="2020-03-02T10:33:41+08:00">2020-03-02</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>In previous tasks, we only used <strong>L2</strong> regularization to prevent overfitting. It can be implemented by penalizing the squared magnitude of all parameters directly in the objective. And here we will learn another way of regularization: Dropout. It is a effective, simple regularization technique. The idea of Dropout is that we randomly choose the inputs while training, and then we create a thinner inputs for training. Note that for testing process we should not implement Dropout.  For more information, you can click the link and read the paper: <a href="http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf" target="_blank" rel="noopener">http://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf</a></p>
<p>The implementation of Dropout in Python is quite straightforward. We can use <code>np.random.binomial()</code> method to create a binary matrix with the shape of inputs then the some elements of inputs can be randomly set to zero. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout_forward</span><span class="params">(x, dropout_param)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Performs the forward pass for (inverted) dropout.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: Input data, of any shape</span></span><br><span class="line"><span class="string">    - dropout_param: A dictionary with the following keys:</span></span><br><span class="line"><span class="string">      - p: Dropout parameter. We drop each neuron output with probability p.</span></span><br><span class="line"><span class="string">      - mode: 'test' or 'train'. If the mode is train, then perform dropout;</span></span><br><span class="line"><span class="string">        if the mode is test, then just return the input.</span></span><br><span class="line"><span class="string">      - seed: Seed for the random number generator. Passing seed makes this</span></span><br><span class="line"><span class="string">        function deterministic, which is needed for gradient checking but not</span></span><br><span class="line"><span class="string">        in real networks.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Outputs:</span></span><br><span class="line"><span class="string">    - out: Array of the same shape as x.</span></span><br><span class="line"><span class="string">    - cache: tuple (dropout_param, mask). In training mode, mask is the dropout</span></span><br><span class="line"><span class="string">      mask that was used to multiply the input; in test mode, mask is None.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    p, mode = dropout_param[<span class="string">'p'</span>], dropout_param[<span class="string">'mode'</span>]</span><br><span class="line">    <span class="keyword">if</span> <span class="string">'seed'</span> <span class="keyword">in</span> dropout_param:</span><br><span class="line">        np.random.seed(dropout_param[<span class="string">'seed'</span>])</span><br><span class="line"></span><br><span class="line">    mask = <span class="literal">None</span></span><br><span class="line">    out = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">'train'</span>:</span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Implement training phase forward pass for inverted dropout.   #</span></span><br><span class="line">        <span class="comment"># Store the dropout mask in the mask variable.                        #</span></span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        mask = np.random.binomial(<span class="number">1</span>, p, size=x.shape)</span><br><span class="line">        out = x * mask</span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment">#                           END OF YOUR CODE                          #</span></span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">    <span class="keyword">elif</span> mode == <span class="string">'test'</span>:</span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Implement the test phase forward pass for inverted dropout.   #</span></span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        out = x </span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment">#                            END OF YOUR CODE                         #</span></span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line"></span><br><span class="line">    cache = (dropout_param, mask)</span><br><span class="line">    out = out.astype(x.dtype, copy=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">dropout_backward</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Perform the backward pass for (inverted) dropout.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivatives, of any shape</span></span><br><span class="line"><span class="string">    - cache: (dropout_param, mask) from dropout_forward.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dropout_param, mask = cache</span><br><span class="line">    mode = dropout_param[<span class="string">'mode'</span>]</span><br><span class="line"></span><br><span class="line">    dx = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">'train'</span>:</span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Implement training phase backward pass for inverted dropout   #</span></span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        dx = dout * mask</span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment">#                          END OF YOUR CODE                           #</span></span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">    <span class="keyword">elif</span> mode == <span class="string">'test'</span>:</span><br><span class="line">        dx = dout</span><br><span class="line">    <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure>
<p>Then you can see how Dropout influences the performance of training. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train two identical nets, one with dropout and one without</span></span><br><span class="line">np.random.seed(<span class="number">231</span>)</span><br><span class="line">num_train = <span class="number">500</span></span><br><span class="line">small_data = &#123;</span><br><span class="line">  <span class="string">'X_train'</span>: data[<span class="string">'X_train'</span>][:num_train],</span><br><span class="line">  <span class="string">'y_train'</span>: data[<span class="string">'y_train'</span>][:num_train],</span><br><span class="line">  <span class="string">'X_val'</span>: data[<span class="string">'X_val'</span>],</span><br><span class="line">  <span class="string">'y_val'</span>: data[<span class="string">'y_val'</span>],</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">solvers = &#123;&#125;</span><br><span class="line">dropout_choices = [<span class="number">0</span>, <span class="number">0.75</span>]</span><br><span class="line"><span class="keyword">for</span> dropout <span class="keyword">in</span> dropout_choices:</span><br><span class="line">  model = FullyConnectedNet([<span class="number">500</span>], dropout=dropout)</span><br><span class="line">  print(dropout)</span><br><span class="line"></span><br><span class="line">  solver = Solver(model, small_data,</span><br><span class="line">                  num_epochs=<span class="number">25</span>, batch_size=<span class="number">100</span>,</span><br><span class="line">                  update_rule=<span class="string">'adam'</span>,</span><br><span class="line">                  optim_config=&#123;</span><br><span class="line">                    <span class="string">'learning_rate'</span>: <span class="number">5e-4</span>,</span><br><span class="line">                  &#125;,</span><br><span class="line">                  verbose=<span class="literal">True</span>, print_every=<span class="number">100</span>)</span><br><span class="line">  solver.train()</span><br><span class="line">  solvers[dropout] = solver</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_accs = []</span><br><span class="line">val_accs = []</span><br><span class="line"><span class="keyword">for</span> dropout <span class="keyword">in</span> dropout_choices:</span><br><span class="line">  solver = solvers[dropout]</span><br><span class="line">  train_accs.append(solver.train_acc_history[<span class="number">-1</span>])</span><br><span class="line">  val_accs.append(solver.val_acc_history[<span class="number">-1</span>])</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"><span class="keyword">for</span> dropout <span class="keyword">in</span> dropout_choices:</span><br><span class="line">  plt.plot(solvers[dropout].train_acc_history, <span class="string">'o'</span>, label=<span class="string">'%.2f dropout'</span> % dropout)</span><br><span class="line">plt.title(<span class="string">'Train accuracy'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epoch'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Accuracy'</span>)</span><br><span class="line">plt.legend(ncol=<span class="number">2</span>, loc=<span class="string">'lower right'</span>)</span><br><span class="line">  </span><br><span class="line">plt.subplot(<span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line"><span class="keyword">for</span> dropout <span class="keyword">in</span> dropout_choices:</span><br><span class="line">  plt.plot(solvers[dropout].val_acc_history, <span class="string">'o'</span>, label=<span class="string">'%.2f dropout'</span> % dropout)</span><br><span class="line">plt.title(<span class="string">'Val accuracy'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epoch'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Accuracy'</span>)</span><br><span class="line">plt.legend(ncol=<span class="number">2</span>, loc=<span class="string">'lower right'</span>)</span><br><span class="line"></span><br><span class="line">plt.gcf().set_size_inches(<span class="number">15</span>, <span class="number">15</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/03/02/Dropout/1583120497975.png" alt="1583120497975"></p>
<p>Obviously, both 0.75-Dropout and 0.00-Dropout do well in the training process with similar accuracy, while in testing process, 0.75-Dropout does better than 0.00-Dropout, which means it can effectively eliminate overfitting. </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/01/BatchNormalization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Bania">
      <meta itemprop="description" content="A platform for discussing programming and technology">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BaniaBlog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/03/01/BatchNormalization/" class="post-title-link" itemprop="url">BatchNormalization</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-03-01 17:15:29 / Modified: 19:47:28" itemprop="dateCreated datePublished" datetime="2020-03-01T17:15:29+08:00">2020-03-01</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h4 id="Weight-Initialization"><a href="#Weight-Initialization" class="headerlink" title="Weight Initialization"></a>Weight Initialization</h4><p>In last exercise, we learn how to write modular code for the Two-Layer neural network, which makes it easier to build a neural-network model. Today, we will dive deep into it and talk about Batch Normalization which is a method to deal with the problems associated with weights initialization. </p>
<p>Note that we do not know the final value of every weight , but it is more reasonable to assume that approximately half of the weights will be positive and half of them will be negative. Thus, a reasonable-sounding idea might be set all the weights to zero. This turns out to be a mistake because if every neuron in the network computes the same output, then the same gradients during back propagation will be obtained and parameters updates will be the same. </p>
<p>Therefore, we want the weights to be random but very close to zero. The implementation for one weight matrix might look like<code>W = 0.01*np.random.randn(D, H)</code>, then we can get a weight matrix with zero mean and unit standard deviation. </p>
<p>However, here comes the problem of weights initialization that with more layers, we have to be more careful to set the learning rate and the learning rate usually should be very small. This can waste a lot of time for training. We can see this problem when we used a five-layers network to train 50 images in last exercise. This is because the distribution of each layer’s input changes during training as the parameters of the previous layers every time we initialize change. We refer this phenomenon as <strong>internal covariate shift</strong> and we address this problem by normalizing layer inputs or we can call it Batch normalization. </p>
<h4 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h4><p>When we use Batch Normalization, we make it as a part of the model architecture and perform the normalization for each training mini-batch. Batch Normalization allows us to use larger learning rates and be less careful about initialization. The implementation of Batch Normalization is quite straightforward. You can click the link for more information about it :<a href="https://arxiv.org/abs/1502.03167" target="_blank" rel="noopener">https://arxiv.org/abs/1502.03167</a></p>
<p>The forward batch normalization algorithm is like that: </p>
<p><img src="/2020/03/01/BatchNormalization/1583061383633.png" alt="1583061383633"> </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_forward</span><span class="params">(x, gamma, beta, bn_param)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Forward pass for batch normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    During training the sample mean and (uncorrected) sample variance are</span></span><br><span class="line"><span class="string">    computed from minibatch statistics and used to normalize the incoming data.</span></span><br><span class="line"><span class="string">    During training we also keep an exponentially decaying running mean of the</span></span><br><span class="line"><span class="string">    mean and variance of each feature, and these averages are used to normalize</span></span><br><span class="line"><span class="string">    data at test-time.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    At each timestep we update the running averages for mean and variance using</span></span><br><span class="line"><span class="string">    an exponential decay based on the momentum parameter:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    running_mean = momentum * running_mean + (1 - momentum) * sample_mean</span></span><br><span class="line"><span class="string">    running_var = momentum * running_var + (1 - momentum) * sample_var</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Note that the batch normalization paper suggests a different test-time</span></span><br><span class="line"><span class="string">    behavior: they compute sample mean and variance for each feature using a</span></span><br><span class="line"><span class="string">    large number of training images rather than using a running average. For</span></span><br><span class="line"><span class="string">    this implementation we have chosen to use running averages instead since</span></span><br><span class="line"><span class="string">    they do not require an additional estimation step; the torch7</span></span><br><span class="line"><span class="string">    implementation of batch normalization also uses running averages.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">    - x: Data of shape (N, D)</span></span><br><span class="line"><span class="string">    - gamma: Scale parameter of shape (D,)</span></span><br><span class="line"><span class="string">    - beta: Shift paremeter of shape (D,)</span></span><br><span class="line"><span class="string">    - bn_param: Dictionary with the following keys:</span></span><br><span class="line"><span class="string">      - mode: 'train' or 'test'; required</span></span><br><span class="line"><span class="string">      - eps: Constant for numeric stability</span></span><br><span class="line"><span class="string">      - momentum: Constant for running mean / variance.</span></span><br><span class="line"><span class="string">      - running_mean: Array of shape (D,) giving running mean of features</span></span><br><span class="line"><span class="string">      - running_var Array of shape (D,) giving running variance of features</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: of shape (N, D)</span></span><br><span class="line"><span class="string">    - cache: A tuple of values needed in the backward pass</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    mode = bn_param[<span class="string">'mode'</span>]</span><br><span class="line">    eps = bn_param.get(<span class="string">'eps'</span>, <span class="number">1e-5</span>)</span><br><span class="line">    momentum = bn_param.get(<span class="string">'momentum'</span>, <span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">    N, D = x.shape</span><br><span class="line">    running_mean = bn_param.get(<span class="string">'running_mean'</span>, np.zeros(D, dtype=x.dtype))</span><br><span class="line">    running_var = bn_param.get(<span class="string">'running_var'</span>, np.zeros(D, dtype=x.dtype))</span><br><span class="line"></span><br><span class="line">    out, cache = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">'train'</span>:</span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Implement the training-time forward pass for batch norm.      #</span></span><br><span class="line">        <span class="comment"># Use minibatch statistics to compute the mean and variance, use      #</span></span><br><span class="line">        <span class="comment"># these statistics to normalize the incoming data, and scale and      #</span></span><br><span class="line">        <span class="comment"># shift the normalized data using gamma and beta.                     #</span></span><br><span class="line">        <span class="comment">#                                                                     #</span></span><br><span class="line">        <span class="comment"># You should store the output in the variable out. Any intermediates  #</span></span><br><span class="line">        <span class="comment"># that you need for the backward pass should be stored in the cache   #</span></span><br><span class="line">        <span class="comment"># variable.                                                           #</span></span><br><span class="line">        <span class="comment">#                                                                     #</span></span><br><span class="line">        <span class="comment"># You should also use your computed sample mean and variance together #</span></span><br><span class="line">        <span class="comment"># with the momentum variable to update the running mean and running   #</span></span><br><span class="line">        <span class="comment"># variance, storing your result in the running_mean and running_var   #</span></span><br><span class="line">        <span class="comment"># variables.                                                          #</span></span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        </span><br><span class="line">        x_mean = np.mean(x, axis=<span class="number">0</span>)</span><br><span class="line">        x_var = np.mean((x - x_mean)**<span class="number">2</span>, axis=<span class="number">0</span>)</span><br><span class="line">        x_hat = (x - x_mean) / np.sqrt(x_var + eps)</span><br><span class="line">        out = gamma*x_hat + beta</span><br><span class="line">        </span><br><span class="line">        running_mean = momentum * running_mean + (<span class="number">1</span> - momentum) * x_mean</span><br><span class="line">        running_var = momentum * running_var + (<span class="number">1</span> - momentum) * x_var</span><br><span class="line">        </span><br><span class="line">        cache =( x,x_hat, gamma,  x_mean, x_var, eps)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment">#                           END OF YOUR CODE                          #</span></span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">    <span class="keyword">elif</span> mode == <span class="string">'test'</span>:</span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Implement the test-time forward pass for batch normalization. #</span></span><br><span class="line">        <span class="comment"># Use the running mean and variance to normalize the incoming data,   #</span></span><br><span class="line">        <span class="comment"># then scale and shift the normalized data using gamma and beta.      #</span></span><br><span class="line">        <span class="comment"># Store the result in the out variable.                               #</span></span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        x_hat = (x - running_mean)/np.sqrt(running_var + eps)</span><br><span class="line">        out = gamma*x_hat + beta</span><br><span class="line">        cahe =( x, x_hat,gamma, running_mean, running_var, eps)</span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment">#                          END OF YOUR CODE                           #</span></span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">'Invalid forward batchnorm mode "%s"'</span> % mode)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Store the updated running means back into bn_param</span></span><br><span class="line">    bn_param[<span class="string">'running_mean'</span>] = running_mean</span><br><span class="line">    bn_param[<span class="string">'running_var'</span>] = running_var</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure>
<p>For the backward algorithm, the paper has calculate the gradients for us and we can use these equations to write the code.</p>
<p><img src="/2020/03/01/BatchNormalization/1583062312769.png" alt="1583062312769"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_backward</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Backward pass for batch normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    For this implementation, you should write out a computation graph for</span></span><br><span class="line"><span class="string">    batch normalization on paper and propagate gradients backward through</span></span><br><span class="line"><span class="string">    intermediate nodes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivatives, of shape (N, D)</span></span><br><span class="line"><span class="string">    - cache: Variable of intermediates from batchnorm_forward.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradient with respect to inputs x, of shape (N, D)</span></span><br><span class="line"><span class="string">    - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)</span></span><br><span class="line"><span class="string">    - dbeta: Gradient with respect to shift parameter beta, of shape (D,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dx, dgamma, dbeta = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the backward pass for batch normalization. Store the    #</span></span><br><span class="line">    <span class="comment"># results in the dx, dgamma, and dbeta variables.                         #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    x,x_hat,gamma,mean, var ,eps = cache </span><br><span class="line">    dx_hat =dout *gamma</span><br><span class="line">    dvar = np.sum(dx_hat * (x - mean)*(var + eps)**(<span class="number">-3</span>/<span class="number">2</span>)/(<span class="number">-2</span>), axis=<span class="number">0</span>)</span><br><span class="line">    dmean = np.sum(-dx_hat/np.sqrt(var+eps), axis=<span class="number">0</span>) + dvar*np.mean(<span class="number">-2</span>*(x-mean))</span><br><span class="line">    dx = dx_hat/np.sqrt(var+eps)+dvar*<span class="number">2</span>*(x-mean)/x.shape[<span class="number">0</span>]+dmean/x.shape[<span class="number">0</span>]</span><br><span class="line">    dgamma = np.sum(dout*x_hat, axis=<span class="number">0</span>)</span><br><span class="line">    dbeta = np.sum(dout, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dx, dgamma, dbeta</span><br></pre></td></tr></table></figure>
<p>Then, we can add batch layer into the neural networks and we can see the improved performance after implementing Batch Normalization. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">231</span>)</span><br><span class="line"><span class="comment"># Try training a very deep net with batchnorm</span></span><br><span class="line">hidden_dims = [<span class="number">100</span>, <span class="number">100</span>, <span class="number">100</span>, <span class="number">100</span>, <span class="number">100</span>]</span><br><span class="line"></span><br><span class="line">num_train = <span class="number">1000</span></span><br><span class="line">small_data = &#123;</span><br><span class="line">  <span class="string">'X_train'</span>: data[<span class="string">'X_train'</span>][:num_train],</span><br><span class="line">  <span class="string">'y_train'</span>: data[<span class="string">'y_train'</span>][:num_train],</span><br><span class="line">  <span class="string">'X_val'</span>: data[<span class="string">'X_val'</span>],</span><br><span class="line">  <span class="string">'y_val'</span>: data[<span class="string">'y_val'</span>],</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">weight_scale = <span class="number">2e-2</span></span><br><span class="line">bn_model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, use_batchnorm=<span class="literal">True</span>)</span><br><span class="line">model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, use_batchnorm=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">bn_solver = Solver(bn_model, small_data,</span><br><span class="line">                num_epochs=<span class="number">10</span>, batch_size=<span class="number">50</span>,</span><br><span class="line">                update_rule=<span class="string">'adam'</span>,</span><br><span class="line">                optim_config=&#123;</span><br><span class="line">                  <span class="string">'learning_rate'</span>: <span class="number">1e-3</span>,</span><br><span class="line">                &#125;,</span><br><span class="line">                verbose=<span class="literal">True</span>, print_every=<span class="number">200</span>)</span><br><span class="line">bn_solver.train()</span><br><span class="line"></span><br><span class="line">solver = Solver(model, small_data,</span><br><span class="line">                num_epochs=<span class="number">10</span>, batch_size=<span class="number">50</span>,</span><br><span class="line">                update_rule=<span class="string">'adam'</span>,</span><br><span class="line">                optim_config=&#123;</span><br><span class="line">                  <span class="string">'learning_rate'</span>: <span class="number">1e-3</span>,</span><br><span class="line">                &#125;,</span><br><span class="line">                verbose=<span class="literal">True</span>, print_every=<span class="number">200</span>)</span><br><span class="line">solver.train()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/03/01/BatchNormalization/1583062559917.png" alt="1583062559917"></p>
<p><img src="/2020/03/01/BatchNormalization/1583062577631.png" alt="1583062577631"></p>
<p><img src="/2020/03/01/BatchNormalization/1583062593173.png" alt="1583062593173"></p>
<p><img src="/2020/03/01/BatchNormalization/1583062611523.png" alt="1583062611523"></p>
<p>With Batch Normalization, the model can train faster than baseline. Also we can see the initialization of weights in different situations can achieve better performance than the model without Batch Normalization.  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">231</span>)</span><br><span class="line"><span class="comment"># Try training a very deep net with batchnorm</span></span><br><span class="line">hidden_dims = [<span class="number">50</span>, <span class="number">50</span>, <span class="number">50</span>, <span class="number">50</span>, <span class="number">50</span>, <span class="number">50</span>, <span class="number">50</span>]</span><br><span class="line"></span><br><span class="line">num_train = <span class="number">1000</span></span><br><span class="line">small_data = &#123;</span><br><span class="line">  <span class="string">'X_train'</span>: data[<span class="string">'X_train'</span>][:num_train],</span><br><span class="line">  <span class="string">'y_train'</span>: data[<span class="string">'y_train'</span>][:num_train],</span><br><span class="line">  <span class="string">'X_val'</span>: data[<span class="string">'X_val'</span>],</span><br><span class="line">  <span class="string">'y_val'</span>: data[<span class="string">'y_val'</span>],</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">bn_solvers = &#123;&#125;</span><br><span class="line">solvers = &#123;&#125;</span><br><span class="line">weight_scales = np.logspace(<span class="number">-4</span>, <span class="number">0</span>, num=<span class="number">20</span>)</span><br><span class="line"><span class="keyword">for</span> i, weight_scale <span class="keyword">in</span> enumerate(weight_scales):</span><br><span class="line">  print(<span class="string">'Running weight scale %d / %d'</span> % (i + <span class="number">1</span>, len(weight_scales)))</span><br><span class="line">  bn_model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, use_batchnorm=<span class="literal">True</span>)</span><br><span class="line">  model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, use_batchnorm=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">  bn_solver = Solver(bn_model, small_data,</span><br><span class="line">                  num_epochs=<span class="number">10</span>, batch_size=<span class="number">50</span>,</span><br><span class="line">                  update_rule=<span class="string">'adam'</span>,</span><br><span class="line">                  optim_config=&#123;</span><br><span class="line">                    <span class="string">'learning_rate'</span>: <span class="number">1e-3</span>,</span><br><span class="line">                  &#125;,</span><br><span class="line">                  verbose=<span class="literal">False</span>, print_every=<span class="number">200</span>)</span><br><span class="line">  bn_solver.train()</span><br><span class="line">  bn_solvers[weight_scale] = bn_solver</span><br><span class="line"></span><br><span class="line">  solver = Solver(model, small_data,</span><br><span class="line">                  num_epochs=<span class="number">10</span>, batch_size=<span class="number">50</span>,</span><br><span class="line">                  update_rule=<span class="string">'adam'</span>,</span><br><span class="line">                  optim_config=&#123;</span><br><span class="line">                    <span class="string">'learning_rate'</span>: <span class="number">1e-3</span>,</span><br><span class="line">                  &#125;,</span><br><span class="line">                  verbose=<span class="literal">False</span>, print_every=<span class="number">200</span>)</span><br><span class="line">  solver.train()</span><br><span class="line">  solvers[weight_scale] = solver</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot results of weight scale experiment</span></span><br><span class="line">best_train_accs, bn_best_train_accs = [], []</span><br><span class="line">best_val_accs, bn_best_val_accs = [], []</span><br><span class="line">final_train_loss, bn_final_train_loss = [], []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ws <span class="keyword">in</span> weight_scales:</span><br><span class="line">  best_train_accs.append(max(solvers[ws].train_acc_history))</span><br><span class="line">  bn_best_train_accs.append(max(bn_solvers[ws].train_acc_history))</span><br><span class="line">  </span><br><span class="line">  best_val_accs.append(max(solvers[ws].val_acc_history))</span><br><span class="line">  bn_best_val_accs.append(max(bn_solvers[ws].val_acc_history))</span><br><span class="line">  </span><br><span class="line">  final_train_loss.append(np.mean(solvers[ws].loss_history[<span class="number">-100</span>:]))</span><br><span class="line">  bn_final_train_loss.append(np.mean(bn_solvers[ws].loss_history[<span class="number">-100</span>:]))</span><br><span class="line">  </span><br><span class="line">plt.subplot(<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">'Best val accuracy vs weight initialization scale'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Weight initialization scale'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Best val accuracy'</span>)</span><br><span class="line">plt.semilogx(weight_scales, best_val_accs, <span class="string">'-o'</span>, label=<span class="string">'baseline'</span>)</span><br><span class="line">plt.semilogx(weight_scales, bn_best_val_accs, <span class="string">'-o'</span>, label=<span class="string">'batchnorm'</span>)</span><br><span class="line">plt.legend(ncol=<span class="number">2</span>, loc=<span class="string">'lower right'</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">'Best train accuracy vs weight initialization scale'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Weight initialization scale'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Best training accuracy'</span>)</span><br><span class="line">plt.semilogx(weight_scales, best_train_accs, <span class="string">'-o'</span>, label=<span class="string">'baseline'</span>)</span><br><span class="line">plt.semilogx(weight_scales, bn_best_train_accs, <span class="string">'-o'</span>, label=<span class="string">'batchnorm'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">plt.title(<span class="string">'Final training loss vs weight initialization scale'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Weight initialization scale'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Final training loss'</span>)</span><br><span class="line">plt.semilogx(weight_scales, final_train_loss, <span class="string">'-o'</span>, label=<span class="string">'baseline'</span>)</span><br><span class="line">plt.semilogx(weight_scales, bn_final_train_loss, <span class="string">'-o'</span>, label=<span class="string">'batchnorm'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.gca().set_ylim(<span class="number">1.0</span>, <span class="number">3.5</span>)</span><br><span class="line"></span><br><span class="line">plt.gcf().set_size_inches(<span class="number">10</span>, <span class="number">15</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/03/01/BatchNormalization/1583063126691.png" alt="1583063126691"></p>
<p><img src="/2020/03/01/BatchNormalization/1583063140003.png" alt="1583063140003"></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/24/Update-rules/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Bania">
      <meta itemprop="description" content="A platform for discussing programming and technology">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BaniaBlog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/24/Update-rules/" class="post-title-link" itemprop="url">Update rules</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-02-24 17:13:24 / Modified: 20:28:52" itemprop="dateCreated datePublished" datetime="2020-02-24T17:13:24+08:00">2020-02-24</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>In previous exercises, we have used vanilla SGD as our update rule. More sophisticated update rules can make it easier to train deep networks. We will implement a few of the most commonly used update rules and compare them to vanilla SGD.</p>
<h4 id="SGD-Momentum"><a href="#SGD-Momentum" class="headerlink" title="SGD+Momentum"></a>SGD+Momentum</h4><p>In SGD, the update rule is :$W = W-\alpha dW​$ where $\alpha​$ is the learning rate and also the key to determining the convergence of the loss function and the training speed. If $\alpha​$ is small, we need more steps to get the minimum loss while if it is too large, the model will not converge. That is the reason why we should be very careful to set the learning rate when we use SGD and sometimes it is quite hard to find the appropriate learning rate. Fortunately, we have a method that can solve the disadvantages of SGD. </p>
<p>Stochastic gradient descent with momentum is a widely used update rule that tends to make deep networks converge faster than vanilla SGD.  The update rule of this method is :</p>
<script type="math/tex; mode=display">
V_t=\gamma V_{t-1} - \alpha dW_{t-1}</script><script type="math/tex; mode=display">
W_t=W_{t-1}+V_t</script><p>where the momentum term $\gamma$ is usually set to 0.9 or a similar value. </p>
<p>The momentum part here is essential to make the learning process converge faster because it increases for dimensions whose gradients point in the same directions and reduces updates for dimensions whose gradients change directions. </p>
<p>In python, we have below code:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sgd_momentum</span><span class="params">(w, dw, config=None)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Performs stochastic gradient descent with momentum.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    config format:</span></span><br><span class="line"><span class="string">    - learning_rate: Scalar learning rate.</span></span><br><span class="line"><span class="string">    - momentum: Scalar between 0 and 1 giving the momentum value.</span></span><br><span class="line"><span class="string">      Setting momentum = 0 reduces to sgd.</span></span><br><span class="line"><span class="string">    - velocity: A numpy array of the same shape as w and dw used to store a</span></span><br><span class="line"><span class="string">      moving average of the gradients.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> config <span class="keyword">is</span> <span class="literal">None</span>: config = &#123;&#125;</span><br><span class="line">    config.setdefault(<span class="string">'learning_rate'</span>, <span class="number">1e-2</span>)</span><br><span class="line">    config.setdefault(<span class="string">'momentum'</span>, <span class="number">0.9</span>)</span><br><span class="line">    v = config.get(<span class="string">'velocity'</span>, np.zeros_like(w))</span><br><span class="line"></span><br><span class="line">    next_w = <span class="literal">None</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the momentum update formula. Store the updated value in #</span></span><br><span class="line">    <span class="comment"># the next_w variable. You should also use and update the velocity v.     #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    v = config[<span class="string">'momentum'</span>] * v - config[<span class="string">'learning_rate'</span>]*dw</span><br><span class="line">    next_w = w + v</span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    config[<span class="string">'velocity'</span>] = v</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> next_w, config</span><br></pre></td></tr></table></figure>
<p>Then we can see its performance compared to SGD.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">num_train = <span class="number">4000</span></span><br><span class="line">small_data = &#123;</span><br><span class="line">  <span class="string">'X_train'</span>: data[<span class="string">'X_train'</span>][:num_train],</span><br><span class="line">  <span class="string">'y_train'</span>: data[<span class="string">'y_train'</span>][:num_train],</span><br><span class="line">  <span class="string">'X_val'</span>: data[<span class="string">'X_val'</span>],</span><br><span class="line">  <span class="string">'y_val'</span>: data[<span class="string">'y_val'</span>],</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">solvers = &#123;&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> update_rule <span class="keyword">in</span> [<span class="string">'sgd'</span>, <span class="string">'sgd_momentum'</span>]:</span><br><span class="line">  print(<span class="string">'running with '</span>, update_rule)</span><br><span class="line">  model = FullyConnectedNet([<span class="number">100</span>, <span class="number">100</span>, <span class="number">100</span>, <span class="number">100</span>, <span class="number">100</span>], weight_scale=<span class="number">5e-2</span>)</span><br><span class="line"></span><br><span class="line">  solver = Solver(model, small_data,</span><br><span class="line">                  num_epochs=<span class="number">5</span>, batch_size=<span class="number">100</span>,</span><br><span class="line">                  update_rule=update_rule,</span><br><span class="line">                  optim_config=&#123;</span><br><span class="line">                    <span class="string">'learning_rate'</span>: <span class="number">1e-2</span>,</span><br><span class="line">                  &#125;,</span><br><span class="line">                  verbose=<span class="literal">True</span>)</span><br><span class="line">  solvers[update_rule] = solver</span><br><span class="line">  solver.train()</span><br><span class="line">  print()</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">'Training loss'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Iteration'</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">'Training accuracy'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epoch'</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">plt.title(<span class="string">'Validation accuracy'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epoch'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> update_rule, solver <span class="keyword">in</span> list(solvers.items()):</span><br><span class="line">  plt.subplot(<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">  plt.plot(solver.loss_history, <span class="string">'o'</span>, label=update_rule)</span><br><span class="line">  </span><br><span class="line">  plt.subplot(<span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">  plt.plot(solver.train_acc_history, <span class="string">'-o'</span>, label=update_rule)</span><br><span class="line"></span><br><span class="line">  plt.subplot(<span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">  plt.plot(solver.val_acc_history, <span class="string">'-o'</span>, label=update_rule)</span><br><span class="line">  </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]:</span><br><span class="line">  plt.subplot(<span class="number">3</span>, <span class="number">1</span>, i)</span><br><span class="line">  plt.legend(loc=<span class="string">'upper center'</span>, ncol=<span class="number">4</span>)</span><br><span class="line">plt.gcf().set_size_inches(<span class="number">15</span>, <span class="number">15</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/02/24/Update-rules/1582538833839.png" alt="1582538833839"></p>
<p><img src="/2020/02/24/Update-rules/1582538861111.png" alt="1582538861111"></p>
<p>Apparently, SGD+Momentum converges faster than SGD because with the same epoch, the former can achieve higher accuracy and have less loss. </p>
<h4 id="RMSprop"><a href="#RMSprop" class="headerlink" title="RMSprop"></a>RMSprop</h4><p>RMSprop is a method that can change the learning rate automatically and make it fit the model well. </p>
<script type="math/tex; mode=display">
\hat W_t = \chi \hat W_{t-1} + (1-\chi)dW_{t-1}^2</script><script type="math/tex; mode=display">
W_t = W_{t-1} - \frac{\alpha}{\sqrt{ \hat W_t + \epsilon }}dW_{t-1}</script><p>where $\chi$ is the decay rate and usually set to 0.9; learning rate $\alpha$ is usually 0.01 . In python, we have:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">rmsprop</span><span class="params">(x, dx, config=None)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Uses the RMSProp update rule, which uses a moving average of squared</span></span><br><span class="line"><span class="string">    gradient values to set adaptive per-parameter learning rates.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    config format:</span></span><br><span class="line"><span class="string">    - learning_rate: Scalar learning rate.</span></span><br><span class="line"><span class="string">    - decay_rate: Scalar between 0 and 1 giving the decay rate for the squared</span></span><br><span class="line"><span class="string">      gradient cache.</span></span><br><span class="line"><span class="string">    - epsilon: Small scalar used for smoothing to avoid dividing by zero.</span></span><br><span class="line"><span class="string">    - cache: Moving average of second moments of gradients.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> config <span class="keyword">is</span> <span class="literal">None</span>: config = &#123;&#125;</span><br><span class="line">    config.setdefault(<span class="string">'learning_rate'</span>, <span class="number">1e-2</span>)</span><br><span class="line">    config.setdefault(<span class="string">'decay_rate'</span>, <span class="number">0.99</span>)</span><br><span class="line">    config.setdefault(<span class="string">'epsilon'</span>, <span class="number">1e-8</span>)</span><br><span class="line">    config.setdefault(<span class="string">'cache'</span>, np.zeros_like(x))</span><br><span class="line"></span><br><span class="line">    next_x = <span class="literal">None</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the RMSprop update formula, storing the next value of x #</span></span><br><span class="line">    <span class="comment"># in the next_x variable. Don't forget to update cache value stored in    #</span></span><br><span class="line">    <span class="comment"># config['cache'].                                                        #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    config[<span class="string">'cache'</span>] = config[<span class="string">'decay_rate'</span>]*config[<span class="string">'cache'</span>]+(<span class="number">1</span>-config[<span class="string">'decay_rate'</span>])*np.square(dx)</span><br><span class="line">    next_x = x -  config[<span class="string">'learning_rate'</span>]*dx / (np.sqrt(config[<span class="string">'cache'</span>])+config[<span class="string">'epsilon'</span>])</span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> next_x, config</span><br></pre></td></tr></table></figure>
<h4 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h4><p>Adaptive Moment Estimation  is another way that computes adaptive learning rate for each parameter. The algorithm is quite like combining Momentum and RMSprop. </p>
<script type="math/tex; mode=display">
m_t = \beta_1 m_{t-1} + (1-\beta_1) dW_{t-1}</script><script type="math/tex; mode=display">
v_t = \beta_2 v_{t-1} + (1-\beta_2) dW_{t-1}^2</script><script type="math/tex; mode=display">
\hat m_t = \frac{m_t}{1-\beta_1^t}</script><script type="math/tex; mode=display">
\hat v_t = \frac{v_t}{1-\beta_2^t}</script><script type="math/tex; mode=display">
W_t = W_{t-1} - \frac{\alpha \hat m_t}{\sqrt{\hat v_t +\epsilon}}</script><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">adam</span><span class="params">(x, dx, config=None)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Uses the Adam update rule, which incorporates moving averages of both the</span></span><br><span class="line"><span class="string">    gradient and its square and a bias correction term.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    config format:</span></span><br><span class="line"><span class="string">    - learning_rate: Scalar learning rate.</span></span><br><span class="line"><span class="string">    - beta1: Decay rate for moving average of first moment of gradient.</span></span><br><span class="line"><span class="string">    - beta2: Decay rate for moving average of second moment of gradient.</span></span><br><span class="line"><span class="string">    - epsilon: Small scalar used for smoothing to avoid dividing by zero.</span></span><br><span class="line"><span class="string">    - m: Moving average of gradient.</span></span><br><span class="line"><span class="string">    - v: Moving average of squared gradient.</span></span><br><span class="line"><span class="string">    - t: Iteration number.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> config <span class="keyword">is</span> <span class="literal">None</span>: config = &#123;&#125;</span><br><span class="line">    config.setdefault(<span class="string">'learning_rate'</span>, <span class="number">1e-3</span>)</span><br><span class="line">    config.setdefault(<span class="string">'beta1'</span>, <span class="number">0.9</span>)</span><br><span class="line">    config.setdefault(<span class="string">'beta2'</span>, <span class="number">0.999</span>)</span><br><span class="line">    config.setdefault(<span class="string">'epsilon'</span>, <span class="number">1e-8</span>)</span><br><span class="line">    config.setdefault(<span class="string">'m'</span>, np.zeros_like(x))</span><br><span class="line">    config.setdefault(<span class="string">'v'</span>, np.zeros_like(x))</span><br><span class="line">    config.setdefault(<span class="string">'t'</span>, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    next_x = <span class="literal">None</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the Adam update formula, storing the next value of x in #</span></span><br><span class="line">    <span class="comment"># the next_x variable. Don't forget to update the m, v, and t variables   #</span></span><br><span class="line">    <span class="comment"># stored in config.                                                       #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    config[<span class="string">'t'</span>] +=<span class="number">1</span></span><br><span class="line">    config[<span class="string">'m'</span>] = config[<span class="string">'beta1'</span>]*config[<span class="string">'m'</span>] + (<span class="number">1</span>-config[<span class="string">'beta1'</span>])*dx</span><br><span class="line">    config[<span class="string">'v'</span>] = config[<span class="string">'beta2'</span>]*config[<span class="string">'v'</span>] + (<span class="number">1</span>-config[<span class="string">'beta2'</span>])*np.square(dx)</span><br><span class="line">        </span><br><span class="line">    m_temp = config[<span class="string">'m'</span>]/(<span class="number">1</span>- np.power(config[<span class="string">'beta1'</span>], config[<span class="string">'t'</span>]))</span><br><span class="line">    v_temp = config[<span class="string">'v'</span>]/(<span class="number">1</span>- np.power(config[<span class="string">'beta2'</span>] ,config[<span class="string">'t'</span>]))</span><br><span class="line">    </span><br><span class="line">    next_x = x-config[<span class="string">'learning_rate'</span>] * m_temp / (np.sqrt(v_temp)+config[<span class="string">'epsilon'</span>])</span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> next_x, config</span><br></pre></td></tr></table></figure>
<h4 id="Comparison"><a href="#Comparison" class="headerlink" title="Comparison"></a>Comparison</h4><p>Finally, we can compare these rules by seeing how they perform on the data. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">learning_rates = &#123;<span class="string">'rmsprop'</span>: <span class="number">1e-4</span>, <span class="string">'adam'</span>: <span class="number">1e-3</span>&#125;</span><br><span class="line"><span class="keyword">for</span> update_rule <span class="keyword">in</span> [<span class="string">'adam'</span>, <span class="string">'rmsprop'</span>]:</span><br><span class="line">  print(<span class="string">'running with '</span>, update_rule)</span><br><span class="line">  model = FullyConnectedNet([<span class="number">100</span>, <span class="number">100</span>, <span class="number">100</span>, <span class="number">100</span>, <span class="number">100</span>], weight_scale=<span class="number">5e-2</span>)</span><br><span class="line"></span><br><span class="line">  solver = Solver(model, small_data,</span><br><span class="line">                  num_epochs=<span class="number">5</span>, batch_size=<span class="number">100</span>,</span><br><span class="line">                  update_rule=update_rule,</span><br><span class="line">                  optim_config=&#123;</span><br><span class="line">                    <span class="string">'learning_rate'</span>: learning_rates[update_rule]</span><br><span class="line">                  &#125;,</span><br><span class="line">                  verbose=<span class="literal">True</span>)</span><br><span class="line">  solvers[update_rule] = solver</span><br><span class="line">  solver.train()</span><br><span class="line">  print()</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">'Training loss'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Iteration'</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">'Training accuracy'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epoch'</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">plt.title(<span class="string">'Validation accuracy'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epoch'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> update_rule, solver <span class="keyword">in</span> list(solvers.items()):</span><br><span class="line">  plt.subplot(<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">  plt.plot(solver.loss_history, <span class="string">'o'</span>, label=update_rule)</span><br><span class="line">  </span><br><span class="line">  plt.subplot(<span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">  plt.plot(solver.train_acc_history, <span class="string">'-o'</span>, label=update_rule)</span><br><span class="line"></span><br><span class="line">  plt.subplot(<span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">  plt.plot(solver.val_acc_history, <span class="string">'-o'</span>, label=update_rule)</span><br><span class="line">  </span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]:</span><br><span class="line">  plt.subplot(<span class="number">3</span>, <span class="number">1</span>, i)</span><br><span class="line">  plt.legend(loc=<span class="string">'upper center'</span>, ncol=<span class="number">4</span>)</span><br><span class="line">plt.gcf().set_size_inches(<span class="number">15</span>, <span class="number">15</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/02/24/Update-rules/1582546014282.png" alt="1582546014282"></p>
<p><img src="/2020/02/24/Update-rules/1582546042532.png" alt="1582546042532"></p>
<p><img src="/2020/02/24/Update-rules/1582546061048.png" alt="1582546061048"></p>
<p>Obviously, Adam performs well on the data for achieving the highest accuracy within the same epoch. When epoch is small, RMSprop performs better than SGD+Momentum, but when epoch is getting large, the gap between them is narrowing. SGD at the end is the last one we should consider to use.</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block home" lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/24/Two-Layer-Network/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Bania">
      <meta itemprop="description" content="A platform for discussing programming and technology">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BaniaBlog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          
            <a href="/2020/02/24/Two-Layer-Network/" class="post-title-link" itemprop="url">Two-Layer Network</a>
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-02-24 09:49:23 / Modified: 11:35:36" itemprop="dateCreated datePublished" datetime="2020-02-24T09:49:23+08:00">2020-02-24</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <p>In assignment1, we have implemented a simple example of neural networks(Two-Layer Network), but the problem is that the implementation is not very modular since the loss and gradient are computed in a single monolithic function. That means if we want to create a complex network, we may find it difficult to implement. So the first part of the assignment2 question1 is to make the implementation more modular based on Two-Layer Network.</p>
<p>For each layer we will implement a <code>forward</code> and a <code>backward</code> function. The <code>forward</code> function will receive inputs, weights, and other parameters and will return both an output and a <code>cache</code> object storing data needed for the backward pass, like this:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layer_forward</span><span class="params">(x, w)</span>:</span></span><br><span class="line">  <span class="string">""" Receive inputs x and weights w """</span></span><br><span class="line">  <span class="comment"># Do some computations ...</span></span><br><span class="line">  z = <span class="comment"># ... some intermediate value</span></span><br><span class="line">  <span class="comment"># Do some more computations ...</span></span><br><span class="line">  out = <span class="comment"># the output</span></span><br><span class="line"></span><br><span class="line">  cache = (x, w, z, out) <span class="comment"># Values we need to compute gradients</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure>
<p>The backward pass will receive upstream derivatives and the <code>cache</code> object, and will return gradients with respect to the inputs and weights, like this:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">layer_backward</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  Receive derivative of loss with respect to outputs and cache,</span></span><br><span class="line"><span class="string">  and compute derivative with respect to inputs.</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  <span class="comment"># Unpack cache values</span></span><br><span class="line">  x, w, z, out = cache</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Use values in cache to compute derivatives</span></span><br><span class="line">  dx = <span class="comment"># Derivative of loss with respect to x</span></span><br><span class="line">  dw = <span class="comment"># Derivative of loss with respect to w</span></span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> dx, dw</span><br></pre></td></tr></table></figure>
<p>The implementation will be based on the structure of  Networks. In this case, we have <code>affine----&gt;relu----&gt;affine</code> network, so it is wise to create methods for each layers and activation function. </p>
<p>Open layers.py file and implement the methods below:</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">affine_forward</span><span class="params">(x, w, b)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the forward pass for an affine (fully-connected) layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The input x has shape (N, d_1, ..., d_k) and contains a minibatch of N</span></span><br><span class="line"><span class="string">    examples, where each example x[i] has shape (d_1, ..., d_k). We will</span></span><br><span class="line"><span class="string">    reshape each input into a vector of dimension D = d_1 * ... * d_k, and</span></span><br><span class="line"><span class="string">    then transform it to an output vector of dimension M.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: A numpy array containing input data, of shape (N, d_1, ..., d_k)</span></span><br><span class="line"><span class="string">    - w: A numpy array of weights, of shape (D, M)</span></span><br><span class="line"><span class="string">    - b: A numpy array of biases, of shape (M,)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: output, of shape (N, M)</span></span><br><span class="line"><span class="string">    - cache: (x, w, b)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    out = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the affine forward pass. Store the result in out. You   #</span></span><br><span class="line">    <span class="comment"># will need to reshape the input into rows.                               #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    out = np.reshape(x,(x.shape[<span class="number">0</span>], <span class="number">-1</span>)).dot(w)+b</span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    cache = (x, w, b)</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">affine_backward</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the backward pass for an affine layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivative, of shape (N, M)</span></span><br><span class="line"><span class="string">    - cache: Tuple of:</span></span><br><span class="line"><span class="string">      - x: Input data, of shape (N, d_1, ... d_k)</span></span><br><span class="line"><span class="string">      - w: Weights, of shape (D, M)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradient with respect to x, of shape (N, d1, ..., d_k)</span></span><br><span class="line"><span class="string">    - dw: Gradient with respect to w, of shape (D, M)</span></span><br><span class="line"><span class="string">    - db: Gradient with respect to b, of shape (M,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    x, w, b = cache</span><br><span class="line">    dx, dw, db = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the affine backward pass.                               #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    dx = np.reshape(dout.dot(w.T),x.shape)</span><br><span class="line">    dw = np.reshape(x,(x.shape[<span class="number">0</span>], <span class="number">-1</span>)).T.dot(dout)</span><br><span class="line">    db = np.sum(dout, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="keyword">return</span> dx, dw, db</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu_forward</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the forward pass for a layer of rectified linear units (ReLUs).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">    - x: Inputs, of any shape</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: Output, of the same shape as x</span></span><br><span class="line"><span class="string">    - cache: x</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    out = <span class="literal">None</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the ReLU forward pass.                                  #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    out = np.maximum(<span class="number">0</span>, x)</span><br><span class="line">    </span><br><span class="line">   </span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #         ###########################################################################</span></span><br><span class="line">    cache = x</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">relu_backward</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Computes the backward pass for a layer of rectified linear units (ReLUs).</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivatives, of any shape</span></span><br><span class="line"><span class="string">    - cache: Input x, of same shape as dout</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - dx: Gradient with respect to x</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dx, x = <span class="literal">None</span>, cache</span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the ReLU backward pass.                                 #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    dout[x&lt;=<span class="number">0</span>]=<span class="number">0</span></span><br><span class="line">    dx = dout </span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="keyword">return</span> dx</span><br></pre></td></tr></table></figure>
<p>Note that we have<code>affine---&gt;relu</code> module, we can combine affine and relu module together to create <code>affine_relu_forward/backward</code>, which will make the implementation easier. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">affine_relu_forward</span><span class="params">(x, w, b)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Convenience layer that perorms an affine transform followed by a ReLU</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - x: Input to the affine layer</span></span><br><span class="line"><span class="string">    - w, b: Weights for the affine layer</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: Output from the ReLU</span></span><br><span class="line"><span class="string">    - cache: Object to give to the backward pass</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    a, fc_cache = affine_forward(x, w, b)</span><br><span class="line">    out, relu_cache = relu_forward(a)</span><br><span class="line">    cache = (fc_cache, relu_cache)</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">affine_relu_backward</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Backward pass for the affine-relu convenience layer</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    fc_cache, relu_cache = cache</span><br><span class="line">    da = relu_backward(dout, relu_cache)</span><br><span class="line">    dx, dw, db = affine_backward(da, fc_cache)</span><br><span class="line">    <span class="keyword">return</span> dx, dw, db</span><br></pre></td></tr></table></figure>
<p>Then we can test all the methods we have created in FullyConnectedNets.ipynb according to the error. </p>
<p><img src="/2020/02/24/Two-Layer-Network/1582513523961.png" alt="1582513523961"></p>
<p><img src="/2020/02/24/Two-Layer-Network/1582513543672.png" alt="1582513543672"></p>
<p><img src="/2020/02/24/Two-Layer-Network/1582513558613.png" alt="1582513558613"></p>
<p><img src="/2020/02/24/Two-Layer-Network/1582513570779.png" alt="1582513570779"></p>
<p><img src="/2020/02/24/Two-Layer-Network/1582513594658.png" alt="1582513594658"></p>
<p>All the methods have achieve the requirements so that we can use these functions to make our implementation easy to read and modify. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> builtins <span class="keyword">import</span> range</span><br><span class="line"><span class="keyword">from</span> builtins <span class="keyword">import</span> object</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> cs231n.layers <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> cs231n.layer_utils <span class="keyword">import</span> *</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TwoLayerNet</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A two-layer fully-connected neural network with ReLU nonlinearity and</span></span><br><span class="line"><span class="string">    softmax loss that uses a modular layer design. We assume an input dimension</span></span><br><span class="line"><span class="string">    of D, a hidden dimension of H, and perform classification over C classes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The architecure should be affine - relu - affine - softmax.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Note that this class does not implement gradient descent; instead, it</span></span><br><span class="line"><span class="string">    will interact with a separate Solver object that is responsible for running</span></span><br><span class="line"><span class="string">    optimization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    The learnable parameters of the model are stored in the dictionary</span></span><br><span class="line"><span class="string">    self.params that maps parameter names to numpy arrays.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_dim=<span class="number">3</span>*<span class="number">32</span>*<span class="number">32</span>, hidden_dim=<span class="number">100</span>, num_classes=<span class="number">10</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 weight_scale=<span class="number">1e-3</span>, reg=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Initialize a new network.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Inputs:</span></span><br><span class="line"><span class="string">        - input_dim: An integer giving the size of the input</span></span><br><span class="line"><span class="string">        - hidden_dim: An integer giving the size of the hidden layer</span></span><br><span class="line"><span class="string">        - num_classes: An integer giving the number of classes to classify</span></span><br><span class="line"><span class="string">        - dropout: Scalar between 0 and 1 giving dropout strength.</span></span><br><span class="line"><span class="string">        - weight_scale: Scalar giving the standard deviation for random</span></span><br><span class="line"><span class="string">          initialization of the weights.</span></span><br><span class="line"><span class="string">        - reg: Scalar giving L2 regularization strength.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.params = &#123;&#125;</span><br><span class="line">        self.reg = reg</span><br><span class="line"></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Initialize the weights and biases of the two-layer net. Weights    #</span></span><br><span class="line">        <span class="comment"># should be initialized from a Gaussian with standard deviation equal to   #</span></span><br><span class="line">        <span class="comment"># weight_scale, and biases should be initialized to zero. All weights and  #</span></span><br><span class="line">        <span class="comment"># biases should be stored in the dictionary self.params, with first layer  #</span></span><br><span class="line">        <span class="comment"># weights and biases using the keys 'W1' and 'b1' and second layer weights #</span></span><br><span class="line">        <span class="comment"># and biases using the keys 'W2' and 'b2'.                                 #</span></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        self.params[<span class="string">'W1'</span>] = weight_scale*np.random.randn(input_dim, hidden_dim)</span><br><span class="line">        self.params[<span class="string">'b1'</span>] = np.zeros(hidden_dim)</span><br><span class="line">        self.params[<span class="string">'W2'</span>] = weight_scale*np.random.randn(hidden_dim, num_classes)</span><br><span class="line">        self.params[<span class="string">'b2'</span>] = np.zeros(num_classes)</span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        <span class="comment">#                             END OF YOUR CODE                             #</span></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Compute loss and gradient for a minibatch of data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Inputs:</span></span><br><span class="line"><span class="string">        - X: Array of input data of shape (N, d_1, ..., d_k)</span></span><br><span class="line"><span class="string">        - y: Array of labels, of shape (N,). y[i] gives the label for X[i].</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">        If y is None, then run a test-time forward pass of the model and return:</span></span><br><span class="line"><span class="string">        - scores: Array of shape (N, C) giving classification scores, where</span></span><br><span class="line"><span class="string">          scores[i, c] is the classification score for X[i] and class c.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        If y is not None, then run a training-time forward and backward pass and</span></span><br><span class="line"><span class="string">        return a tuple of:</span></span><br><span class="line"><span class="string">        - loss: Scalar value giving the loss</span></span><br><span class="line"><span class="string">        - grads: Dictionary with the same keys as self.params, mapping parameter</span></span><br><span class="line"><span class="string">          names to gradients of the loss with respect to those parameters.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        scores = <span class="literal">None</span></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Implement the forward pass for the two-layer net, computing the    #</span></span><br><span class="line">        <span class="comment"># class scores for X and storing them in the scores variable.              #</span></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        W1, b1 = self.params[<span class="string">'W1'</span>], self.params[<span class="string">'b1'</span>]</span><br><span class="line">        W2, b2 = self.params[<span class="string">'W2'</span>], self.params[<span class="string">'b2'</span>]</span><br><span class="line">        N, D = np.reshape(X,(X.shape[<span class="number">0</span>],<span class="number">-1</span>)).shape</span><br><span class="line">        scores,cache1 = affine_relu_forward(X, W1, b1)</span><br><span class="line">        scores,cache2 = affine_forward(scores, W2, b2)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        <span class="comment">#                             END OF YOUR CODE                             #</span></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># If y is None then we are in test mode so just return scores</span></span><br><span class="line">        <span class="keyword">if</span> y <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> scores</span><br><span class="line"></span><br><span class="line">        loss, grads = <span class="number">0</span>, &#123;&#125;</span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Implement the backward pass for the two-layer net. Store the loss  #</span></span><br><span class="line">        <span class="comment"># in the loss variable and gradients in the grads dictionary. Compute data #</span></span><br><span class="line">        <span class="comment"># loss using softmax, and make sure that grads[k] holds the gradients for  #</span></span><br><span class="line">        <span class="comment"># self.params[k]. Don't forget to add L2 regularization!                   #</span></span><br><span class="line">        <span class="comment">#                                                                          #</span></span><br><span class="line">        <span class="comment"># <span class="doctag">NOTE:</span> To ensure that your implementation matches ours and you pass the   #</span></span><br><span class="line">        <span class="comment"># automated tests, make sure that your L2 regularization includes a factor #</span></span><br><span class="line">        <span class="comment"># of 0.5 to simplify the expression for the gradient.                      #</span></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        scores -= np.max(scores, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        pro_scores = np.exp(scores)/ np.sum(np.exp(scores), axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        loss = -np.sum(np.log(pro_scores[np.arange(N),y]))</span><br><span class="line">        </span><br><span class="line">        loss /= N</span><br><span class="line">        loss += <span class="number">0.5</span>*self.reg*np.sum(W1*W1)+ <span class="number">0.5</span>*self.reg*np.sum(W2*W2)</span><br><span class="line">        </span><br><span class="line">        pro_scores[np.arange(N), y] -= <span class="number">1</span></span><br><span class="line">        pro_scores /= N</span><br><span class="line">        </span><br><span class="line">        dhidden, dW2, db2 = affine_backward(pro_scores, cache2)</span><br><span class="line">        _, dW1, db1 = affine_relu_backward(dhidden, cache1)</span><br><span class="line">        </span><br><span class="line">        dW1 += self.reg*W1</span><br><span class="line">        dW2 += self.reg*W2</span><br><span class="line">        </span><br><span class="line">        grads = &#123;<span class="string">'W1'</span>:dW1, <span class="string">'b1'</span>:db1, <span class="string">'W2'</span>:dW2, <span class="string">'b2'</span>:db2&#125;</span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        <span class="comment">#                             END OF YOUR CODE                             #</span></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss, grads</span><br></pre></td></tr></table></figure>
<p>We just use two lines to obtain the scores and the loss, which has greatly simplified the programming.  To see whether we implement the methods correctly we can use the code below to test the error of each parameter. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">231</span>)</span><br><span class="line">N, D, H, C = <span class="number">3</span>, <span class="number">5</span>, <span class="number">50</span>, <span class="number">7</span></span><br><span class="line">X = np.random.randn(N, D)</span><br><span class="line">y = np.random.randint(C, size=N)</span><br><span class="line"></span><br><span class="line">std = <span class="number">1e-3</span></span><br><span class="line">model = TwoLayerNet(input_dim=D, hidden_dim=H, num_classes=C, weight_scale=std)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Testing initialization ... '</span>)</span><br><span class="line">W1_std = abs(model.params[<span class="string">'W1'</span>].std() - std)</span><br><span class="line">b1 = model.params[<span class="string">'b1'</span>]</span><br><span class="line">W2_std = abs(model.params[<span class="string">'W2'</span>].std() - std)</span><br><span class="line">b2 = model.params[<span class="string">'b2'</span>]</span><br><span class="line"><span class="keyword">assert</span> W1_std &lt; std / <span class="number">10</span>, <span class="string">'First layer weights do not seem right'</span></span><br><span class="line"><span class="keyword">assert</span> np.all(b1 == <span class="number">0</span>), <span class="string">'First layer biases do not seem right'</span></span><br><span class="line"><span class="keyword">assert</span> W2_std &lt; std / <span class="number">10</span>, <span class="string">'Second layer weights do not seem right'</span></span><br><span class="line"><span class="keyword">assert</span> np.all(b2 == <span class="number">0</span>), <span class="string">'Second layer biases do not seem right'</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'Testing test-time forward pass ... '</span>)</span><br><span class="line">model.params[<span class="string">'W1'</span>] = np.linspace(<span class="number">-0.7</span>, <span class="number">0.3</span>, num=D*H).reshape(D, H)</span><br><span class="line">model.params[<span class="string">'b1'</span>] = np.linspace(<span class="number">-0.1</span>, <span class="number">0.9</span>, num=H)</span><br><span class="line">model.params[<span class="string">'W2'</span>] = np.linspace(<span class="number">-0.3</span>, <span class="number">0.4</span>, num=H*C).reshape(H, C)</span><br><span class="line">model.params[<span class="string">'b2'</span>] = np.linspace(<span class="number">-0.9</span>, <span class="number">0.1</span>, num=C)</span><br><span class="line">X = np.linspace(<span class="number">-5.5</span>, <span class="number">4.5</span>, num=N*D).reshape(D, N).T</span><br><span class="line">scores = model.loss(X)</span><br><span class="line">correct_scores = np.asarray(</span><br><span class="line">  [[<span class="number">11.53165108</span>,  <span class="number">12.2917344</span>,   <span class="number">13.05181771</span>,  <span class="number">13.81190102</span>,  <span class="number">14.57198434</span>, <span class="number">15.33206765</span>,  <span class="number">16.09215096</span>],</span><br><span class="line">   [<span class="number">12.05769098</span>,  <span class="number">12.74614105</span>,  <span class="number">13.43459113</span>,  <span class="number">14.1230412</span>,   <span class="number">14.81149128</span>, <span class="number">15.49994135</span>,  <span class="number">16.18839143</span>],</span><br><span class="line">   [<span class="number">12.58373087</span>,  <span class="number">13.20054771</span>,  <span class="number">13.81736455</span>,  <span class="number">14.43418138</span>,  <span class="number">15.05099822</span>, <span class="number">15.66781506</span>,  <span class="number">16.2846319</span> ]])</span><br><span class="line">scores_diff = np.abs(scores - correct_scores).sum()</span><br><span class="line"><span class="keyword">assert</span> scores_diff &lt; <span class="number">1e-6</span>, <span class="string">'Problem with test-time forward pass'</span></span><br><span class="line"></span><br><span class="line">print(<span class="string">'Testing training loss (no regularization)'</span>)</span><br><span class="line">y = np.asarray([<span class="number">0</span>, <span class="number">5</span>, <span class="number">1</span>])</span><br><span class="line">loss, grads = model.loss(X, y)</span><br><span class="line">correct_loss = <span class="number">3.4702243556</span></span><br><span class="line"><span class="keyword">assert</span> abs(loss - correct_loss) &lt; <span class="number">1e-10</span>, <span class="string">'Problem with training-time loss'</span></span><br><span class="line"></span><br><span class="line">model.reg = <span class="number">1.0</span></span><br><span class="line">loss, grads = model.loss(X, y)</span><br><span class="line">correct_loss = <span class="number">26.5948426952</span></span><br><span class="line"><span class="keyword">assert</span> abs(loss - correct_loss) &lt; <span class="number">1e-10</span>, <span class="string">'Problem with regularization loss'</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> reg <span class="keyword">in</span> [<span class="number">0.0</span>, <span class="number">0.7</span>]:</span><br><span class="line">  print(<span class="string">'Running numeric gradient check with reg = '</span>, reg)</span><br><span class="line">  model.reg = reg</span><br><span class="line">  loss, grads = model.loss(X, y)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> name <span class="keyword">in</span> sorted(grads):</span><br><span class="line">    f = <span class="keyword">lambda</span> _: model.loss(X, y)[<span class="number">0</span>]</span><br><span class="line">    grad_num = eval_numerical_gradient(f, model.params[name], verbose=<span class="literal">False</span>)</span><br><span class="line">    print(<span class="string">'%s relative error: %.2e'</span> % (name, rel_error(grad_num, grads[name])))</span><br></pre></td></tr></table></figure>
<p>The error of each parameter should be less than 1e-7.</p>
<p><img src="/2020/02/24/Two-Layer-Network/1582514136392.png" alt="1582514136392"></p>
<p>Finally, we use this Two-Layer Network on the data with <code>Slover</code> which is a modular class that fits the model into it and can train the model, predict the final output etc.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">model = TwoLayerNet(input_dim=<span class="number">3</span>*<span class="number">32</span>*<span class="number">32</span>, hidden_dim=<span class="number">100</span>, num_classes=<span class="number">10</span>,</span><br><span class="line">                 weight_scale=<span class="number">1e-3</span>, reg=<span class="number">0.1</span>)</span><br><span class="line">solver = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="comment">##############################################################################</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Use a Solver instance to train a TwoLayerNet that achieves at least  #</span></span><br><span class="line"><span class="comment"># 50% accuracy on the validation set.                                        #</span></span><br><span class="line"><span class="comment">##############################################################################</span></span><br><span class="line">data2 = &#123;</span><br><span class="line">      <span class="string">'X_train'</span>: data[<span class="string">'X_train'</span>],</span><br><span class="line">      <span class="string">'y_train'</span>: data[<span class="string">'y_train'</span>],</span><br><span class="line">      <span class="string">'X_val'</span>: data[<span class="string">'X_val'</span>],</span><br><span class="line">      <span class="string">'y_val'</span>: data[<span class="string">'y_val'</span>],</span><br><span class="line">    &#125;</span><br><span class="line">solver = Solver(model, data2,</span><br><span class="line">                    update_rule=<span class="string">'sgd'</span>,</span><br><span class="line">                    optim_config=&#123;</span><br><span class="line">                      <span class="string">'learning_rate'</span>: <span class="number">1e-3</span>,</span><br><span class="line">                    &#125;,</span><br><span class="line">                    lr_decay=<span class="number">0.95</span>,</span><br><span class="line">                    num_epochs=<span class="number">10</span>, batch_size=<span class="number">100</span>,</span><br><span class="line">                    print_every=<span class="number">100</span>)</span><br><span class="line">solver.train()</span><br><span class="line"><span class="comment">##############################################################################</span></span><br><span class="line"><span class="comment">#                             END OF YOUR CODE                               #</span></span><br><span class="line"><span class="comment">##############################################################################</span></span><br></pre></td></tr></table></figure>
<p>Then, draw the accuracy graph and loss graph to see how they change with the epoch.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Run this cell to visualize training loss and train / val accuracy</span></span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">'Training loss'</span>)</span><br><span class="line">plt.plot(solver.loss_history, <span class="string">'o'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Iteration'</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">2</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">'Accuracy'</span>)</span><br><span class="line">plt.plot(solver.train_acc_history, <span class="string">'-o'</span>, label=<span class="string">'train'</span>)</span><br><span class="line">plt.plot(solver.val_acc_history, <span class="string">'-o'</span>, label=<span class="string">'val'</span>)</span><br><span class="line">plt.plot([<span class="number">0.5</span>] * len(solver.val_acc_history), <span class="string">'k--'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Epoch'</span>)</span><br><span class="line">plt.legend(loc=<span class="string">'lower right'</span>)</span><br><span class="line">plt.gcf().set_size_inches(<span class="number">15</span>, <span class="number">12</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/02/24/Two-Layer-Network/1582514639365.png" alt="1582514639365"></p>
<p><img src="/2020/02/24/Two-Layer-Network/1582514661060.png" alt="1582514661060"></p>
<p>This part of exercise is prepared for the Multilayer Networks in which we have to deal with more complex networks, but the idea is the same that modular programming like above can make it more achievable. </p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

  </div>

  
  <nav class="pagination">
    <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="extend next" rel="next" href="/page/2/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Bania</p>
  <div class="site-description" itemprop="description">A platform for discussing programming and technology</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">16</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Bania</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.7.1
  </div>

<div>
<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<span id="busuanzi_container_site_pv" style='display:none'>
  Total visits: <span id="busuanzi_value_site_pv"></span> times
  <span class="post-meta-divider">|</span>
</span>
<span id="busuanzi_container_site_uv" style='display:none'>
  <span id="busuanzi_value_site_uv"></span>people have viewed my bolg.
</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
        loader: {
          load: ['[tex]/mhchem']
        },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
