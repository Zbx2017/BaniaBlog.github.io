<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Pisces","version":"7.7.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Problem OverviewThe last classifier in assignment1 is Neural Network. We need to build a two-layers network from scratch, which means we know the function for each layer. But in many cases, if we deal">
<meta property="og:type" content="article">
<meta property="og:title" content="Neural Net">
<meta property="og:url" content="http://yoursite.com/2020/02/18/Neural-Net/index.html">
<meta property="og:site_name" content="BaniaBlog">
<meta property="og:description" content="Problem OverviewThe last classifier in assignment1 is Neural Network. We need to build a two-layers network from scratch, which means we know the function for each layer. But in many cases, if we deal">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://yoursite.com/2020/02/18/Neural-Net/1581992206941.png">
<meta property="og:image" content="http://yoursite.com/2020/02/18/Neural-Net/1581992588211.png">
<meta property="og:image" content="http://yoursite.com/2020/02/18/Neural-Net/1581995492794.png">
<meta property="og:image" content="http://yoursite.com/2020/02/18/Neural-Net/1581996049609.png">
<meta property="og:image" content="http://yoursite.com/2020/02/18/Neural-Net/1581996072635.png">
<meta property="og:image" content="http://yoursite.com/2020/02/18/Neural-Net/1581998109205.png">
<meta property="og:image" content="http://yoursite.com/2020/02/18/Neural-Net/1582003562296.png">
<meta property="og:image" content="http://yoursite.com/2020/02/18/Neural-Net/1582003592110.png">
<meta property="og:image" content="http://yoursite.com/2020/02/18/Neural-Net/1582003605931.png">
<meta property="og:image" content="http://yoursite.com/2020/02/18/Neural-Net/1582003620532.png">
<meta property="og:image" content="http://yoursite.com/2020/02/18/Neural-Net/1582003637573.png">
<meta property="og:image" content="http://yoursite.com/2020/02/18/Neural-Net/1582003790638.png">
<meta property="og:image" content="http://yoursite.com/2020/02/18/Neural-Net/1582003850830.png">
<meta property="og:image" content="http://yoursite.com/2020/02/18/Neural-Net/1582003872346.png">
<meta property="og:image" content="http://yoursite.com/2020/02/18/Neural-Net/1582003979640.png">
<meta property="og:image" content="http://yoursite.com/2020/02/18/Neural-Net/1582004046620.png">
<meta property="article:published_time" content="2020-02-18T00:41:58.000Z">
<meta property="article:modified_time" content="2020-08-10T04:49:56.344Z">
<meta property="article:author" content="Bania">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/2020/02/18/Neural-Net/1581992206941.png">

<link rel="canonical" href="http://yoursite.com/2020/02/18/Neural-Net/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>Neural Net | BaniaBlog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="BaniaBlog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">BaniaBlog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">Sharing Technology</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/Zbx2017/Zbx2017.github.io" class="github-corner" title="Bania GitHub" aria-label="Bania GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
    
  
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/18/Neural-Net/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Bania">
      <meta itemprop="description" content="A platform for discussing programming and technology">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BaniaBlog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Neural Net
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-02-18 08:41:58" itemprop="dateCreated datePublished" datetime="2020-02-18T08:41:58+08:00">2020-02-18</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-08-10 12:49:56" itemprop="dateModified" datetime="2020-08-10T12:49:56+08:00">2020-08-10</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h4 id="Problem-Overview"><a href="#Problem-Overview" class="headerlink" title="Problem Overview"></a>Problem Overview</h4><p>The last classifier in assignment1 is Neural Network. We need to build a two-layers network from scratch, which means we know the function for each layer. But in many cases, if we deal with a complex problem and need a complex model, it is very hard to know exactly the function for each layer and this can make the model unexplainable. That is why sometimes we may prefer to use machine learning algorithms to deal with problems. </p>
<p>Compared to previous classifier, I find it more difficult to understand and the calculation is a little bit more complex. In this exercise, using your pen to do the math on a paper may be clear and comprehensible before you start to write the code. </p>
<p>At the end, you can see that neural network performs well in the classification and has a high validation accuracy. The problem with it, however, is the efficiency because of using two layers linear function and one hidden layer(using ReLU). </p>
<h4 id="Neural-Network"><a href="#Neural-Network" class="headerlink" title="Neural Network"></a>Neural Network</h4><ol>
<li>Model Architectures </li>
</ol>
<p>When you start to write the code, you should keep on mind of the structure of the model. We use two layers &amp; one hidden layer network, so that you can easily know what exactly the structure is. Just like that:</p>
<p><img src="/2020/02/18/Neural-Net/1581992206941.png" alt="1581992206941"> </p>
<h4 id><a href="#" class="headerlink" title=" "></a> </h4><p>For input layer, we have </p>
<script type="math/tex; mode=display">
f_{in}=W_1X+b_1</script><p>and output layer :</p>
<script type="math/tex; mode=display">
f_{out}=W_2X+b_2</script><p>For hidden layer, we use ReLU as activation function.</p>
<script type="math/tex; mode=display">
Re(x)=max(0,x)</script><p>There are also many activation functions, you can choose them to test the result, but ReLU is suitable to most problems. </p>
<p><img src="/2020/02/18/Neural-Net/1581992588211.png" alt="1581992588211"></p>
<ol>
<li>Computational Graph</li>
</ol>
<p>Notice that we can easily get the loss of the networks by calculating the scores first and then use softmax loss to get the loss. Gradient calculation, on the contrary, is more difficult to obtain. If you calculate it directly from input to output, it will be complicated and easy to make mistakes. If the model is very complex , it is unrealistic to do in that way. Thus, we choose to use computational graph &amp; Backpropagation. Let me give you a simple example to help you understand it. </p>
<p>Now, we have a function like that and we want to get the derivative of each parameters. </p>
<p><img src="/2020/02/18/Neural-Net/1581995492794.png" alt="1581995492794"> </p>
<p>Based on the computational graph, we can clearly see the relationships between each parameters, and we can use chain rule to write the derivative of each parameters. </p>
<script type="math/tex; mode=display">
\frac{\partial f}{\partial x} = \frac{\partial f}{\partial q}\frac{\partial q}{\partial x}</script><script type="math/tex; mode=display">
\frac{\partial f}{\partial y} = \frac{\partial f}{\partial q}\frac{\partial q}{\partial y}</script><p><img src="/2020/02/18/Neural-Net/1581996049609.png" alt="1581996049609"></p>
<p><img src="/2020/02/18/Neural-Net/1581996072635.png" alt="1581996072635"></p>
<p>We know basically how it works and now we can get our computational graph based on the model. </p>
<p><img src="/2020/02/18/Neural-Net/1581998109205.png" alt="1581998109205"></p>
<ol>
<li>Backpropagation</li>
</ol>
<p>After we get the computational graph, we can use backpropagation to get the gradients. Because we use softmax loss, we can obtain the gradient of $\frac{\partial L}{\partial y}$​, then we can use the chain rule to get all the gradients. </p>
<script type="math/tex; mode=display">
\frac{\partial L}{\partial W_2} = \frac{\partial L}{\partial y}\frac{\partial y}{\partial W_2} = y_1^{T}\frac{\partial L}{\partial y} \\</script><script type="math/tex; mode=display">
\frac{\partial L}{\partial b_2} = \frac{\partial L}{\partial y}\frac{\partial y}{\partial b_2} = 1_{(1\times N)}\frac{\partial L}{\partial y} \\</script><script type="math/tex; mode=display">
\frac{\partial L}{\partial y_1} = \frac{\partial L}{\partial y}\frac{\partial y}{\partial y_1} =\frac{\partial L}{\partial y}W_2^T\\</script><script type="math/tex; mode=display">
\frac{\partial y_1}{\partial y_2} = \begin{cases}
1& y_{2_{ij}} >0\\
0& otherwise
\end{cases} \\</script><script type="math/tex; mode=display">
\frac{\partial L}{\partial y_2}=\frac{\partial L}{\partial y_1}\frac{\partial y_1}{\partial y_2}=\frac{\partial L}{\partial y_1}, (\frac{\partial L}{\partial y_1}[y_1<=0] = 0)\\</script><script type="math/tex; mode=display">
\frac{\partial L}{\partial W_1} = \frac{\partial L}{\partial y_2}\frac{\partial y_2}{\partial W_1} = x^{T}\frac{\partial L}{\partial y_2} \\</script><script type="math/tex; mode=display">
\frac{\partial L}{\partial b_1} = \frac{\partial L}{\partial y_2}\frac{\partial y_2}{\partial b_2} = 1_{(1\times C)}\frac{\partial L}{\partial y_2} \\</script><p>Based on these equations, we can write the code easily without any big mistakes. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> past.builtins <span class="keyword">import</span> xrange</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">TwoLayerNet</span><span class="params">(object)</span>:</span></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">  A two-layer fully-connected neural network. The net has an input dimension of</span></span><br><span class="line"><span class="string">  N, a hidden layer dimension of H, and performs classification over C classes.</span></span><br><span class="line"><span class="string">  We train the network with a softmax loss function and L2 regularization on the</span></span><br><span class="line"><span class="string">  weight matrices. The network uses a ReLU nonlinearity after the first fully</span></span><br><span class="line"><span class="string">  connected layer.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  In other words, the network has the following architecture:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  input - fully connected layer - ReLU - fully connected layer - softmax</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  The outputs of the second fully-connected layer are the scores for each class.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden_size, output_size, std=<span class="number">1e-4</span>)</span>:</span></span><br><span class="line">              </span><br><span class="line">        </span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">    Initialize the model. Weights are initialized to small random values and</span></span><br><span class="line"><span class="string">    biases are initialized to zero. Weights and biases are stored in the</span></span><br><span class="line"><span class="string">    variable self.params, which is a dictionary with the following keys:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    W1: First layer weights; has shape (D, H)</span></span><br><span class="line"><span class="string">    b1: First layer biases; has shape (H,)</span></span><br><span class="line"><span class="string">    W2: Second layer weights; has shape (H, C)</span></span><br><span class="line"><span class="string">    b2: Second layer biases; has shape (C,)</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - input_size: The dimension D of the input data.</span></span><br><span class="line"><span class="string">    - hidden_size: The number of neurons H in the hidden layer.</span></span><br><span class="line"><span class="string">    - output_size: The number of classes C.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.params = &#123;&#125;</span><br><span class="line">        self.params[<span class="string">'W1'</span>] = std * np.random.randn(input_size, hidden_size)</span><br><span class="line">        self.params[<span class="string">'b1'</span>] = np.zeros(hidden_size)</span><br><span class="line">        self.params[<span class="string">'W2'</span>] = std * np.random.randn(hidden_size, output_size)</span><br><span class="line">        self.params[<span class="string">'b2'</span>] = np.zeros(output_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(self, X, y=None, reg=<span class="number">0.0</span>)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the loss and gradients for a two layer fully connected neural</span></span><br><span class="line"><span class="string">    network.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - X: Input data of shape (N, D). Each X[i] is a training sample.</span></span><br><span class="line"><span class="string">    - y: Vector of training labels. y[i] is the label for X[i], and each y[i] is</span></span><br><span class="line"><span class="string">      an integer in the range 0 &lt;= y[i] &lt; C. This parameter is optional; if it</span></span><br><span class="line"><span class="string">      is not passed then we only return scores, and if it is passed then we</span></span><br><span class="line"><span class="string">      instead return the loss and gradients.</span></span><br><span class="line"><span class="string">    - reg: Regularization strength.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    If y is None, return a matrix scores of shape (N, C) where scores[i, c] is</span></span><br><span class="line"><span class="string">    the score for class c on input X[i].</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    If y is not None, instead return a tuple of:</span></span><br><span class="line"><span class="string">    - loss: Loss (data loss and regularization loss) for this batch of training</span></span><br><span class="line"><span class="string">      samples.</span></span><br><span class="line"><span class="string">    - grads: Dictionary mapping parameter names to gradients of those parameters</span></span><br><span class="line"><span class="string">      with respect to the loss function; has the same keys as self.params.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">    <span class="comment"># Unpack variables from the params dictionary</span></span><br><span class="line">        W1, b1 = self.params[<span class="string">'W1'</span>], self.params[<span class="string">'b1'</span>]</span><br><span class="line">        W2, b2 = self.params[<span class="string">'W2'</span>], self.params[<span class="string">'b2'</span>]</span><br><span class="line">        N, D = X.shape</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the forward pass</span></span><br><span class="line">        scores = <span class="literal">None</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Perform the forward pass, computing the class scores for the input. #</span></span><br><span class="line">    <span class="comment"># Store the result in the scores variable, which should be an array of      #</span></span><br><span class="line">    <span class="comment"># shape (N, C).                                                             #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">        score_layer1 = X.dot(W1) + b1</span><br><span class="line">        score_ReLU =np.maximum(score_layer1,<span class="number">0</span>)</span><br><span class="line">        scores = score_ReLU.dot(W2) + b2</span><br><span class="line"></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment">#                              END OF YOUR CODE                             #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># If the targets are not given then jump out, we're done</span></span><br><span class="line">        <span class="keyword">if</span> y <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> scores</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Compute the loss</span></span><br><span class="line">        loss = <span class="literal">None</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Finish the forward pass, and compute the loss. This should include  #</span></span><br><span class="line">    <span class="comment"># both the data loss and L2 regularization for W1 and W2. Store the result  #</span></span><br><span class="line">    <span class="comment"># in the variable loss, which should be a scalar. Use the Softmax           #</span></span><br><span class="line">    <span class="comment"># classifier loss.                                                          #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">        </span><br><span class="line">        scores -= np.max(scores, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        pro_scores = np.exp(scores)/ np.sum(np.exp(scores), axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">        loss = -np.sum(np.log(pro_scores[np.arange(N),y]))</span><br><span class="line">        </span><br><span class="line">        loss /= N</span><br><span class="line">        loss += reg*np.sum(W1*W1)+ reg*np.sum(W2*W2)</span><br><span class="line">        </span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment">#                              END OF YOUR CODE                             #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># Backward pass: compute gradients</span></span><br><span class="line">        grads = &#123;&#125;</span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Compute the backward pass, computing the derivatives of the weights #</span></span><br><span class="line">    <span class="comment"># and biases. Store the results in the grads dictionary. For example,       #</span></span><br><span class="line">    <span class="comment"># grads['W1'] should store the gradient on W1, and be a matrix of same size #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">        pro_scores[np.arange(N), y] -= <span class="number">1</span></span><br><span class="line">        pro_scores /= N</span><br><span class="line">        </span><br><span class="line">        dW2 = score_ReLU.T.dot(pro_scores)</span><br><span class="line">        db2 = np.sum(pro_scores, axis=<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        dscore_ReLU = pro_scores.dot(W2.T)</span><br><span class="line">        dscore_ReLU[score_ReLU &lt;= <span class="number">0</span> ] = <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        dW1 = X.T.dot(dscore_ReLU)</span><br><span class="line">        db1 = np.sum(dscore_ReLU, axis=<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        dW1 += <span class="number">2</span>*reg*W1</span><br><span class="line">        dW2 += <span class="number">2</span>*reg*W2</span><br><span class="line">        </span><br><span class="line">        grads = &#123;<span class="string">'W1'</span>:dW1, <span class="string">'b1'</span>:db1, <span class="string">'W2'</span>:dW2, <span class="string">'b2'</span>:db2&#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment">#                              END OF YOUR CODE                             #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss, grads</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, X, y, X_val, y_val,</span></span></span><br><span class="line"><span class="function"><span class="params">            learning_rate=<span class="number">1e-3</span>, learning_rate_decay=<span class="number">0.95</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">            reg=<span class="number">5e-6</span>, num_iters=<span class="number">100</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">            batch_size=<span class="number">200</span>, verbose=False)</span>:</span></span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">    Train this neural network using stochastic gradient descent.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - X: A numpy array of shape (N, D) giving training data.</span></span><br><span class="line"><span class="string">    - y: A numpy array f shape (N,) giving training labels; y[i] = c means that</span></span><br><span class="line"><span class="string">      X[i] has label c, where 0 &lt;= c &lt; C.</span></span><br><span class="line"><span class="string">    - X_val: A numpy array of shape (N_val, D) giving validation data.</span></span><br><span class="line"><span class="string">    - y_val: A numpy array of shape (N_val,) giving validation labels.</span></span><br><span class="line"><span class="string">    - learning_rate: Scalar giving learning rate for optimization.</span></span><br><span class="line"><span class="string">    - learning_rate_decay: Scalar giving factor used to decay the learning rate</span></span><br><span class="line"><span class="string">      after each epoch.</span></span><br><span class="line"><span class="string">    - reg: Scalar giving regularization strength.</span></span><br><span class="line"><span class="string">    - num_iters: Number of steps to take when optimizing.</span></span><br><span class="line"><span class="string">    - batch_size: Number of training examples to use per step.</span></span><br><span class="line"><span class="string">    - verbose: boolean; if true print progress during optimization.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        num_train = X.shape[<span class="number">0</span>]</span><br><span class="line">        iterations_per_epoch = max(num_train / batch_size, <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Use SGD to optimize the parameters in self.model</span></span><br><span class="line">        loss_history = []</span><br><span class="line">        train_acc_history = []</span><br><span class="line">        val_acc_history = []</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> it <span class="keyword">in</span> xrange(num_iters):</span><br><span class="line">            </span><br><span class="line">            X_batch = <span class="literal">None</span></span><br><span class="line">            y_batch = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">      <span class="comment">#########################################################################</span></span><br><span class="line">      <span class="comment"># <span class="doctag">TODO:</span> Create a random minibatch of training data and labels, storing  #</span></span><br><span class="line">      <span class="comment"># them in X_batch and y_batch respectively.                             #</span></span><br><span class="line">      <span class="comment">#########################################################################</span></span><br><span class="line">            choice_index = np.random.choice(num_train, batch_size)</span><br><span class="line">            X_batch = X[choice_index]</span><br><span class="line">            y_batch = y[choice_index]</span><br><span class="line">      </span><br><span class="line">      <span class="comment">#########################################################################</span></span><br><span class="line">      <span class="comment">#                             END OF YOUR CODE                          #</span></span><br><span class="line">      <span class="comment">#########################################################################</span></span><br><span class="line"></span><br><span class="line">      <span class="comment"># Compute loss and gradients using the current minibatch</span></span><br><span class="line">            loss, grads = self.loss(X_batch, y=y_batch, reg=reg)</span><br><span class="line">            loss_history.append(loss)</span><br><span class="line"></span><br><span class="line">      <span class="comment">#########################################################################</span></span><br><span class="line">      <span class="comment"># <span class="doctag">TODO:</span> Use the gradients in the grads dictionary to update the         #</span></span><br><span class="line">      <span class="comment"># parameters of the network (stored in the dictionary self.params)      #</span></span><br><span class="line">      <span class="comment"># using stochastic gradient descent. You'll need to use the gradients   #</span></span><br><span class="line">      <span class="comment"># stored in the grads dictionary defined above.                         #</span></span><br><span class="line">      <span class="comment">#########################################################################</span></span><br><span class="line">            <span class="keyword">for</span> key <span class="keyword">in</span> self.params:</span><br><span class="line">                </span><br><span class="line">            </span><br><span class="line">                self.params[key] -= learning_rate * grads[key]</span><br><span class="line">      </span><br><span class="line">      <span class="comment">#########################################################################</span></span><br><span class="line">      <span class="comment">#                             END OF YOUR CODE                          #</span></span><br><span class="line">      <span class="comment">#########################################################################</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> verbose <span class="keyword">and</span> it % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                </span><br><span class="line">                print(<span class="string">'iteration %d / %d: loss %f'</span> % (it, num_iters, loss))</span><br><span class="line"></span><br><span class="line">      <span class="comment"># Every epoch, check train and val accuracy and decay learning rate.</span></span><br><span class="line">            <span class="keyword">if</span> it % iterations_per_epoch == <span class="number">0</span>:</span><br><span class="line">            </span><br><span class="line">        <span class="comment"># Check accuracy</span></span><br><span class="line">                train_acc = (self.predict(X_batch) == y_batch).mean()</span><br><span class="line">                val_acc = (self.predict(X_val) == y_val).mean()</span><br><span class="line">                train_acc_history.append(train_acc)</span><br><span class="line">                val_acc_history.append(val_acc)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Decay learning rate</span></span><br><span class="line">                learning_rate *= learning_rate_decay</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> &#123;</span><br><span class="line">      <span class="string">'loss_history'</span>: loss_history,</span><br><span class="line">      <span class="string">'train_acc_history'</span>: train_acc_history,</span><br><span class="line">      <span class="string">'val_acc_history'</span>: val_acc_history,</span><br><span class="line">       &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        </span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">    Use the trained weights of this two-layer network to predict labels for</span></span><br><span class="line"><span class="string">    data points. For each data point we predict scores for each of the C</span></span><br><span class="line"><span class="string">    classes, and assign each data point to the class with the highest score.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - X: A numpy array of shape (N, D) giving N D-dimensional data points to</span></span><br><span class="line"><span class="string">      classify.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - y_pred: A numpy array of shape (N,) giving predicted labels for each of</span></span><br><span class="line"><span class="string">      the elements of X. For all i, y_pred[i] = c means that X[i] is predicted</span></span><br><span class="line"><span class="string">       to have class c, where 0 &lt;= c &lt; C.</span></span><br><span class="line"><span class="string">       """</span></span><br><span class="line">        y_pred = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement this function; it should be VERY simple!                #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">        y_pred = np.argmax(self.loss(X), axis=<span class="number">1</span>)</span><br><span class="line">  </span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                              END OF YOUR CODE                           #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br></pre></td></tr></table></figure>
<h4 id="Checking"><a href="#Checking" class="headerlink" title="Checking"></a>Checking</h4><p>Now you can go to two_layer_net.ipynb to train your model, but before it you should check the algorithm you create to see whether it is correct. We use a simple dataset to check. </p>
<p><img src="/2020/02/18/Neural-Net/1582003562296.png" alt="1582003562296"></p>
<p><img src="/2020/02/18/Neural-Net/1582003592110.png" alt="1582003592110"></p>
<p><img src="/2020/02/18/Neural-Net/1582003605931.png" alt="1582003605931"></p>
<p><img src="/2020/02/18/Neural-Net/1582003620532.png" alt="1582003620532"></p>
<p><img src="/2020/02/18/Neural-Net/1582003637573.png" alt="1582003637573"></p>
<p>We can see that the loss and gradients we calculate using the algorithm we write have reach the requirements, so that we can go to the next step : training our model.</p>
<h4 id="Training-the-Model"><a href="#Training-the-Model" class="headerlink" title="Training the Model"></a>Training the Model</h4><p>First, we just set those parameters randomly and you can see that the validation accuracy is very low. </p>
<p><img src="/2020/02/18/Neural-Net/1582003790638.png" alt="1582003790638"></p>
<p><img src="/2020/02/18/Neural-Net/1582003850830.png" alt="1582003850830"></p>
<p><img src="/2020/02/18/Neural-Net/1582003872346.png" alt="1582003872346"></p>
<p><strong>What’s wrong?</strong>. Looking at the visualizations above, we see that the loss is decreasing more or less linearly, which seems to suggest that the learning rate may be too low. Moreover, there is no gap between the training and validation accuracy, suggesting that the model we used has low capacity, and that we should increase its size. On the other hand, with a very large model we would expect to see more overfitting, which would manifest itself as a very large gap between the training and validation accuracy.</p>
<p> <strong>Tuning</strong>. Tuning the hyperparameters and developing intuition for how they affect the final performance is a large part of using Neural Networks, so we want  to get a lot of practice. Below,we should experiment with different values of the various hyperparameters, including hidden layer size, learning rate, number of training epochs, and regularization strength. You might also consider tuning the learning rate decay, but you should be able to get good performance using the default value.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">best_net = <span class="literal">None</span> <span class="comment"># store the best model into this </span></span><br><span class="line"></span><br><span class="line"><span class="comment">#################################################################################</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span> Tune hyperparameters using the validation set. Store your best trained  #</span></span><br><span class="line"><span class="comment"># model in best_net.                                                            #</span></span><br><span class="line"><span class="comment">#                                                                               #</span></span><br><span class="line"><span class="comment"># To help debug your network, it may help to use visualizations similar to the  #</span></span><br><span class="line"><span class="comment"># ones we used above; these visualizations will have significant qualitative    #</span></span><br><span class="line"><span class="comment"># differences from the ones we saw above for the poorly tuned network.          #</span></span><br><span class="line"><span class="comment">#                                                                               #</span></span><br><span class="line"><span class="comment"># Tweaking hyperparameters by hand can be fun, but you might find it useful to  #</span></span><br><span class="line"><span class="comment"># write code to sweep through possible combinations of hyperparameters          #</span></span><br><span class="line"><span class="comment"># automatically like we did on the previous exercises.                          #</span></span><br><span class="line"><span class="comment">#################################################################################</span></span><br><span class="line">learning_rate = [<span class="number">0.9e-3</span>,<span class="number">1e-3</span>,<span class="number">1.1e-3</span>]</span><br><span class="line">hlsize = [<span class="number">80</span>,<span class="number">100</span>,<span class="number">120</span>, <span class="number">150</span>,<span class="number">180</span>]</span><br><span class="line">reg_strengths = [<span class="number">0.1</span>, <span class="number">0.15</span>,<span class="number">0.05</span>]</span><br><span class="line">learning_rate_decay = []</span><br><span class="line">best_val = <span class="number">-1</span></span><br><span class="line">results = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> h_i <span class="keyword">in</span> hlsize:</span><br><span class="line">    <span class="keyword">for</span> lr_i <span class="keyword">in</span> learning_rate:</span><br><span class="line">        <span class="keyword">for</span> rs_i <span class="keyword">in</span> reg_strengths:</span><br><span class="line">            net = TwoLayerNet(input_size, h_i, num_classes)</span><br><span class="line">            net.train(X_train, y_train, X_val, y_val, num_iters=<span class="number">2000</span>, batch_size=<span class="number">200</span>,</span><br><span class="line">                learning_rate=lr_i, learning_rate_decay=<span class="number">0.95</span>,</span><br><span class="line">                reg=rs_i, verbose=<span class="literal">True</span>)</span><br><span class="line">            y_train_pred = net.predict(X_train)</span><br><span class="line">            y_val_pred = net.predict(X_val)</span><br><span class="line">            y_train_acc = np.mean(y_train == y_train_pred)</span><br><span class="line">            y_val_acc = np.mean(y_val == y_val_pred)</span><br><span class="line">            results[(lr_i, rs_i)] = y_train_acc , y_val_acc</span><br><span class="line">            <span class="keyword">if</span> y_val_acc &gt; best_val:</span><br><span class="line">                best_val = y_val_acc</span><br><span class="line">                best_net = net</span><br><span class="line"></span><br><span class="line"><span class="comment">#################################################################################</span></span><br><span class="line"><span class="comment">#                               END OF YOUR CODE                                #</span></span><br><span class="line"><span class="comment">#################################################################################</span></span><br></pre></td></tr></table></figure>
<p><img src="/2020/02/18/Neural-Net/1582003979640.png" alt="1582003979640"></p>
<p>Finally, we get the 0.528 test accuracy. Consequently, the parameters of neural network are very crucial for its performance. Getting the best parameters can take a long time for you should change them every time based on the last performance. When you get the best or near best parameters, the neural network can greatly improve the accuracy.</p>
<p><img src="/2020/02/18/Neural-Net/1582004046620.png" alt="1582004046620"></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Python/" rel="tag"># Python</a>
              <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/02/16/Softmax/" rel="prev" title="Softmax">
      <i class="fa fa-chevron-left"></i> Softmax
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/02/24/Fully-Connected-Networks/" rel="next" title="Fully-Connected Networks">
      Fully-Connected Networks <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#Problem-Overview"><span class="nav-number">1.</span> <span class="nav-text">Problem Overview</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Neural-Network"><span class="nav-number">2.</span> <span class="nav-text">Neural Network</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#null"><span class="nav-number">3.</span> <span class="nav-text"> </span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Checking"><span class="nav-number">4.</span> <span class="nav-text">Checking</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Training-the-Model"><span class="nav-number">5.</span> <span class="nav-text">Training the Model</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Bania</p>
  <div class="site-description" itemprop="description">A platform for discussing programming and technology</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">14</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Bania</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.7.1
  </div>

<div>
<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<span id="busuanzi_container_site_pv" style='display:none'>
  Total visits: <span id="busuanzi_value_site_pv"></span> times
  <span class="post-meta-divider">|</span>
</span>
<span id="busuanzi_container_site_uv" style='display:none'>
  <span id="busuanzi_value_site_uv"></span>people have viewed my bolg.
</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
        loader: {
          load: ['[tex]/mhchem']
        },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
