<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Pisces","version":"7.7.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Previously, we have learn how to build a Two-Layers Network with modular programming in which we have implemented forward&#x2F;backward functions for different layers and also combine some layers together.">
<meta property="og:type" content="article">
<meta property="og:title" content="Fully-Connected Networks">
<meta property="og:url" content="http://yoursite.com/2020/02/24/Fully-Connected-Networks/index.html">
<meta property="og:site_name" content="BaniaBlog">
<meta property="og:description" content="Previously, we have learn how to build a Two-Layers Network with modular programming in which we have implemented forward&#x2F;backward functions for different layers and also combine some layers together.">
<meta property="og:locale" content="en_US">
<meta property="article:published_time" content="2020-02-24T01:40:42.000Z">
<meta property="article:modified_time" content="2020-03-02T04:31:44.966Z">
<meta property="article:author" content="Bania">
<meta property="article:tag" content="Python">
<meta property="article:tag" content=" Java">
<meta property="article:tag" content=" Machine Learning">
<meta property="article:tag" content=" Data Science">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://yoursite.com/2020/02/24/Fully-Connected-Networks/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>Fully-Connected Networks | BaniaBlog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="BaniaBlog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">BaniaBlog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">Sharing Technology</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/Zbx2017/Zbx2017.github.io" class="github-corner" title="Bania GitHub" aria-label="Bania GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
    
  
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/24/Fully-Connected-Networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Bania">
      <meta itemprop="description" content="A platform for discussing programming and technology">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BaniaBlog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          Fully-Connected Networks
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2020-02-24 09:40:42" itemprop="dateCreated datePublished" datetime="2020-02-24T09:40:42+08:00">2020-02-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2020-03-02 12:31:44" itemprop="dateModified" datetime="2020-03-02T12:31:44+08:00">2020-03-02</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>Previously, we have learn how to build a Two-Layers Network with modular programming in which we have implemented forward/backward functions for different layers and also combine some layers together. In this part we will continue to do the same work but will build a more complex network adding Batch Normalization and Dropout Layers.</p>
<h4 id="Layers-Modularization"><a href="#Layers-Modularization" class="headerlink" title="Layers Modularization"></a>Layers Modularization</h4><p>For fully-connected network, the architecture of the networok is like that: </p>
<p><code>{affine - [batch norm] - relu - [dropout]} x (L - 1) - affine - softmax</code></p>
<p>Note that Batch Normalization and Dropout are not necessary and we have added parameters for users to decide whether they should use them. In Two-Layers network we combine affine and relu together, and this time we should create <code>affine-batch norm-relu</code>, <code>affine-relu-dropout</code> and <code>affine-batch norm-relu-dropout</code> in layers_utils.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">abr_forward</span><span class="params">(x,w,b,gamma,beta,bn_param)</span>:</span></span><br><span class="line">    </span><br><span class="line">    a, aff_cache = affine_forward(x,w,b)</span><br><span class="line">    b, batch_cache = batchnorm_forward(a, gamma, beta, bn_param)</span><br><span class="line">    out, relu_cache = relu_forward(b)</span><br><span class="line">    cache = (aff_cache, batch_cache, relu_cache)</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">abr_backward</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    aff_cache, batch_cache, relu_cache = cache</span><br><span class="line">    dr = relu_backward(dout, relu_cache)</span><br><span class="line">    dbatch, dgamma, dbeta = batchnorm_backward(dr, batch_cache)</span><br><span class="line">    dx, dw, db = affine_backward(dbatch, aff_cache)</span><br><span class="line">    <span class="keyword">return</span> dx, dw, db, dgamma, dbeta</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ard_forward</span><span class="params">(x, w, b, dropout_param)</span>:</span></span><br><span class="line">    ar_out, ar_cache = affine_relu_forward(x,w,b)</span><br><span class="line">    out, d_cache = dropout_forward(ar_out, dropout_param)</span><br><span class="line">    cache = (ar_cache, d_cache)</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">ard_backward</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    ar_cache, d_cache = cache</span><br><span class="line">    ddropout = dropout_backward(dout, d_cache)</span><br><span class="line">    dx, dw, db = affine_relu_backward(ddropout, ar_cache)</span><br><span class="line">    <span class="keyword">return</span> dx, dw, db</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">abrd_forward</span><span class="params">(x, w, b,gamma,beta,bn_param, dropout_param)</span>:</span></span><br><span class="line">    abr_out, abr_cache = abr_forward(x, w, b,gamma,beta,bn_param)</span><br><span class="line">    out, d_cache = dropout_forward(abr_out, dropout_param)</span><br><span class="line">    cache = (abr_cache, d_cache)</span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">abrd_backward</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    abr_cache, d_cache = cache</span><br><span class="line">    ddropout = dropout_backward(dout, d_cache)</span><br><span class="line">    dx, dw, db, dgamma, dbeta = abr_backward(ddropout, abr_cache)</span><br><span class="line">    <span class="keyword">return</span> dx, dw, db, dgamma, dbeta</span><br></pre></td></tr></table></figure>
<p>Then, we can turn to fc_net.py to finish all the exercises. </p>
<h4 id="Build-Fully-Connected-Network"><a href="#Build-Fully-Connected-Network" class="headerlink" title="Build Fully-Connected Network"></a>Build Fully-Connected Network</h4><p>First, we need to initialize weights, bias, gamma and beta. Note that we do not know the layer number of  the network, so it might be a little difficult to find the correct size of each parameters. The easier way is to assume a network with certain layers and try to write the general code  to satisfy the demands. This idea is also suitable for next part of exercise and you can use this technique to debug your code. Then, we have to calculate loss and gradients. It is easier to use the functions we have created in layers_utils.py, if not you will find it hard to implement the method. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">FullyConnectedNet</span><span class="params">(object)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    A fully-connected neural network with an arbitrary number of hidden layers,</span></span><br><span class="line"><span class="string">    ReLU nonlinearities, and a softmax loss function. This will also implement</span></span><br><span class="line"><span class="string">    dropout and batch normalization as options. For a network with L layers,</span></span><br><span class="line"><span class="string">    the architecture will be</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    &#123;affine - [batch norm] - relu - [dropout]&#125; x (L - 1) - affine - softmax</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    where batch normalization and dropout are optional, and the &#123;...&#125; block is</span></span><br><span class="line"><span class="string">    repeated L - 1 times.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Similar to the TwoLayerNet above, learnable parameters are stored in the</span></span><br><span class="line"><span class="string">    self.params dictionary and will be learned using the Solver class.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, hidden_dims, input_dim=<span class="number">3</span>*<span class="number">32</span>*<span class="number">32</span>, num_classes=<span class="number">10</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 dropout=<span class="number">0</span>, use_batchnorm=False, reg=<span class="number">0.0</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">                 weight_scale=<span class="number">1e-2</span>, dtype=np.float32, seed=None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Initialize a new FullyConnectedNet.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Inputs:</span></span><br><span class="line"><span class="string">        - hidden_dims: A list of integers giving the size of each hidden layer.</span></span><br><span class="line"><span class="string">        - input_dim: An integer giving the size of the input.</span></span><br><span class="line"><span class="string">        - num_classes: An integer giving the number of classes to classify.</span></span><br><span class="line"><span class="string">        - dropout: Scalar between 0 and 1 giving dropout strength. If dropout=0 then</span></span><br><span class="line"><span class="string">          the network should not use dropout at all.</span></span><br><span class="line"><span class="string">        - use_batchnorm: Whether or not the network should use batch normalization.</span></span><br><span class="line"><span class="string">        - reg: Scalar giving L2 regularization strength.</span></span><br><span class="line"><span class="string">        - weight_scale: Scalar giving the standard deviation for random</span></span><br><span class="line"><span class="string">          initialization of the weights.</span></span><br><span class="line"><span class="string">        - dtype: A numpy datatype object; all computations will be performed using</span></span><br><span class="line"><span class="string">          this datatype. float32 is faster but less accurate, so you should use</span></span><br><span class="line"><span class="string">          float64 for numeric gradient checking.</span></span><br><span class="line"><span class="string">        - seed: If not None, then pass this random seed to the dropout layers. This</span></span><br><span class="line"><span class="string">          will make the dropout layers deteriminstic so we can gradient check the</span></span><br><span class="line"><span class="string">          model.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.use_batchnorm = use_batchnorm</span><br><span class="line">        self.use_dropout = dropout &gt; <span class="number">0</span></span><br><span class="line">        self.reg = reg</span><br><span class="line">        self.num_layers = <span class="number">1</span> + len(hidden_dims)</span><br><span class="line">        self.dtype = dtype</span><br><span class="line">        self.params = &#123;&#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Initialize the parameters of the network, storing all values in    #</span></span><br><span class="line">        <span class="comment"># the self.params dictionary. Store weights and biases for the first layer #</span></span><br><span class="line">        <span class="comment"># in W1 and b1; for the second layer use W2 and b2, etc. Weights should be #</span></span><br><span class="line">        <span class="comment"># initialized from a normal distribution with standard deviation equal to  #</span></span><br><span class="line">        <span class="comment"># weight_scale and biases should be initialized to zero.                   #</span></span><br><span class="line">        <span class="comment">#                                                                          #</span></span><br><span class="line">        <span class="comment"># When using batch normalization, store scale and shift parameters for the #</span></span><br><span class="line">        <span class="comment"># first layer in gamma1 and beta1; for the second layer use gamma2 and     #</span></span><br><span class="line">        <span class="comment"># beta2, etc. Scale parameters should be initialized to one and shift      #</span></span><br><span class="line">        <span class="comment"># parameters should be initialized to zero.                                #</span></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        self.params[<span class="string">'W1'</span>] = weight_scale*np.random.randn(input_dim, hidden_dims[<span class="number">0</span>])</span><br><span class="line">        self.params[<span class="string">'b1'</span>] = np.zeros(hidden_dims[<span class="number">0</span>])</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(len(hidden_dims)<span class="number">-1</span>):</span><br><span class="line">            self.params[<span class="string">'W'</span>+str(i+<span class="number">2</span>)] = weight_scale*np.random.randn(hidden_dims[i], hidden_dims[i+<span class="number">1</span>])</span><br><span class="line">            self.params[<span class="string">'b'</span>+str(i+<span class="number">2</span>)] = np.zeros(hidden_dims[i+<span class="number">1</span>])</span><br><span class="line">            </span><br><span class="line">        self.params[<span class="string">'W'</span>+str(self.num_layers)] = weight_scale*np.random.randn(hidden_dims[<span class="number">-1</span>], num_classes)</span><br><span class="line">        self.params[<span class="string">'b'</span>+str(self.num_layers)] = np.zeros(num_classes)</span><br><span class="line">        <span class="keyword">if</span> self.use_batchnorm:</span><br><span class="line">            <span class="keyword">for</span> j <span class="keyword">in</span> range(len(hidden_dims)):</span><br><span class="line">                self.params[<span class="string">'gamma'</span>+ str(j+<span class="number">1</span>)] = np.zeros(hidden_dims[j])+<span class="number">1</span></span><br><span class="line">                self.params[<span class="string">'beta'</span>+ str(j+<span class="number">1</span>)] = np.zeros(hidden_dims[j])</span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        <span class="comment">#                             END OF YOUR CODE                             #</span></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># When using dropout we need to pass a dropout_param dictionary to each</span></span><br><span class="line">        <span class="comment"># dropout layer so that the layer knows the dropout probability and the mode</span></span><br><span class="line">        <span class="comment"># (train / test). You can pass the same dropout_param to each dropout layer.</span></span><br><span class="line">        self.dropout_param = &#123;&#125;</span><br><span class="line">        <span class="keyword">if</span> self.use_dropout:</span><br><span class="line">            self.dropout_param = &#123;<span class="string">'mode'</span>: <span class="string">'train'</span>, <span class="string">'p'</span>: dropout&#125;</span><br><span class="line">            <span class="keyword">if</span> seed <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                self.dropout_param[<span class="string">'seed'</span>] = seed</span><br><span class="line"></span><br><span class="line">        <span class="comment"># With batch normalization we need to keep track of running means and</span></span><br><span class="line">        <span class="comment"># variances, so we need to pass a special bn_param object to each batch</span></span><br><span class="line">        <span class="comment"># normalization layer. You should pass self.bn_params[0] to the forward pass</span></span><br><span class="line">        <span class="comment"># of the first batch normalization layer, self.bn_params[1] to the forward</span></span><br><span class="line">        <span class="comment"># pass of the second batch normalization layer, etc.</span></span><br><span class="line">        self.bn_params = []</span><br><span class="line">        <span class="keyword">if</span> self.use_batchnorm:</span><br><span class="line">            self.bn_params = [&#123;<span class="string">'mode'</span>: <span class="string">'train'</span>&#125; <span class="keyword">for</span> i <span class="keyword">in</span> range(self.num_layers - <span class="number">1</span>)]</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Cast all parameters to the correct datatype</span></span><br><span class="line">        <span class="keyword">for</span> k, v <span class="keyword">in</span> self.params.items():</span><br><span class="line">            self.params[k] = v.astype(dtype)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(self, X, y=None)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Compute loss and gradient for the fully-connected net.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">        Input / output: Same as TwoLayerNet above.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        X = X.astype(self.dtype)</span><br><span class="line">        mode = <span class="string">'test'</span> <span class="keyword">if</span> y <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">else</span> <span class="string">'train'</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># Set train/test mode for batchnorm params and dropout param since they</span></span><br><span class="line">        <span class="comment"># behave differently during training and testing.</span></span><br><span class="line">        <span class="keyword">if</span> self.use_dropout:</span><br><span class="line">            self.dropout_param[<span class="string">'mode'</span>] = mode</span><br><span class="line">        <span class="keyword">if</span> self.use_batchnorm:</span><br><span class="line">            <span class="keyword">for</span> bn_param <span class="keyword">in</span> self.bn_params:</span><br><span class="line">                bn_param[<span class="string">'mode'</span>] = mode</span><br><span class="line"></span><br><span class="line">        scores = <span class="literal">None</span></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Implement the forward pass for the fully-connected net, computing  #</span></span><br><span class="line">        <span class="comment"># the class scores for X and storing them in the scores variable.          #</span></span><br><span class="line">        <span class="comment">#                                                                          #</span></span><br><span class="line">        <span class="comment"># When using dropout, you'll need to pass self.dropout_param to each       #</span></span><br><span class="line">        <span class="comment"># dropout forward pass.                                                    #</span></span><br><span class="line">        <span class="comment">#                                                                          #</span></span><br><span class="line">        <span class="comment"># When using batch normalization, you'll need to pass self.bn_params[0] to #</span></span><br><span class="line">        <span class="comment"># the forward pass for the first batch normalization layer, pass           #</span></span><br><span class="line">        <span class="comment"># self.bn_params[1] to the forward pass for the second batch normalization #</span></span><br><span class="line">        <span class="comment"># layer, etc.                                                              #</span></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        L=self.num_layers </span><br><span class="line">        cache = [<span class="literal">None</span>]*L</span><br><span class="line">        <span class="keyword">if</span> self.use_batchnorm:</span><br><span class="line">            <span class="keyword">if</span> self.use_dropout:                </span><br><span class="line">                 <span class="keyword">for</span> i <span class="keyword">in</span> range(L<span class="number">-1</span>):</span><br><span class="line">                    <span class="keyword">if</span> i == <span class="number">0</span> :</span><br><span class="line">                        scores,cache[i] = abrd_forward(X, self.params[<span class="string">'W1'</span>], </span><br><span class="line">                                                       self.params[<span class="string">'b1'</span>], self.params[<span class="string">'gamma1'</span>], </span><br><span class="line">                                                       self.params[<span class="string">'beta1'</span>], self.bn_params[i],self.dropout_param )</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                         scores,cache[i] = abrd_forward(scores, self.params[<span class="string">'W'</span>+str(i+<span class="number">1</span>)], </span><br><span class="line">                                                        self.params[<span class="string">'b'</span>+str(i+<span class="number">1</span>)], </span><br><span class="line">                                                      self.params[<span class="string">'gamma'</span>+str(i+<span class="number">1</span>)], self.params[<span class="string">'beta'</span>+str(i+<span class="number">1</span>)],</span><br><span class="line">                                                        self.bn_params[i], self.dropout_param)                 </span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> range(L<span class="number">-1</span>):</span><br><span class="line">                    <span class="keyword">if</span> i == <span class="number">0</span> :</span><br><span class="line">                        scores,cache[i] = abr_forward(X, self.params[<span class="string">'W1'</span>], self.params[<span class="string">'b1'</span>], </span><br><span class="line">                                                      self.params[<span class="string">'gamma1'</span>], self.params[<span class="string">'beta1'</span>], self.bn_params[i])</span><br><span class="line">                    <span class="keyword">else</span>:</span><br><span class="line">                         scores,cache[i] = abr_forward(scores, self.params[<span class="string">'W'</span>+str(i+<span class="number">1</span>)], self.params[<span class="string">'b'</span>+str(i+<span class="number">1</span>)], </span><br><span class="line">                                                      self.params[<span class="string">'gamma'</span>+str(i+<span class="number">1</span>)], self.params[<span class="string">'beta'</span>+str(i+<span class="number">1</span>)], self.bn_params[i])                               </span><br><span class="line">        <span class="keyword">elif</span> self.use_dropout:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(L<span class="number">-1</span>):                </span><br><span class="line">                <span class="keyword">if</span> i ==<span class="number">0</span> :</span><br><span class="line">                    scores,cache[i] = ard_forward(X, self.params[<span class="string">'W1'</span>], self.params[<span class="string">'b1'</span>], self.dropout_param) </span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    scores,cache[i] = ard_forward(scores, self.params[<span class="string">'W'</span>+str(i+<span class="number">1</span>)], self.params[<span class="string">'b'</span>+str(i+<span class="number">1</span>)], self.dropout_param) </span><br><span class="line">                        </span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(L<span class="number">-1</span>):</span><br><span class="line">                <span class="keyword">if</span> i ==<span class="number">0</span> :</span><br><span class="line">                    scores,cache[i] = affine_relu_forward(X, self.params[<span class="string">'W1'</span>], self.params[<span class="string">'b1'</span>]) </span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    scores,cache[i] = affine_relu_forward(scores, self.params[<span class="string">'W'</span>+str(i+<span class="number">1</span>)], self.params[<span class="string">'b'</span>+str(i+<span class="number">1</span>)]) </span><br><span class="line">                    </span><br><span class="line">                    </span><br><span class="line">        scores,cache[L<span class="number">-1</span>] = affine_forward(scores, self.params[<span class="string">'W'</span>+str(L)], self.params[<span class="string">'b'</span>+str(L)])</span><br><span class="line">                    </span><br><span class="line"></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        <span class="comment">#                             END OF YOUR CODE                             #</span></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># If test mode return early</span></span><br><span class="line">        <span class="keyword">if</span> mode == <span class="string">'test'</span>:</span><br><span class="line">            <span class="keyword">return</span> scores</span><br><span class="line"></span><br><span class="line">        loss, grads = <span class="number">0.0</span>, &#123;&#125;</span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Implement the backward pass for the fully-connected net. Store the #</span></span><br><span class="line">        <span class="comment"># loss in the loss variable and gradients in the grads dictionary. Compute #</span></span><br><span class="line">        <span class="comment"># data loss using softmax, and make sure that grads[k] holds the gradients #</span></span><br><span class="line">        <span class="comment"># for self.params[k]. Don't forget to add L2 regularization!               #</span></span><br><span class="line">        <span class="comment">#                                                                          #</span></span><br><span class="line">        <span class="comment"># When using batch normalization, you don't need to regularize the scale   #</span></span><br><span class="line">        <span class="comment"># and shift parameters.                                                    #</span></span><br><span class="line">        <span class="comment">#                                                                          #</span></span><br><span class="line">        <span class="comment"># <span class="doctag">NOTE:</span> To ensure that your implementation matches ours and you pass the   #</span></span><br><span class="line">        <span class="comment"># automated tests, make sure that your L2 regularization includes a factor #</span></span><br><span class="line">        <span class="comment"># of 0.5 to simplify the expression for the gradient.                      #</span></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        loss, pro_scores = softmax_loss(scores,y)</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(L):</span><br><span class="line">            loss += <span class="number">0.5</span>*self.reg*np.sum(self.params[<span class="string">'W'</span>+str(i+<span class="number">1</span>)]*self.params[<span class="string">'W'</span>+str(i+<span class="number">1</span>)])</span><br><span class="line">        <span class="keyword">if</span> self.use_batchnorm:</span><br><span class="line">            <span class="keyword">if</span> self.use_dropout:</span><br><span class="line">                </span><br><span class="line">                dhidden, grads[<span class="string">'W'</span>+str(L)], grads[<span class="string">'b'</span>+str(L)] = affine_backward(pro_scores, cache[L<span class="number">-1</span>])</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> range(L<span class="number">-2</span>,<span class="number">-1</span>,<span class="number">-1</span>):</span><br><span class="line">                    dhidden, grads[<span class="string">'W'</span>+str(i+<span class="number">1</span>)], grads[<span class="string">'b'</span>+str(i+<span class="number">1</span>)],  grads[<span class="string">'gamma'</span>+str(i+<span class="number">1</span>)], </span><br><span class="line">                    grads[<span class="string">'beta'</span>+str(i+<span class="number">1</span>)]= abrd_backward(dhidden, cache[i])</span><br><span class="line">                    </span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                dhidden, grads[<span class="string">'W'</span>+str(L)], grads[<span class="string">'b'</span>+str(L)] = affine_backward(pro_scores, cache[L<span class="number">-1</span>])</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> range(L<span class="number">-2</span>,<span class="number">-1</span>,<span class="number">-1</span>):</span><br><span class="line">                    dhidden, grads[<span class="string">'W'</span>+str(i+<span class="number">1</span>)], grads[<span class="string">'b'</span>+str(i+<span class="number">1</span>)],  grads[<span class="string">'gamma'</span>+str(i+<span class="number">1</span>)],</span><br><span class="line">                    grads[<span class="string">'beta'</span>+str(i+<span class="number">1</span>)]= abr_backward(dhidden, cache[i])</span><br><span class="line">  </span><br><span class="line">        <span class="keyword">elif</span> self.use_dropout: </span><br><span class="line">            dhidden, grads[<span class="string">'W'</span>+str(L)], grads[<span class="string">'b'</span>+str(L)] = affine_backward(pro_scores, cache[L<span class="number">-1</span>])</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(L<span class="number">-2</span>,<span class="number">-1</span>,<span class="number">-1</span>):</span><br><span class="line">                dhidden, grads[<span class="string">'W'</span>+str(i+<span class="number">1</span>)], grads[<span class="string">'b'</span>+str(i+<span class="number">1</span>)] = ard_backward(dhidden, cache[i])</span><br><span class="line">                          </span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            dhidden, grads[<span class="string">'W'</span>+str(L)], grads[<span class="string">'b'</span>+str(L)] = affine_backward(pro_scores, cache[L<span class="number">-1</span>])</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(L<span class="number">-2</span>,<span class="number">-1</span>,<span class="number">-1</span>):</span><br><span class="line">                dhidden, grads[<span class="string">'W'</span>+str(i+<span class="number">1</span>)], grads[<span class="string">'b'</span>+str(i+<span class="number">1</span>)] = affine_relu_backward(dhidden, cache[i])</span><br><span class="line">                </span><br><span class="line">                </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(L):</span><br><span class="line">            grads[<span class="string">'W'</span>+str(i+<span class="number">1</span>)] += self.reg*self.params[<span class="string">'W'</span>+str(i+<span class="number">1</span>)]</span><br><span class="line">            grads[<span class="string">'b'</span>+str(i+<span class="number">1</span>)] += self.reg*self.params[<span class="string">'b'</span>+str(i+<span class="number">1</span>)]</span><br><span class="line">            </span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line">        <span class="comment">#                             END OF YOUR CODE                             #</span></span><br><span class="line">        <span class="comment">############################################################################</span></span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss, grads</span><br></pre></td></tr></table></figure>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/02/18/Neural-Net/" rel="prev" title="Neural Net">
      <i class="fa fa-chevron-left"></i> Neural Net
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/02/24/Two-Layer-Network/" rel="next" title="Two-Layer Network">
      Two-Layer Network <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#Layers-Modularization"><span class="nav-number">1.</span> <span class="nav-text">Layers Modularization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Build-Fully-Connected-Network"><span class="nav-number">2.</span> <span class="nav-text">Build Fully-Connected Network</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Bania</p>
  <div class="site-description" itemprop="description">A platform for discussing programming and technology</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">17</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Bania</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.7.1
  </div>

<div>
<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<span id="busuanzi_container_site_pv" style='display:none'>
  Total visits: <span id="busuanzi_value_site_pv"></span> times
  <span class="post-meta-divider">|</span>
</span>
<span id="busuanzi_container_site_uv" style='display:none'>
  <span id="busuanzi_value_site_uv"></span>people have viewed my bolg.
</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

  

</body>
</html>
