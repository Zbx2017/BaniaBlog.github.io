<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Pisces","version":"7.7.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Weight InitializationIn last exercise, we learn how to write modular code for the Two-Layer neural network, which makes it easier to build a neural-network model. Today, we will dive deep into it and">
<meta property="og:type" content="article">
<meta property="og:title" content="BatchNormalization">
<meta property="og:url" content="http://yoursite.com/2020/03/01/BatchNormalization/index.html">
<meta property="og:site_name" content="BaniaBlog">
<meta property="og:description" content="Weight InitializationIn last exercise, we learn how to write modular code for the Two-Layer neural network, which makes it easier to build a neural-network model. Today, we will dive deep into it and">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://yoursite.com/2020/03/01/BatchNormalization/1583061383633.png">
<meta property="og:image" content="http://yoursite.com/2020/03/01/BatchNormalization/1583062312769.png">
<meta property="og:image" content="http://yoursite.com/2020/03/01/BatchNormalization/1583062559917.png">
<meta property="og:image" content="http://yoursite.com/2020/03/01/BatchNormalization/1583062577631.png">
<meta property="og:image" content="http://yoursite.com/2020/03/01/BatchNormalization/1583062593173.png">
<meta property="og:image" content="http://yoursite.com/2020/03/01/BatchNormalization/1583062611523.png">
<meta property="og:image" content="http://yoursite.com/2020/03/01/BatchNormalization/1583063126691.png">
<meta property="og:image" content="http://yoursite.com/2020/03/01/BatchNormalization/1583063140003.png">
<meta property="article:published_time" content="2020-03-01T09:15:29.000Z">
<meta property="article:modified_time" content="2020-03-01T11:47:28.547Z">
<meta property="article:author" content="Bania">
<meta property="article:tag" content="Python">
<meta property="article:tag" content=" Java">
<meta property="article:tag" content=" Machine Learning">
<meta property="article:tag" content=" Data Science">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/2020/03/01/BatchNormalization/1583061383633.png">

<link rel="canonical" href="http://yoursite.com/2020/03/01/BatchNormalization/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>BatchNormalization | BaniaBlog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="BaniaBlog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">BaniaBlog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">Sharing Technology</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/Zbx2017/Zbx2017.github.io" class="github-corner" title="Bania GitHub" aria-label="Bania GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
    
  
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/03/01/BatchNormalization/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Bania">
      <meta itemprop="description" content="A platform for discussing programming and technology">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BaniaBlog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          BatchNormalization
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-03-01 17:15:29 / Modified: 19:47:28" itemprop="dateCreated datePublished" datetime="2020-03-01T17:15:29+08:00">2020-03-01</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h4 id="Weight-Initialization"><a href="#Weight-Initialization" class="headerlink" title="Weight Initialization"></a>Weight Initialization</h4><p>In last exercise, we learn how to write modular code for the Two-Layer neural network, which makes it easier to build a neural-network model. Today, we will dive deep into it and talk about Batch Normalization which is a method to deal with the problems associated with weights initialization. </p>
<p>Note that we do not know the final value of every weight , but it is more reasonable to assume that approximately half of the weights will be positive and half of them will be negative. Thus, a reasonable-sounding idea might be set all the weights to zero. This turns out to be a mistake because if every neuron in the network computes the same output, then the same gradients during back propagation will be obtained and parameters updates will be the same. </p>
<p>Therefore, we want the weights to be random but very close to zero. The implementation for one weight matrix might look like<code>W = 0.01*np.random.randn(D, H)</code>, then we can get a weight matrix with zero mean and unit standard deviation. </p>
<p>However, here comes the problem of weights initialization that with more layers, we have to be more careful to set the learning rate and the learning rate usually should be very small. This can waste a lot of time for training. We can see this problem when we used a five-layers network to train 50 images in last exercise. This is because the distribution of each layer’s input changes during training as the parameters of the previous layers every time we initialize change. We refer this phenomenon as <strong>internal covariate shift</strong> and we address this problem by normalizing layer inputs or we can call it Batch normalization. </p>
<h4 id="Batch-Normalization"><a href="#Batch-Normalization" class="headerlink" title="Batch Normalization"></a>Batch Normalization</h4><p>When we use Batch Normalization, we make it as a part of the model architecture and perform the normalization for each training mini-batch. Batch Normalization allows us to use larger learning rates and be less careful about initialization. The implementation of Batch Normalization is quite straightforward. You can click the link for more information about it :<a href="https://arxiv.org/abs/1502.03167" target="_blank" rel="noopener">https://arxiv.org/abs/1502.03167</a></p>
<p>The forward batch normalization algorithm is like that: </p>
<p><img src="/2020/03/01/BatchNormalization/1583061383633.png" alt="1583061383633"> </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_forward</span><span class="params">(x, gamma, beta, bn_param)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Forward pass for batch normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    During training the sample mean and (uncorrected) sample variance are</span></span><br><span class="line"><span class="string">    computed from minibatch statistics and used to normalize the incoming data.</span></span><br><span class="line"><span class="string">    During training we also keep an exponentially decaying running mean of the</span></span><br><span class="line"><span class="string">    mean and variance of each feature, and these averages are used to normalize</span></span><br><span class="line"><span class="string">    data at test-time.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    At each timestep we update the running averages for mean and variance using</span></span><br><span class="line"><span class="string">    an exponential decay based on the momentum parameter:</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    running_mean = momentum * running_mean + (1 - momentum) * sample_mean</span></span><br><span class="line"><span class="string">    running_var = momentum * running_var + (1 - momentum) * sample_var</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Note that the batch normalization paper suggests a different test-time</span></span><br><span class="line"><span class="string">    behavior: they compute sample mean and variance for each feature using a</span></span><br><span class="line"><span class="string">    large number of training images rather than using a running average. For</span></span><br><span class="line"><span class="string">    this implementation we have chosen to use running averages instead since</span></span><br><span class="line"><span class="string">    they do not require an additional estimation step; the torch7</span></span><br><span class="line"><span class="string">    implementation of batch normalization also uses running averages.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Input:</span></span><br><span class="line"><span class="string">    - x: Data of shape (N, D)</span></span><br><span class="line"><span class="string">    - gamma: Scale parameter of shape (D,)</span></span><br><span class="line"><span class="string">    - beta: Shift paremeter of shape (D,)</span></span><br><span class="line"><span class="string">    - bn_param: Dictionary with the following keys:</span></span><br><span class="line"><span class="string">      - mode: 'train' or 'test'; required</span></span><br><span class="line"><span class="string">      - eps: Constant for numeric stability</span></span><br><span class="line"><span class="string">      - momentum: Constant for running mean / variance.</span></span><br><span class="line"><span class="string">      - running_mean: Array of shape (D,) giving running mean of features</span></span><br><span class="line"><span class="string">      - running_var Array of shape (D,) giving running variance of features</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - out: of shape (N, D)</span></span><br><span class="line"><span class="string">    - cache: A tuple of values needed in the backward pass</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    mode = bn_param[<span class="string">'mode'</span>]</span><br><span class="line">    eps = bn_param.get(<span class="string">'eps'</span>, <span class="number">1e-5</span>)</span><br><span class="line">    momentum = bn_param.get(<span class="string">'momentum'</span>, <span class="number">0.9</span>)</span><br><span class="line"></span><br><span class="line">    N, D = x.shape</span><br><span class="line">    running_mean = bn_param.get(<span class="string">'running_mean'</span>, np.zeros(D, dtype=x.dtype))</span><br><span class="line">    running_var = bn_param.get(<span class="string">'running_var'</span>, np.zeros(D, dtype=x.dtype))</span><br><span class="line"></span><br><span class="line">    out, cache = <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    <span class="keyword">if</span> mode == <span class="string">'train'</span>:</span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Implement the training-time forward pass for batch norm.      #</span></span><br><span class="line">        <span class="comment"># Use minibatch statistics to compute the mean and variance, use      #</span></span><br><span class="line">        <span class="comment"># these statistics to normalize the incoming data, and scale and      #</span></span><br><span class="line">        <span class="comment"># shift the normalized data using gamma and beta.                     #</span></span><br><span class="line">        <span class="comment">#                                                                     #</span></span><br><span class="line">        <span class="comment"># You should store the output in the variable out. Any intermediates  #</span></span><br><span class="line">        <span class="comment"># that you need for the backward pass should be stored in the cache   #</span></span><br><span class="line">        <span class="comment"># variable.                                                           #</span></span><br><span class="line">        <span class="comment">#                                                                     #</span></span><br><span class="line">        <span class="comment"># You should also use your computed sample mean and variance together #</span></span><br><span class="line">        <span class="comment"># with the momentum variable to update the running mean and running   #</span></span><br><span class="line">        <span class="comment"># variance, storing your result in the running_mean and running_var   #</span></span><br><span class="line">        <span class="comment"># variables.                                                          #</span></span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        </span><br><span class="line">        x_mean = np.mean(x, axis=<span class="number">0</span>)</span><br><span class="line">        x_var = np.mean((x - x_mean)**<span class="number">2</span>, axis=<span class="number">0</span>)</span><br><span class="line">        x_hat = (x - x_mean) / np.sqrt(x_var + eps)</span><br><span class="line">        out = gamma*x_hat + beta</span><br><span class="line">        </span><br><span class="line">        running_mean = momentum * running_mean + (<span class="number">1</span> - momentum) * x_mean</span><br><span class="line">        running_var = momentum * running_var + (<span class="number">1</span> - momentum) * x_var</span><br><span class="line">        </span><br><span class="line">        cache =( x,x_hat, gamma,  x_mean, x_var, eps)</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment">#                           END OF YOUR CODE                          #</span></span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">    <span class="keyword">elif</span> mode == <span class="string">'test'</span>:</span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span> Implement the test-time forward pass for batch normalization. #</span></span><br><span class="line">        <span class="comment"># Use the running mean and variance to normalize the incoming data,   #</span></span><br><span class="line">        <span class="comment"># then scale and shift the normalized data using gamma and beta.      #</span></span><br><span class="line">        <span class="comment"># Store the result in the out variable.                               #</span></span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        x_hat = (x - running_mean)/np.sqrt(running_var + eps)</span><br><span class="line">        out = gamma*x_hat + beta</span><br><span class="line">        cahe =( x, x_hat,gamma, running_mean, running_var, eps)</span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">        <span class="comment">#                          END OF YOUR CODE                           #</span></span><br><span class="line">        <span class="comment">#######################################################################</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> ValueError(<span class="string">'Invalid forward batchnorm mode "%s"'</span> % mode)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Store the updated running means back into bn_param</span></span><br><span class="line">    bn_param[<span class="string">'running_mean'</span>] = running_mean</span><br><span class="line">    bn_param[<span class="string">'running_var'</span>] = running_var</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> out, cache</span><br></pre></td></tr></table></figure>
<p>For the backward algorithm, the paper has calculate the gradients for us and we can use these equations to write the code.</p>
<p><img src="/2020/03/01/BatchNormalization/1583062312769.png" alt="1583062312769"></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">batchnorm_backward</span><span class="params">(dout, cache)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Backward pass for batch normalization.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    For this implementation, you should write out a computation graph for</span></span><br><span class="line"><span class="string">    batch normalization on paper and propagate gradients backward through</span></span><br><span class="line"><span class="string">    intermediate nodes.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dout: Upstream derivatives, of shape (N, D)</span></span><br><span class="line"><span class="string">    - cache: Variable of intermediates from batchnorm_forward.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - dx: Gradient with respect to inputs x, of shape (N, D)</span></span><br><span class="line"><span class="string">    - dgamma: Gradient with respect to scale parameter gamma, of shape (D,)</span></span><br><span class="line"><span class="string">    - dbeta: Gradient with respect to shift parameter beta, of shape (D,)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    dx, dgamma, dbeta = <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span> Implement the backward pass for batch normalization. Store the    #</span></span><br><span class="line">    <span class="comment"># results in the dx, dgamma, and dbeta variables.                         #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    x,x_hat,gamma,mean, var ,eps = cache </span><br><span class="line">    dx_hat =dout *gamma</span><br><span class="line">    dvar = np.sum(dx_hat * (x - mean)*(var + eps)**(<span class="number">-3</span>/<span class="number">2</span>)/(<span class="number">-2</span>), axis=<span class="number">0</span>)</span><br><span class="line">    dmean = np.sum(-dx_hat/np.sqrt(var+eps), axis=<span class="number">0</span>) + dvar*np.mean(<span class="number">-2</span>*(x-mean))</span><br><span class="line">    dx = dx_hat/np.sqrt(var+eps)+dvar*<span class="number">2</span>*(x-mean)/x.shape[<span class="number">0</span>]+dmean/x.shape[<span class="number">0</span>]</span><br><span class="line">    dgamma = np.sum(dout*x_hat, axis=<span class="number">0</span>)</span><br><span class="line">    dbeta = np.sum(dout, axis=<span class="number">0</span>)</span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                            #</span></span><br><span class="line">    <span class="comment">###########################################################################</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> dx, dgamma, dbeta</span><br></pre></td></tr></table></figure>
<p>Then, we can add batch layer into the neural networks and we can see the improved performance after implementing Batch Normalization. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">231</span>)</span><br><span class="line"><span class="comment"># Try training a very deep net with batchnorm</span></span><br><span class="line">hidden_dims = [<span class="number">100</span>, <span class="number">100</span>, <span class="number">100</span>, <span class="number">100</span>, <span class="number">100</span>]</span><br><span class="line"></span><br><span class="line">num_train = <span class="number">1000</span></span><br><span class="line">small_data = &#123;</span><br><span class="line">  <span class="string">'X_train'</span>: data[<span class="string">'X_train'</span>][:num_train],</span><br><span class="line">  <span class="string">'y_train'</span>: data[<span class="string">'y_train'</span>][:num_train],</span><br><span class="line">  <span class="string">'X_val'</span>: data[<span class="string">'X_val'</span>],</span><br><span class="line">  <span class="string">'y_val'</span>: data[<span class="string">'y_val'</span>],</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">weight_scale = <span class="number">2e-2</span></span><br><span class="line">bn_model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, use_batchnorm=<span class="literal">True</span>)</span><br><span class="line">model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, use_batchnorm=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">bn_solver = Solver(bn_model, small_data,</span><br><span class="line">                num_epochs=<span class="number">10</span>, batch_size=<span class="number">50</span>,</span><br><span class="line">                update_rule=<span class="string">'adam'</span>,</span><br><span class="line">                optim_config=&#123;</span><br><span class="line">                  <span class="string">'learning_rate'</span>: <span class="number">1e-3</span>,</span><br><span class="line">                &#125;,</span><br><span class="line">                verbose=<span class="literal">True</span>, print_every=<span class="number">200</span>)</span><br><span class="line">bn_solver.train()</span><br><span class="line"></span><br><span class="line">solver = Solver(model, small_data,</span><br><span class="line">                num_epochs=<span class="number">10</span>, batch_size=<span class="number">50</span>,</span><br><span class="line">                update_rule=<span class="string">'adam'</span>,</span><br><span class="line">                optim_config=&#123;</span><br><span class="line">                  <span class="string">'learning_rate'</span>: <span class="number">1e-3</span>,</span><br><span class="line">                &#125;,</span><br><span class="line">                verbose=<span class="literal">True</span>, print_every=<span class="number">200</span>)</span><br><span class="line">solver.train()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/03/01/BatchNormalization/1583062559917.png" alt="1583062559917"></p>
<p><img src="/2020/03/01/BatchNormalization/1583062577631.png" alt="1583062577631"></p>
<p><img src="/2020/03/01/BatchNormalization/1583062593173.png" alt="1583062593173"></p>
<p><img src="/2020/03/01/BatchNormalization/1583062611523.png" alt="1583062611523"></p>
<p>With Batch Normalization, the model can train faster than baseline. Also we can see the initialization of weights in different situations can achieve better performance than the model without Batch Normalization.  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line">np.random.seed(<span class="number">231</span>)</span><br><span class="line"><span class="comment"># Try training a very deep net with batchnorm</span></span><br><span class="line">hidden_dims = [<span class="number">50</span>, <span class="number">50</span>, <span class="number">50</span>, <span class="number">50</span>, <span class="number">50</span>, <span class="number">50</span>, <span class="number">50</span>]</span><br><span class="line"></span><br><span class="line">num_train = <span class="number">1000</span></span><br><span class="line">small_data = &#123;</span><br><span class="line">  <span class="string">'X_train'</span>: data[<span class="string">'X_train'</span>][:num_train],</span><br><span class="line">  <span class="string">'y_train'</span>: data[<span class="string">'y_train'</span>][:num_train],</span><br><span class="line">  <span class="string">'X_val'</span>: data[<span class="string">'X_val'</span>],</span><br><span class="line">  <span class="string">'y_val'</span>: data[<span class="string">'y_val'</span>],</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">bn_solvers = &#123;&#125;</span><br><span class="line">solvers = &#123;&#125;</span><br><span class="line">weight_scales = np.logspace(<span class="number">-4</span>, <span class="number">0</span>, num=<span class="number">20</span>)</span><br><span class="line"><span class="keyword">for</span> i, weight_scale <span class="keyword">in</span> enumerate(weight_scales):</span><br><span class="line">  print(<span class="string">'Running weight scale %d / %d'</span> % (i + <span class="number">1</span>, len(weight_scales)))</span><br><span class="line">  bn_model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, use_batchnorm=<span class="literal">True</span>)</span><br><span class="line">  model = FullyConnectedNet(hidden_dims, weight_scale=weight_scale, use_batchnorm=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">  bn_solver = Solver(bn_model, small_data,</span><br><span class="line">                  num_epochs=<span class="number">10</span>, batch_size=<span class="number">50</span>,</span><br><span class="line">                  update_rule=<span class="string">'adam'</span>,</span><br><span class="line">                  optim_config=&#123;</span><br><span class="line">                    <span class="string">'learning_rate'</span>: <span class="number">1e-3</span>,</span><br><span class="line">                  &#125;,</span><br><span class="line">                  verbose=<span class="literal">False</span>, print_every=<span class="number">200</span>)</span><br><span class="line">  bn_solver.train()</span><br><span class="line">  bn_solvers[weight_scale] = bn_solver</span><br><span class="line"></span><br><span class="line">  solver = Solver(model, small_data,</span><br><span class="line">                  num_epochs=<span class="number">10</span>, batch_size=<span class="number">50</span>,</span><br><span class="line">                  update_rule=<span class="string">'adam'</span>,</span><br><span class="line">                  optim_config=&#123;</span><br><span class="line">                    <span class="string">'learning_rate'</span>: <span class="number">1e-3</span>,</span><br><span class="line">                  &#125;,</span><br><span class="line">                  verbose=<span class="literal">False</span>, print_every=<span class="number">200</span>)</span><br><span class="line">  solver.train()</span><br><span class="line">  solvers[weight_scale] = solver</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot results of weight scale experiment</span></span><br><span class="line">best_train_accs, bn_best_train_accs = [], []</span><br><span class="line">best_val_accs, bn_best_val_accs = [], []</span><br><span class="line">final_train_loss, bn_final_train_loss = [], []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> ws <span class="keyword">in</span> weight_scales:</span><br><span class="line">  best_train_accs.append(max(solvers[ws].train_acc_history))</span><br><span class="line">  bn_best_train_accs.append(max(bn_solvers[ws].train_acc_history))</span><br><span class="line">  </span><br><span class="line">  best_val_accs.append(max(solvers[ws].val_acc_history))</span><br><span class="line">  bn_best_val_accs.append(max(bn_solvers[ws].val_acc_history))</span><br><span class="line">  </span><br><span class="line">  final_train_loss.append(np.mean(solvers[ws].loss_history[<span class="number">-100</span>:]))</span><br><span class="line">  bn_final_train_loss.append(np.mean(bn_solvers[ws].loss_history[<span class="number">-100</span>:]))</span><br><span class="line">  </span><br><span class="line">plt.subplot(<span class="number">3</span>, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">plt.title(<span class="string">'Best val accuracy vs weight initialization scale'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Weight initialization scale'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Best val accuracy'</span>)</span><br><span class="line">plt.semilogx(weight_scales, best_val_accs, <span class="string">'-o'</span>, label=<span class="string">'baseline'</span>)</span><br><span class="line">plt.semilogx(weight_scales, bn_best_val_accs, <span class="string">'-o'</span>, label=<span class="string">'batchnorm'</span>)</span><br><span class="line">plt.legend(ncol=<span class="number">2</span>, loc=<span class="string">'lower right'</span>)</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">3</span>, <span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">plt.title(<span class="string">'Best train accuracy vs weight initialization scale'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Weight initialization scale'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Best training accuracy'</span>)</span><br><span class="line">plt.semilogx(weight_scales, best_train_accs, <span class="string">'-o'</span>, label=<span class="string">'baseline'</span>)</span><br><span class="line">plt.semilogx(weight_scales, bn_best_train_accs, <span class="string">'-o'</span>, label=<span class="string">'batchnorm'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line"></span><br><span class="line">plt.subplot(<span class="number">3</span>, <span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">plt.title(<span class="string">'Final training loss vs weight initialization scale'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Weight initialization scale'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Final training loss'</span>)</span><br><span class="line">plt.semilogx(weight_scales, final_train_loss, <span class="string">'-o'</span>, label=<span class="string">'baseline'</span>)</span><br><span class="line">plt.semilogx(weight_scales, bn_final_train_loss, <span class="string">'-o'</span>, label=<span class="string">'batchnorm'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.gca().set_ylim(<span class="number">1.0</span>, <span class="number">3.5</span>)</span><br><span class="line"></span><br><span class="line">plt.gcf().set_size_inches(<span class="number">10</span>, <span class="number">15</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="/2020/03/01/BatchNormalization/1583063126691.png" alt="1583063126691"></p>
<p><img src="/2020/03/01/BatchNormalization/1583063140003.png" alt="1583063140003"></p>

    </div>

    
    
    

      <footer class="post-footer">

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/02/24/Update-rules/" rel="prev" title="Update rules">
      <i class="fa fa-chevron-left"></i> Update rules
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/03/02/Dropout/" rel="next" title="Dropout">
      Dropout <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#Weight-Initialization"><span class="nav-number">1.</span> <span class="nav-text">Weight Initialization</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Batch-Normalization"><span class="nav-number">2.</span> <span class="nav-text">Batch Normalization</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Bania</p>
  <div class="site-description" itemprop="description">A platform for discussing programming and technology</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">15</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Bania</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.7.1
  </div>

<div>
<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<span id="busuanzi_container_site_pv" style='display:none'>
  Total visits: <span id="busuanzi_value_site_pv"></span> times
  <span class="post-meta-divider">|</span>
</span>
<span id="busuanzi_container_site_uv" style='display:none'>
  <span id="busuanzi_value_site_uv"></span>people have viewed my bolg.
</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

  

</body>
</html>
