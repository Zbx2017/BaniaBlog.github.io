<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Pisces","version":"7.7.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="Problem OverviewIn assignment1, the second problem is using SVM to classify the pictures. To be more specific, we have to finish questions below:  implement a fully-vectorized loss function for the SV">
<meta property="og:type" content="article">
<meta property="og:title" content="SVM">
<meta property="og:url" content="http://yoursite.com/2020/02/14/SVM/index.html">
<meta property="og:site_name" content="BaniaBlog">
<meta property="og:description" content="Problem OverviewIn assignment1, the second problem is using SVM to classify the pictures. To be more specific, we have to finish questions below:  implement a fully-vectorized loss function for the SV">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://yoursite.com/2020/02/14/SVM/12.png">
<meta property="og:image" content="http://yoursite.com/2020/02/14/SVM/1581650269438.png">
<meta property="og:image" content="http://yoursite.com/2020/02/14/SVM/1581651442942.png">
<meta property="og:image" content="http://yoursite.com/2020/02/14/SVM/1581651848316.png">
<meta property="og:image" content="http://yoursite.com/2020/02/14/SVM/1581651765041.png">
<meta property="og:image" content="http://yoursite.com/2020/02/14/SVM/1581652469289.png">
<meta property="og:image" content="http://yoursite.com/2020/02/14/SVM/1581653437812.png">
<meta property="og:image" content="http://yoursite.com/2020/02/14/SVM/1581653462776.png">
<meta property="og:image" content="http://yoursite.com/2020/02/14/SVM/1581653476956.png">
<meta property="og:image" content="http://yoursite.com/2020/02/14/SVM/Users/zhong201707030308/AppData/Roaming/Typora/typora-user-images/1581653495267.png">
<meta property="og:image" content="http://yoursite.com/2020/02/14/SVM/1581653546323.png">
<meta property="og:image" content="http://yoursite.com/2020/02/14/SVM/1581653560001.png">
<meta property="og:image" content="http://yoursite.com/2020/02/14/SVM/1581653577111.png">
<meta property="og:image" content="http://yoursite.com/2020/02/14/SVM/1581653628627.png">
<meta property="og:image" content="http://yoursite.com/2020/02/14/SVM/1581653643802.png">
<meta property="og:image" content="http://yoursite.com/2020/02/14/SVM/1581653654946.png">
<meta property="og:image" content="http://yoursite.com/2020/02/14/SVM/1581654019461.png">
<meta property="og:image" content="http://yoursite.com/2020/02/14/SVM/1581654031173.png">
<meta property="og:image" content="http://yoursite.com/2020/02/14/SVM/1581654617210.png">
<meta property="og:image" content="http://yoursite.com/2020/02/14/SVM/1581654631673.png">
<meta property="og:image" content="http://yoursite.com/2020/02/14/SVM/1581655038395.png">
<meta property="og:image" content="http://yoursite.com/2020/02/14/SVM/1581655455574.png">
<meta property="og:image" content="http://yoursite.com/2020/02/14/SVM/1581655532270.png">
<meta property="og:image" content="http://yoursite.com/2020/02/14/SVM/1581655545859.png">
<meta property="og:image" content="http://yoursite.com/2020/02/14/SVM/1581655565773.png">
<meta property="article:published_time" content="2020-02-14T02:39:36.000Z">
<meta property="article:modified_time" content="2020-02-14T04:55:56.183Z">
<meta property="article:author" content="Bania">
<meta property="article:tag" content="Machine Learning">
<meta property="article:tag" content="Python">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/2020/02/14/SVM/12.png">

<link rel="canonical" href="http://yoursite.com/2020/02/14/SVM/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>SVM | BaniaBlog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="BaniaBlog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">BaniaBlog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">Sharing Technology</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/Zbx2017/Zbx2017.github.io" class="github-corner" title="Bania GitHub" aria-label="Bania GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
    
  
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/14/SVM/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Bania">
      <meta itemprop="description" content="A platform for discussing programming and technology">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BaniaBlog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          SVM
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-02-14 10:39:36 / Modified: 12:55:56" itemprop="dateCreated datePublished" datetime="2020-02-14T10:39:36+08:00">2020-02-14</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h4 id="Problem-Overview"><a href="#Problem-Overview" class="headerlink" title="Problem Overview"></a>Problem Overview</h4><p>In assignment1, the second problem is using SVM to classify the pictures. To be more specific, we have to finish questions below:</p>
<ul>
<li>implement a fully-vectorized <strong>loss function</strong> for the SVM</li>
<li>implement the fully-vectorized expression for its <strong>analytic gradient</strong></li>
<li><strong>check your implementation</strong> using numerical gradient</li>
<li>use a validation set to <strong>tune the learning rate and regularization</strong> strength</li>
<li><strong>optimize</strong> the loss function with <strong>SGD</strong></li>
<li><strong>visualize</strong> the final learned weights</li>
</ul>
<p>In this part, the most important is the SVM algorithm. Note that there are up to 10 categories of the pictures, it is unrealistic to use two-classes SVM. In two-classes SVM, we need a line with maximum margin to separate two classes. The basic model is like that: </p>
<p><img src="/2020/02/14/SVM/12.png" alt="12"></p>
<p><img src="/2020/02/14/SVM/1581650269438.png" alt="1581650269438"> </p>
<p>For multiclass classification, we obviously need a different algorithm for SVM. If you have watched the course online, you basically know that Multiclass SVM uses Hinge loss for loss function. Next, I will illustrate it in details and give the code.</p>
<h4 id="Multiclass-Support-Vector-Machine"><a href="#Multiclass-Support-Vector-Machine" class="headerlink" title="Multiclass Support Vector Machine"></a>Multiclass Support Vector Machine</h4><p>Similar to two-classes SVM, the main idea of Multiclass SVM is also to find a hyperplane to separate different categories. Unlike the two-classes SVM(the hyperplane is a line), the Multiclass SVM has a more than two dimensional hyperplane. Thus, the function of the hyperplane is:</p>
<p><img src="/2020/02/14/SVM/1581651442942.png" alt="1581651442942"></p>
<p>where xi and W is more than two dimensional. That is why SVM is also a linear classifier. </p>
<p>The loss function that Multiclass SVM uses is hinge loss. </p>
<p><img src="/2020/02/14/SVM/1581651848316.png" alt="1581651848316"></p>
<p>where Δ is a fixed margin, and it is helpful to decide which is correctly classified. Because we want the score of correctly classified data is greater than those incorrectly classified. In this problem we set Δ to 1.  </p>
<p><img src="/2020/02/14/SVM/1581651765041.png" alt="1581651765041"></p>
<p>To avoid overfitting, we add a regularization loss to the loss function. Then the ultimate loss function is :</p>
<p><img src="/2020/02/14/SVM/1581652469289.png" alt="1581652469289"></p>
<p>For this loss function, we use gradient descent to find the W with the least loss, so that we can obtain the hyperplane. The code of the algorithm is below. The most difficult part for me is the calculation of dW, which needs the knowledge of matrix derivative. For more detail information, you can click the link: <a href="https://en.wikipedia.org/wiki/Matrix_calculus" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Matrix_calculus</a></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> random <span class="keyword">import</span> shuffle</span><br><span class="line"><span class="keyword">from</span> past.builtins <span class="keyword">import</span> xrange</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">svm_loss_naive</span><span class="params">(W, X, y, reg)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Structured SVM loss function, naive implementation (with loops).</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    Inputs have dimension D, there are C classes, and we operate on minibatches</span></span><br><span class="line"><span class="string">    of N examples.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - W: A numpy array of shape (D, C) containing weights.</span></span><br><span class="line"><span class="string">    - X: A numpy array of shape (N, D) containing a minibatch of data.</span></span><br><span class="line"><span class="string">    - y: A numpy array of shape (N,) containing training labels; y[i] = c means</span></span><br><span class="line"><span class="string">      that X[i] has label c, where 0 &lt;= c &lt; C.</span></span><br><span class="line"><span class="string">    - reg: (float) regularization strength</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    Returns a tuple of:</span></span><br><span class="line"><span class="string">    - loss as single float</span></span><br><span class="line"><span class="string">    - gradient with respect to weights W; an array of same shape as W</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    dW = np.zeros(W.shape)  <span class="comment"># initialize the gradient as zero</span></span><br><span class="line">    <span class="comment"># compute the loss and the gradient</span></span><br><span class="line">    num_classes = W.shape[<span class="number">1</span>]</span><br><span class="line">    num_train = X.shape[<span class="number">0</span>]</span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(num_train):</span><br><span class="line">        scores = X[i].dot(W)</span><br><span class="line">        correct_class_score = scores[y[i]]</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> xrange(num_classes):</span><br><span class="line">            <span class="keyword">if</span> j == y[i]:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            margin = scores[j] - correct_class_score + <span class="number">1</span>  <span class="comment"># note delta = 1</span></span><br><span class="line">            <span class="keyword">if</span> margin &gt; <span class="number">0</span>:</span><br><span class="line">                loss += margin</span><br><span class="line">                dW[:, y[i]] += -X[i].T</span><br><span class="line">                dW[:, j] += X[i].T</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Right now the loss is a sum over all training examples, but we want it</span></span><br><span class="line">    <span class="comment"># to be an average instead so we divide by num_train.</span></span><br><span class="line">    loss /= num_train</span><br><span class="line">    dW /= num_train</span><br><span class="line"></span><br><span class="line">    <span class="comment"># Add regularization to the loss.</span></span><br><span class="line">    loss += reg * np.sum(W * W)</span><br><span class="line">    dW += <span class="number">2</span>*reg * W</span><br><span class="line"></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span>                                                                     #</span></span><br><span class="line">    <span class="comment"># Compute the gradient of the loss function and store it dW.                #</span></span><br><span class="line">    <span class="comment"># Rather that first computing the loss and then computing the derivative,   #</span></span><br><span class="line">    <span class="comment"># it may be simpler to compute the derivative at the same time that the     #</span></span><br><span class="line">    <span class="comment"># loss is being computed. As a result you may need to modify some of the    #</span></span><br><span class="line">    <span class="comment"># code above to compute the gradient.                                       #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, dW</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">svm_loss_vectorized</span><span class="params">(W, X, y, reg)</span>:</span></span><br><span class="line">    </span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Structured SVM loss function, vectorized implementation.</span></span><br><span class="line"><span class="string">  </span></span><br><span class="line"><span class="string">    Inputs and outputs are the same as svm_loss_naive.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    loss = <span class="number">0.0</span></span><br><span class="line">    dW = np.zeros(W.shape)  <span class="comment"># initialize the gradient as zero</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span>                                                                     #</span></span><br><span class="line">    <span class="comment"># Implement a vectorized version of the structured SVM loss, storing the    #</span></span><br><span class="line">    <span class="comment"># result in loss.                                                           #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    scores = X.dot(W)</span><br><span class="line">    num_train = X.shape[<span class="number">0</span>]</span><br><span class="line">    margins = np.maximum(<span class="number">0</span>, (scores - np.reshape(scores[np.arange(num_train), y],(num_train,<span class="number">-1</span>)) + <span class="number">1</span>))</span><br><span class="line">    margins[np.arange(num_train), y] = <span class="number">0</span></span><br><span class="line">    loss += np.sum(margins) / num_train</span><br><span class="line">    loss += reg * np.sum(W * W)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                              #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment"># <span class="doctag">TODO:</span>                                                                     #</span></span><br><span class="line">    <span class="comment"># Implement a vectorized version of the gradient for the structured SVM     #</span></span><br><span class="line">    <span class="comment"># loss, storing the result in dW.                                           #</span></span><br><span class="line">    <span class="comment">#                                                                           #</span></span><br><span class="line">    <span class="comment"># Hint: Instead of computing the gradient from scratch, it may be easier    #</span></span><br><span class="line">    <span class="comment"># to reuse some of the intermediate values that you used to compute the     #</span></span><br><span class="line">    <span class="comment"># loss.                                                                     #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    margins[margins &gt; <span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">    num_minus = np.sum(margins, axis=<span class="number">1</span>)</span><br><span class="line">    margins[np.arange(num_train), y] = -num_minus</span><br><span class="line">    dW = X.T.dot(margins) / num_train + <span class="number">2</span> * reg * W</span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line">    <span class="comment">#                             END OF YOUR CODE                              #</span></span><br><span class="line">    <span class="comment">#############################################################################</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> loss, dW</span><br></pre></td></tr></table></figure>
<p>After finish the code of SVM, you can turn to svm.ipynb file to test your algorithm and do the further exercises. </p>
<h4 id="Checking-your-Code"><a href="#Checking-your-Code" class="headerlink" title="Checking your Code"></a>Checking your Code</h4><p>The idea of debugging the code is based on numerical estimate and analytical estimate. Before you start the word, you should load the data and preprocess it. </p>
<p><img src="/2020/02/14/SVM/1581653437812.png" alt="1581653437812"></p>
<p><img src="/2020/02/14/SVM/1581653462776.png" alt="1581653462776"></p>
<p><img src="/2020/02/14/SVM/1581653476956.png" alt="1581653476956"></p>
<p><img src="/2020/02/14/SVM/Users\zhong201707030308\AppData\Roaming\Typora\typora-user-images\1581653495267.png" alt="1581653495267"></p>
<p><img src="/2020/02/14/SVM/1581653546323.png" alt="1581653546323"></p>
<p><img src="/2020/02/14/SVM/1581653560001.png" alt="1581653560001"></p>
<p><img src="/2020/02/14/SVM/1581653577111.png" alt="1581653577111"></p>
<p>Then you can use the algorithm to calculate the loss and dW. As the graph shown below, the error between numerical estimate and analytical estimate is very low, so we can believe that our algorithm is ok. </p>
<p><img src="/2020/02/14/SVM/1581653628627.png" alt="1581653628627"></p>
<p><img src="/2020/02/14/SVM/1581653643802.png" alt="1581653643802"></p>
<p><img src="/2020/02/14/SVM/1581653654946.png" alt="1581653654946"></p>
<p>Remember that we use two way to calculate loss and dW : with loops and without loops. It is obvious the latter is more efficient. We can check it by observing the running time. </p>
<p><img src="/2020/02/14/SVM/1581654019461.png" alt="1581654019461"></p>
<p><img src="/2020/02/14/SVM/1581654031173.png" alt="1581654031173"></p>
<h4 id="Training-the-Model"><a href="#Training-the-Model" class="headerlink" title="Training the Model"></a>Training the Model</h4><p>The training dataset is so large(49000), so if we use the normal way to calculate loss and dW, it will take a long time. The strategy we use here is SGD( Randomly select a small part of the data to get the loss and dW.) which is more effective. You need to modify some code in linear_classifier.py</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> print_function</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> cs231n.classifiers.linear_svm <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> cs231n.classifiers.softmax <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> past.builtins <span class="keyword">import</span> xrange</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearClassifier</span><span class="params">(object)</span>:</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        self.W = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, X, y, learning_rate=<span class="number">1e-3</span>, reg=<span class="number">1e-5</span>, num_iters=<span class="number">100</span>,</span></span></span><br><span class="line"><span class="function"><span class="params">              batch_size=<span class="number">200</span>, verbose=False)</span>:</span></span><br><span class="line">        </span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Train this linear classifier using stochastic gradient descent.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        Inputs:</span></span><br><span class="line"><span class="string">        - X: A numpy array of shape (N, D) containing training data; there are N</span></span><br><span class="line"><span class="string">          training samples each of dimension D.</span></span><br><span class="line"><span class="string">        - y: A numpy array of shape (N,) containing training labels; y[i] = c</span></span><br><span class="line"><span class="string">          means that X[i] has label 0 &lt;= c &lt; C for C classes.</span></span><br><span class="line"><span class="string">        - learning_rate: (float) learning rate for optimization.</span></span><br><span class="line"><span class="string">        - reg: (float) regularization strength.</span></span><br><span class="line"><span class="string">        - num_iters: (integer) number of steps to take when optimizing</span></span><br><span class="line"><span class="string">        - batch_size: (integer) number of training examples to use at each step.</span></span><br><span class="line"><span class="string">        - verbose: (boolean) If true, print progress during optimization.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        Outputs:</span></span><br><span class="line"><span class="string">        A list containing the value of the loss function at each training iteration.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        num_train, dim = X.shape</span><br><span class="line">        num_classes = np.max(y) + <span class="number">1</span>  <span class="comment"># assume y takes values 0...K-1 where K is number of classes</span></span><br><span class="line">        <span class="keyword">if</span> self.W <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># lazily initialize W</span></span><br><span class="line">            self.W = <span class="number">0.001</span> * np.random.randn(dim, num_classes)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># Run stochastic gradient descent to optimize W</span></span><br><span class="line">        loss_history = []</span><br><span class="line">        <span class="keyword">for</span> it <span class="keyword">in</span> xrange(num_iters):</span><br><span class="line">            </span><br><span class="line">            X_batch = <span class="literal">None</span></span><br><span class="line">            y_batch = <span class="literal">None</span></span><br><span class="line">            </span><br><span class="line"></span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line">            <span class="comment"># <span class="doctag">TODO:</span>                                                                 #</span></span><br><span class="line">            <span class="comment"># Sample batch_size elements from the training data and their           #</span></span><br><span class="line">            <span class="comment"># corresponding labels to use in this round of gradient descent.        #</span></span><br><span class="line">            <span class="comment"># Store the data in X_batch and their corresponding labels in           #</span></span><br><span class="line">            <span class="comment"># y_batch; after sampling X_batch should have shape (dim, batch_size)   #</span></span><br><span class="line">            <span class="comment"># and y_batch should have shape (batch_size,)                           #</span></span><br><span class="line">            <span class="comment">#                                                                       #</span></span><br><span class="line">            <span class="comment"># Hint: Use np.random.choice to generate indices. Sampling with         #</span></span><br><span class="line">            <span class="comment"># replacement is faster than sampling without replacement.              #</span></span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line">            index_choice = np.random.choice(num_train, batch_size)</span><br><span class="line">            X_batch = X[index_choice]</span><br><span class="line">            y_batch = y[index_choice]</span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line">            <span class="comment">#                       END OF YOUR CODE                                #</span></span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line"></span><br><span class="line">            <span class="comment"># evaluate loss and gradient</span></span><br><span class="line">            loss, grad = self.loss(X_batch, y_batch, reg)</span><br><span class="line">            loss_history.append(loss)</span><br><span class="line"></span><br><span class="line">            <span class="comment"># perform parameter update</span></span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line">            <span class="comment"># <span class="doctag">TODO:</span>                                                                 #</span></span><br><span class="line">            <span class="comment"># Update the weights using the gradient and the learning rate.          #</span></span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line">            self.W -= learning_rate*grad </span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line">            <span class="comment">#                       END OF YOUR CODE                                #</span></span><br><span class="line">            <span class="comment">#########################################################################</span></span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> verbose <span class="keyword">and</span> it % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                print(<span class="string">'iteration %d / %d: loss %f'</span> % (it, num_iters, loss))</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> loss_history</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Use the trained weights of this linear classifier to predict labels for</span></span><br><span class="line"><span class="string">        data points.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        Inputs:</span></span><br><span class="line"><span class="string">        - X: A numpy array of shape (N, D) containing training data; there are N</span></span><br><span class="line"><span class="string">          training samples each of dimension D.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">        - y_pred: Predicted labels for the data in X. y_pred is a 1-dimensional</span></span><br><span class="line"><span class="string">          array of length N, and each element is an integer giving the predicted</span></span><br><span class="line"><span class="string">          class.</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        y_pred = np.zeros(X.shape[<span class="number">0</span>])</span><br><span class="line">        <span class="comment">###########################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span>                                                                   #</span></span><br><span class="line">        <span class="comment"># Implement this method. Store the predicted labels in y_pred.            #</span></span><br><span class="line">        <span class="comment">###########################################################################</span></span><br><span class="line">        scores = X.dot(self.W)</span><br><span class="line">        y_pred = np.argmax(scores, axis=<span class="number">1</span>)</span><br><span class="line">        <span class="comment">###########################################################################</span></span><br><span class="line">        <span class="comment">#                           END OF YOUR CODE                              #</span></span><br><span class="line">        <span class="comment">###########################################################################</span></span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(self, X_batch, y_batch, reg)</span>:</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Compute the loss function and its derivative. </span></span><br><span class="line"><span class="string">        Subclasses will override this.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        Inputs:</span></span><br><span class="line"><span class="string">        - X_batch: A numpy array of shape (N, D) containing a minibatch of N</span></span><br><span class="line"><span class="string">          data points; each point has dimension D.</span></span><br><span class="line"><span class="string">        - y_batch: A numpy array of shape (N,) containing labels for the minibatch.</span></span><br><span class="line"><span class="string">        - reg: (float) regularization strength.</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">        Returns: A tuple containing:</span></span><br><span class="line"><span class="string">        - loss as a single float</span></span><br><span class="line"><span class="string">        - gradient with respect to self.W; an array of the same shape as W</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LinearSVM</span><span class="params">(LinearClassifier)</span>:</span></span><br><span class="line">    <span class="string">""" A subclass that uses the Multiclass SVM loss function """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(self, X_batch, y_batch, reg)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> svm_loss_vectorized(self.W, X_batch, y_batch, reg)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Softmax</span><span class="params">(LinearClassifier)</span>:</span></span><br><span class="line">    <span class="string">""" A subclass that uses the Softmax + Cross-entropy loss function """</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">loss</span><span class="params">(self, X_batch, y_batch, reg)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> softmax_loss_vectorized(self.W, X_batch, y_batch, reg)</span><br></pre></td></tr></table></figure>
<p> Then, you can see the time it take to run the algorithm and the change of the loss. </p>
<p><img src="/2020/02/14/SVM/1581654617210.png" alt="1581654617210"></p>
<p><img src="/2020/02/14/SVM/1581654631673.png" alt="1581654631673"></p>
<p>Use the model to predict the data. The accuracy of validation is 0.38 which is higher than knn.</p>
<p><img src="/2020/02/14/SVM/1581655038395.png" alt="1581655038395"></p>
<p>The accuracy is related to learning rate and the regularization rate. Then we use cross validate to find the best learning rate and regularization rate. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Use the validation set to tune hyperparameters (regularization strength and</span></span><br><span class="line"><span class="comment"># learning rate). You should experiment with different ranges for the learning</span></span><br><span class="line"><span class="comment"># rates and regularization strengths; if you are careful you should be able to</span></span><br><span class="line"><span class="comment"># get a classification accuracy of about 0.4 on the validation set.</span></span><br><span class="line">learning_rates = [<span class="number">1.31e-7</span>,<span class="number">1.33e-7</span>,<span class="number">1.35e-7</span>,<span class="number">1.37e-7</span>,<span class="number">1.38e-7</span>,<span class="number">1.4e-7</span>]</span><br><span class="line">regularization_strengths = [<span class="number">2.6e4</span>, <span class="number">2.63e4</span>,<span class="number">2.66e4</span>,<span class="number">2.69e4</span>,<span class="number">2.72e4</span>,<span class="number">2.75e4</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># results is dictionary mapping tuples of the form</span></span><br><span class="line"><span class="comment"># (learning_rate, regularization_strength) to tuples of the form</span></span><br><span class="line"><span class="comment"># (training_accuracy, validation_accuracy). The accuracy is simply the fraction</span></span><br><span class="line"><span class="comment"># of data points that are correctly classified.</span></span><br><span class="line">results = &#123;&#125;</span><br><span class="line">best_val = <span class="number">-1</span>   <span class="comment"># The highest validation accuracy that we have seen so far.</span></span><br><span class="line">best_svm = <span class="literal">None</span> <span class="comment"># The LinearSVM object that achieved the highest validation rate.</span></span><br><span class="line"></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span>                                                                        #</span></span><br><span class="line"><span class="comment"># Write code that chooses the best hyperparameters by tuning on the validation #</span></span><br><span class="line"><span class="comment"># set. For each combination of hyperparameters, train a linear SVM on the      #</span></span><br><span class="line"><span class="comment"># training set, compute its accuracy on the training and validation sets, and  #</span></span><br><span class="line"><span class="comment"># store these numbers in the results dictionary. In addition, store the best   #</span></span><br><span class="line"><span class="comment"># validation accuracy in best_val and the LinearSVM object that achieves this  #</span></span><br><span class="line"><span class="comment"># accuracy in best_svm.                                                        #</span></span><br><span class="line"><span class="comment">#                                                                              #</span></span><br><span class="line"><span class="comment"># Hint: You should use a small value for num_iters as you develop your         #</span></span><br><span class="line"><span class="comment"># validation code so that the SVMs don't take much time to train; once you are #</span></span><br><span class="line"><span class="comment"># confident that your validation code works, you should rerun the validation   #</span></span><br><span class="line"><span class="comment"># code with a larger value for num_iters.                                      #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> lr_i <span class="keyword">in</span> learning_rates:</span><br><span class="line">    <span class="keyword">for</span> rs_i <span class="keyword">in</span> regularization_strengths:</span><br><span class="line">        svm = LinearSVM()</span><br><span class="line">        svm.train(X_train, y_train, learning_rate=lr_i, reg=rs_i,num_iters=<span class="number">1500</span>, verbose=<span class="literal">True</span>)</span><br><span class="line">        y_train_pred = svm.predict(X_train)</span><br><span class="line">        y_val_pred = svm.predict(X_val)</span><br><span class="line">        y_train_acc = np.mean(y_train == y_train_pred)</span><br><span class="line">        y_val_acc = np.mean(y_val == y_val_pred)</span><br><span class="line">        results[(lr_i, rs_i)] = y_train_acc , y_val_acc</span><br><span class="line">        <span class="keyword">if</span> y_val_acc &gt; best_val:</span><br><span class="line">            best_val = y_val_acc</span><br><span class="line">            best_svm = svm</span><br><span class="line"></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment">#                              END OF YOUR CODE                                #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line">    </span><br><span class="line"><span class="comment"># Print out results.</span></span><br><span class="line"><span class="keyword">for</span> lr, reg <span class="keyword">in</span> sorted(results):</span><br><span class="line">    train_accuracy, val_accuracy = results[(lr, reg)]</span><br><span class="line">    print(<span class="string">'lr %e reg %e train accuracy: %f val accuracy: %f'</span> % (</span><br><span class="line">                lr, reg, train_accuracy, val_accuracy))</span><br><span class="line">    </span><br><span class="line">print(<span class="string">'best validation accuracy achieved during cross-validation: %f'</span> % best_val)</span><br></pre></td></tr></table></figure>
<p>We can get the best validate accuracy is 0.394, then use the best model for testing and the accuracy is 0.371</p>
<p><img src="/2020/02/14/SVM/1581655455574.png" alt="1581655455574"></p>
<p><img src="/2020/02/14/SVM/1581655532270.png" alt="1581655532270"></p>
<p><img src="/2020/02/14/SVM/1581655545859.png" alt="1581655545859"></p>
<p><img src="/2020/02/14/SVM/1581655565773.png" alt="1581655565773"></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
              <a href="/tags/Python/" rel="tag"># Python</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/02/13/KNN/" rel="prev" title="KNN">
      <i class="fa fa-chevron-left"></i> KNN
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#Problem-Overview"><span class="nav-number">1.</span> <span class="nav-text">Problem Overview</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Multiclass-Support-Vector-Machine"><span class="nav-number">2.</span> <span class="nav-text">Multiclass Support Vector Machine</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Checking-your-Code"><span class="nav-number">3.</span> <span class="nav-text">Checking your Code</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Training-the-Model"><span class="nav-number">4.</span> <span class="nav-text">Training the Model</span></a></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Bania</p>
  <div class="site-description" itemprop="description">A platform for discussing programming and technology</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">3</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Bania</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.7.1
  </div>

<div>
<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<span id="busuanzi_container_site_pv" style='display:none'>
  Total visits: <span id="busuanzi_value_site_pv"></span> times
  <span class="post-meta-divider">|</span>
</span>
<span id="busuanzi_container_site_uv" style='display:none'>
  <span id="busuanzi_value_site_uv"></span>people have viewed my bolg.
</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

  

</body>
</html>
