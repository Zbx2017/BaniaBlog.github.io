<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 4.2.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"yoursite.com","root":"/","scheme":"Pisces","version":"7.7.1","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":true,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":true,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":true,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="KNNKNN ClassifierIn assignment1 the first question is K-Nearest Neighbor Classifier. You should open the classifiers file and write your code in the k_nearest_neighbor.py.  The first thing you should">
<meta property="og:type" content="article">
<meta property="og:title" content="KNN">
<meta property="og:url" content="http://yoursite.com/2020/02/13/KNN/index.html">
<meta property="og:site_name" content="BaniaBlog">
<meta property="og:description" content="KNNKNN ClassifierIn assignment1 the first question is K-Nearest Neighbor Classifier. You should open the classifiers file and write your code in the k_nearest_neighbor.py.  The first thing you should">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="http://yoursite.com/2020/02/13/KNN/1581326111097.png">
<meta property="og:image" content="http://yoursite.com/2020/02/13/KNN/1581327098979.png">
<meta property="og:image" content="http://yoursite.com/2020/02/13/KNN/1581327308499.png">
<meta property="og:image" content="http://yoursite.com/2020/02/13/KNN/1581327332838.png">
<meta property="og:image" content="http://yoursite.com/2020/02/13/KNN/1581327868618.png">
<meta property="og:image" content="http://yoursite.com/2020/02/13/KNN/1581327880973.png">
<meta property="og:image" content="http://yoursite.com/2020/02/13/KNN/1581327940045.png">
<meta property="og:image" content="http://yoursite.com/2020/02/13/KNN/1581328056138.png">
<meta property="og:image" content="http://yoursite.com/2020/02/13/KNN/1581328071246.png">
<meta property="og:image" content="http://yoursite.com/2020/02/13/KNN/1581328131076.png">
<meta property="og:image" content="http://yoursite.com/2020/02/13/KNN/1581328226483.png">
<meta property="og:image" content="http://yoursite.com/2020/02/13/KNN/1581328356671.png">
<meta property="og:image" content="http://yoursite.com/2020/02/13/KNN/1581328379895.png">
<meta property="og:image" content="http://yoursite.com/2020/02/13/KNN/1581328411502.png">
<meta property="og:image" content="http://yoursite.com/2020/02/13/KNN/1581329141634.png">
<meta property="og:image" content="http://yoursite.com/2020/02/13/KNN/1581329151487.png">
<meta property="og:image" content="http://yoursite.com/2020/02/13/KNN/1581329185444.png">
<meta property="article:published_time" content="2020-02-13T06:57:08.000Z">
<meta property="article:modified_time" content="2020-02-13T08:44:00.781Z">
<meta property="article:author" content="Bania">
<meta property="article:tag" content="Python">
<meta property="article:tag" content="Machine Learning">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="http://yoursite.com/2020/02/13/KNN/1581326111097.png">

<link rel="canonical" href="http://yoursite.com/2020/02/13/KNN/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true
  };
</script>

  <title>KNN | BaniaBlog</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

<link rel="alternate" href="/atom.xml" title="BaniaBlog" type="application/atom+xml">
</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">BaniaBlog</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
        <p class="site-subtitle">Sharing Technology</p>
  </div>

  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-fw fa-home"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-fw fa-archive"></i>Archives</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>Search
        </a>
      </li>
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="off"
           placeholder="Searching..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>
  <div class="reading-progress-bar"></div>
  <a role="button" class="book-mark-link book-mark-link-fixed"></a>

  <a href="https://github.com/Zbx2017/Zbx2017.github.io" class="github-corner" title="Bania GitHub" aria-label="Bania GitHub" rel="noopener" target="_blank"><svg width="80" height="80" viewBox="0 0 250 250" aria-hidden="true"><path d="M0,0 L115,115 L130,115 L142,142 L250,250 L250,0 Z"></path><path d="M128.3,109.0 C113.8,99.7 119.0,89.6 119.0,89.6 C122.0,82.7 120.5,78.6 120.5,78.6 C119.2,72.0 123.4,76.3 123.4,76.3 C127.3,80.9 125.5,87.3 125.5,87.3 C122.9,97.6 130.6,101.9 134.4,103.2" fill="currentColor" style="transform-origin: 130px 106px;" class="octo-arm"></path><path d="M115.0,115.0 C114.9,115.1 118.7,116.5 119.8,115.4 L133.7,101.6 C136.9,99.2 139.9,98.4 142.2,98.6 C133.8,88.0 127.5,74.4 143.8,58.0 C148.5,53.4 154.0,51.2 159.7,51.0 C160.3,49.4 163.2,43.6 171.4,40.1 C171.4,40.1 176.1,42.5 178.8,56.2 C183.1,58.6 187.2,61.8 190.9,65.4 C194.5,69.0 197.7,73.2 200.1,77.6 C213.8,80.2 216.3,84.9 216.3,84.9 C212.7,93.1 206.9,96.0 205.4,96.6 C205.1,102.4 203.0,107.8 198.3,112.5 C181.9,128.9 168.3,122.5 157.7,114.1 C157.9,116.9 156.7,120.9 152.7,124.9 L141.0,136.5 C139.8,137.7 141.6,141.9 141.8,141.8 Z" fill="currentColor" class="octo-body"></path></svg></a>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content">
            

  <div class="posts-expand">
    
  
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block " lang="en">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2020/02/13/KNN/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="Bania">
      <meta itemprop="description" content="A platform for discussing programming and technology">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="BaniaBlog">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          KNN
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2020-02-13 14:57:08 / Modified: 16:44:00" itemprop="dateCreated datePublished" datetime="2020-02-13T14:57:08+08:00">2020-02-13</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h4 id="KNN"><a href="#KNN" class="headerlink" title="KNN"></a>KNN</h4><h5 id="KNN-Classifier"><a href="#KNN-Classifier" class="headerlink" title="KNN Classifier"></a>KNN Classifier</h5><p>In assignment1 the first question is K-Nearest Neighbor Classifier. You should open the classifiers file and write your code in the k_nearest_neighbor.py. </p>
<p>The first thing you should keep in mind is the idea of KNN. Let me give a brief of it. In our dataset, we often split the data into training data and testing data. In testing data, we do not know the value of the target variable(the variable we need to predict), while in training data, the target variable is given and we use this data to train our model. For KNN, we also need the training data to predict the target variable in the testing data. How to achieve this? We can understand it by the name of KNN. For example, now we have a testing data point and we need to know which categories it belongs to. And then we find k points which are near the testing point most. The way we decide the k points is by measuring the distance between the testing point and all training data points. After we get these k points and they all have their own categories, we can see the most common category in these points and the testing point belongs to this category. </p>
<p>The idea of KNN is quite straightforward. In conclusion, we just need three steps:</p>
<ol>
<li>Measuring the distance between testing data and training data.</li>
<li>For every testing data point, selecting nearsest k points. </li>
<li>In those k points, find the most common value of target variable and use it to predict the value of target variable for testing data.</li>
</ol>
<p>Knowing the idea of KNN, now we can code in the file. You can see that there are some instructions in the file, and you just need to follow them. </p>
<p>These code are already written for you and you just need to understand them. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> past.builtins <span class="keyword">import</span> xrange</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">KNearestNeighbor</span><span class="params">(object)</span>:</span></span><br><span class="line">  <span class="string">""" a kNN classifier with L2 distance """</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">     <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, X, y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Train the classifier. For k-nearest neighbors this is just </span></span><br><span class="line"><span class="string">    memorizing the training data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - X: A numpy array of shape (num_train, D) containing the training data</span></span><br><span class="line"><span class="string">    consisting of num_train samples each of dimension D.</span></span><br><span class="line"><span class="string">    - y: A numpy array of shape (N,) containing the training labels, where</span></span><br><span class="line"><span class="string">      y[i] is the label for X[i].</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    self.X_train = X</span><br><span class="line">    self.y_train = y</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict</span><span class="params">(self, X, k=<span class="number">1</span>, num_loops=<span class="number">0</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Predict labels for test data using this classifier.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - X: A numpy array of shape (num_test, D) containing test data consisting</span></span><br><span class="line"><span class="string">         of num_test samples each of dimension D.</span></span><br><span class="line"><span class="string">    - k: The number of nearest neighbors that vote for the predicted labels.</span></span><br><span class="line"><span class="string">    - num_loops: Determines which implementation to use to compute distances</span></span><br><span class="line"><span class="string">      between training points and testing points.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - y: A numpy array of shape (num_test,) containing predicted labels for the</span></span><br><span class="line"><span class="string">      test data, where y[i] is the predicted label for the test point X[i].  </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> num_loops == <span class="number">0</span>:</span><br><span class="line">      dists = self.compute_distances_no_loops(X)</span><br><span class="line">    <span class="keyword">elif</span> num_loops == <span class="number">1</span>:</span><br><span class="line">      dists = self.compute_distances_one_loop(X)</span><br><span class="line">    <span class="keyword">elif</span> num_loops == <span class="number">2</span>:</span><br><span class="line">      dists = self.compute_distances_two_loops(X)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="keyword">raise</span> ValueError(<span class="string">'Invalid value %d for num_loops'</span> % num_loops)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> self.predict_labels(dists, k=k)</span><br></pre></td></tr></table></figure>
<p>Here is the first step: measuring the distance. We have three algorithms to do this. Different algorithms have different performance and you can see it in the next session. </p>
<p>The first algorithm is simple but consume more time.  </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_distances_two_loops</span><span class="params">(self, X)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Compute the distance between each test point in X and each training point</span></span><br><span class="line"><span class="string">    in self.X_train using a nested loop over both the training data and the </span></span><br><span class="line"><span class="string">    test data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - X: A numpy array of shape (num_test, D) containing test data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - dists: A numpy array of shape (num_test, num_train) where dists[i, j]</span></span><br><span class="line"><span class="string">      is the Euclidean distance between the ith test point and the jth training</span></span><br><span class="line"><span class="string">      point.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    num_test = X.shape[<span class="number">0</span>]</span><br><span class="line">    num_train = self.X_train.shape[<span class="number">0</span>]</span><br><span class="line">    dists = np.zeros((num_test, num_train))</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(num_test):</span><br><span class="line">      <span class="keyword">for</span> j <span class="keyword">in</span> xrange(num_train): </span><br><span class="line">        <span class="comment"># calculate the distance</span></span><br><span class="line">        dists[i][j] = np.sum((X[i] - self.X_train[j])**<span class="number">2</span>)**<span class="number">0.5</span></span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        np.sum() method is used to add an matrix, axis is one of the peremeter in this           method. In this case, the X[i] - self.X_train[j] has one axis, and we do not need         to give the axis peremeter or if you like you can set it to 0.</span></span><br><span class="line"><span class="string">        We can also see that in many numpy method there will be axis peremeter.Different         dataset has different dimension and axis is the one you want to delete by using           the method. </span></span><br><span class="line"><span class="string">        For example, by using np.sum(One dimensional dataset, axis=0) it will give you a         point. np.sum(two dimensional dataset, axis=0) will add each row and return a one         dimensional data, while axis=1 will add each column. </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment">#####################################################################</span></span><br><span class="line">        <span class="comment"># <span class="doctag">TODO:</span>                                                             #</span></span><br><span class="line">        <span class="comment"># Compute the l2 distance between the ith test point and the jth    #</span></span><br><span class="line">        <span class="comment"># training point, and store the result in dists[i, j]. You should   #</span></span><br><span class="line">        <span class="comment"># not use a loop over dimension.                                    #</span></span><br><span class="line">        <span class="comment">#####################################################################</span></span><br><span class="line">        <span class="comment">#         pass</span></span><br><span class="line">        <span class="comment">#####################################################################</span></span><br><span class="line">        <span class="comment">#                       END OF YOUR CODE                            #</span></span><br><span class="line">        <span class="comment">#####################################################################</span></span><br><span class="line">    <span class="keyword">return</span> dists</span><br></pre></td></tr></table></figure>
<p>The second algorithm uses one loop.  The most important method is tile().</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_distances_one_loop</span><span class="params">(self, X)</span>:</span></span><br><span class="line">   <span class="string">"""</span></span><br><span class="line"><span class="string">   Compute the distance between each test point in X and each training point</span></span><br><span class="line"><span class="string">   in self.X_train using a single loop over the test data.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">   Input / Output: Same as compute_distances_two_loops</span></span><br><span class="line"><span class="string">   """</span></span><br><span class="line">   num_test = X.shape[<span class="number">0</span>]</span><br><span class="line">   num_train = self.X_train.shape[<span class="number">0</span>]</span><br><span class="line">   dists = np.zeros((num_test, num_train))</span><br><span class="line">   <span class="keyword">for</span> i <span class="keyword">in</span> xrange(num_test):</span><br><span class="line">       diff = np.tile(X[i], (num_train, <span class="number">1</span>)) - self.X_train</span><br><span class="line">       dists[i] = np.sum(diff**<span class="number">2</span>, axis=<span class="number">1</span>)**<span class="number">0.5</span></span><br><span class="line">       <span class="string">"""</span></span><br><span class="line"><span class="string">       np.tile() can duplicate an array in different size. The second peremeter(rows,           columns) indicate the array should be duplicated rows times in the row                   direction and columns times in the column direction. </span></span><br><span class="line"><span class="string">       In this case, we duplicate the X[i] num_train times so the size of the X[i] is           the same with X_train, then do the substraction.</span></span><br><span class="line"><span class="string">       """</span></span><br><span class="line">     <span class="comment">#######################################################################</span></span><br><span class="line">     <span class="comment"># <span class="doctag">TODO:</span>                                                               #</span></span><br><span class="line">     <span class="comment"># Compute the l2 distance between the ith test point and all training #</span></span><br><span class="line">     <span class="comment"># points, and store the result in dists[i, :].                        #</span></span><br><span class="line">     <span class="comment">#######################################################################</span></span><br><span class="line">   </span><br><span class="line">     <span class="comment">#       pass</span></span><br><span class="line">     <span class="comment">#######################################################################</span></span><br><span class="line">     <span class="comment">#                         END OF YOUR CODE                            #</span></span><br><span class="line">     <span class="comment">#######################################################################</span></span><br><span class="line">   <span class="keyword">return</span> dists</span><br></pre></td></tr></table></figure>
<p>The last algorithm is the most effective one because it uses no loops. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">compute_distances_no_loops</span><span class="params">(self, X)</span>:</span></span><br><span class="line">  <span class="string">"""</span></span><br><span class="line"><span class="string">  Compute the distance between each test point in X and each training point</span></span><br><span class="line"><span class="string">  in self.X_train using no explicit loops.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">  Input / Output: Same as compute_distances_two_loops</span></span><br><span class="line"><span class="string">  """</span></span><br><span class="line">  num_test = X.shape[<span class="number">0</span>]</span><br><span class="line">  num_train = self.X_train.shape[<span class="number">0</span>]</span><br><span class="line">  dists = np.zeros((num_test, num_train)) </span><br><span class="line">  <span class="comment"># the main idea is (a-b)**2 = a**2 +b**2 -2ab</span></span><br><span class="line">  multi_matrix = np.dot(X, self.X_train.T)</span><br><span class="line">  X_sum = np.sum(X**<span class="number">2</span>, axis=<span class="number">1</span>, keepdims=<span class="literal">True</span>)</span><br><span class="line">  <span class="comment"># using sum will return a vector so we set keepdims to True and it will not change       the shape of the data.</span></span><br><span class="line">  X_squared = np.tile(X_sum,  multi_matrix.shape[<span class="number">1</span>])</span><br><span class="line">  X_train_sum = np.sum(self.X_train.T**<span class="number">2</span>, axis=<span class="number">0</span>)</span><br><span class="line">  X_train_squared = np.tile(X_train_sum, (multi_matrix.shape[<span class="number">0</span>], <span class="number">1</span>))</span><br><span class="line">  dists = (X_squared + X_train_squared - <span class="number">2</span>*multi_matrix)**<span class="number">0.5</span></span><br><span class="line">  <span class="comment">#########################################################################</span></span><br><span class="line">  <span class="comment"># <span class="doctag">TODO:</span>                                                                 #</span></span><br><span class="line">  <span class="comment"># Compute the l2 distance between all test points and all training      #</span></span><br><span class="line">  <span class="comment"># points without using any explicit loops, and store the result in      #</span></span><br><span class="line">  <span class="comment"># dists.                                                                #</span></span><br><span class="line">  <span class="comment">#                                                                       #</span></span><br><span class="line">  <span class="comment"># You should implement this function using only basic array operations; #</span></span><br><span class="line">  <span class="comment"># in particular you should not use functions from scipy.                #</span></span><br><span class="line">  <span class="comment">#                                                                       #</span></span><br><span class="line">  <span class="comment"># HINT: Try to formulate the l2 distance using matrix multiplication    #</span></span><br><span class="line">  <span class="comment">#       and two broadcast sums.                                         #</span></span><br><span class="line">  <span class="comment">#########################################################################</span></span><br><span class="line">  <span class="comment">#     pass</span></span><br><span class="line">  <span class="comment">#########################################################################</span></span><br><span class="line">  <span class="comment">#                         END OF YOUR CODE                              #</span></span><br><span class="line">  <span class="comment">#########################################################################</span></span><br><span class="line">  <span class="keyword">return</span> dists</span><br></pre></td></tr></table></figure>
<p>Then, we can test those three algorithms to see whether they can get the same result. </p>
<p><img src="/2020/02/13/KNN/1581326111097.png" alt="1581326111097"></p>
<p>Now we will turn to step2 and step3. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">predict_labels</span><span class="params">(self, dists, k=<span class="number">1</span>)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Given a matrix of distances between test points and training points,</span></span><br><span class="line"><span class="string">    predict a label for each test point.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Inputs:</span></span><br><span class="line"><span class="string">    - dists: A numpy array of shape (num_test, num_train) where dists[i, j]</span></span><br><span class="line"><span class="string">      gives the distance betwen the ith test point and the jth training point.</span></span><br><span class="line"><span class="string"></span></span><br><span class="line"><span class="string">    Returns:</span></span><br><span class="line"><span class="string">    - y: A numpy array of shape (num_test,) containing predicted labels for the</span></span><br><span class="line"><span class="string">      test data, where y[i] is the predicted label for the test point X[i].  </span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    num_test = dists.shape[<span class="number">0</span>]</span><br><span class="line">    y_pred = np.zeros(num_test)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> xrange(num_test):</span><br><span class="line">      <span class="comment"># A list of length k storing the labels of the k nearest neighbors to</span></span><br><span class="line">      <span class="comment"># the ith test point.</span></span><br><span class="line">        closest_y = []</span><br><span class="line">      <span class="comment">#########################################################################</span></span><br><span class="line">      <span class="comment"># <span class="doctag">TODO:</span>                                                                 #</span></span><br><span class="line">      <span class="comment"># Use the distance matrix to find the k nearest neighbors of the ith    #</span></span><br><span class="line">      <span class="comment"># testing point, and use self.y_train to find the labels of these       #</span></span><br><span class="line">      <span class="comment"># neighbors. Store these labels in closest_y.                           #</span></span><br><span class="line">      <span class="comment"># Hint: Look up the function numpy.argsort.                             #</span></span><br><span class="line">      <span class="comment">#########################################################################</span></span><br><span class="line">      </span><br><span class="line">        labels_index = np.argsort(dists[i]) </span><br><span class="line">        closest_y = np.array(self.y_train)[labels_index[:k]]</span><br><span class="line">        </span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        np.argsort() can return the index of the value from low to high. Then we can get         the k points. </span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line"><span class="comment">#       pass</span></span><br><span class="line">      <span class="comment">#########################################################################</span></span><br><span class="line">      <span class="comment"># <span class="doctag">TODO:</span>                                                                 #</span></span><br><span class="line">      <span class="comment"># Now that you have found the labels of the k nearest neighbors, you    #</span></span><br><span class="line">      <span class="comment"># need to find the most common label in the list closest_y of labels.   #</span></span><br><span class="line">      <span class="comment"># Store this label in y_pred[i]. Break ties by choosing the smaller     #</span></span><br><span class="line">      <span class="comment"># label.                                                                #</span></span><br><span class="line">      <span class="comment">#########################################################################</span></span><br><span class="line">       </span><br><span class="line"><span class="comment">#         from collections import Counter</span></span><br><span class="line"><span class="comment">#         labels_counter = Counter(closest_y)</span></span><br><span class="line"><span class="comment">#         y_pred[i] = labels_counter.most_common()[0][0]</span></span><br><span class="line">       <span class="comment"># The above is one way to get the most common value in closest_y. And I have the          #more simple way below.  </span></span><br><span class="line">        y_pred[i] = np.argmax(np.bincount(closest_y.tolist()))</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        np.bincount() returns the times of different value appears in closest_y.</span></span><br><span class="line"><span class="string">        np.argmax() returns the most common value in closest_y</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        </span><br><span class="line"><span class="comment">#       pass</span></span><br><span class="line">      <span class="comment">#########################################################################</span></span><br><span class="line">      <span class="comment">#                           END OF YOUR CODE                            # </span></span><br><span class="line">      <span class="comment">#########################################################################</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> y_pred</span><br></pre></td></tr></table></figure>
<p>We can use a simple example to test the model. The result is true, which means now we can use it to classify our data.</p>
<p><img src="/2020/02/13/KNN/1581327098979.png" alt="1581327098979"></p>
<h5 id="Training-and-Testing"><a href="#Training-and-Testing" class="headerlink" title="Training and Testing"></a>Training and Testing</h5><p>In this session, I will show you the whole process of training and testing the model and get the predictions. First, you have to open the knn.ipynb file and you can see you are given some instructions. </p>
<p>1) Load the dataset. </p>
<p><img src="/2020/02/13/KNN/1581327308499.png" alt="1581327308499"></p>
<p><img src="/2020/02/13/KNN/1581327332838.png" alt="1581327332838"></p>
<p>You can find the shape of X_train, y_train, X_test and y_test. load_CIFAR10() is a method that can split the data into training data and testing data. </p>
<p>Then using method below can visualize the dataset. </p>
<p><img src="/2020/02/13/KNN/1581327868618.png" alt="1581327868618"></p>
<p><img src="/2020/02/13/KNN/1581327880973.png" alt="1581327880973"></p>
<p>For the original dataset is too large, we just choose some of the data to analyse. </p>
<p><img src="/2020/02/13/KNN/1581327940045.png" alt="1581327940045"></p>
<p>Then import the knn to train.</p>
<p><img src="/2020/02/13/KNN/1581328056138.png" alt="1581328056138"></p>
<p><img src="/2020/02/13/KNN/1581328071246.png" alt="1581328071246"></p>
<p>We can draw the graph above to see the distance between testing data and training data. Then we can use the model to predict and get the accuracy. We can see that the accuracy is 0.274. It is very low, because we classify the data just by the RGB value of the picture. </p>
<p><img src="/2020/02/13/KNN/1581328131076.png" alt="1581328131076"></p>
<p><img src="/2020/02/13/KNN/1581328226483.png" alt="1581328226483"></p>
<p>When we set k to 1, the accuracy slightly increases. And then we see whether we can get the same distance matrix by using three different algorithms. </p>
<p><img src="/2020/02/13/KNN/1581328356671.png" alt="1581328356671"></p>
<p><img src="/2020/02/13/KNN/1581328379895.png" alt="1581328379895"></p>
<p>Then look at the running time. It is obvious that no-loops is the most effective. </p>
<p><img src="/2020/02/13/KNN/1581328411502.png" alt="1581328411502"></p>
<p>You may notice that when we use k=1 and k=5, the accuracy is different, so can we find the best k value which has the highest accuracy? For this, we can use cross validation. </p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line">num_folds = <span class="number">5</span></span><br><span class="line">k_choices = [<span class="number">1</span>, <span class="number">3</span>, <span class="number">5</span>, <span class="number">8</span>, <span class="number">10</span>, <span class="number">12</span>, <span class="number">15</span>, <span class="number">20</span>, <span class="number">50</span>, <span class="number">100</span>]</span><br><span class="line"></span><br><span class="line">X_train_folds = []</span><br><span class="line">y_train_folds = []</span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span>                                                                        #</span></span><br><span class="line"><span class="comment"># Split up the training data into folds. After splitting, X_train_folds and    #</span></span><br><span class="line"><span class="comment"># y_train_folds should each be lists of length num_folds, where                #</span></span><br><span class="line"><span class="comment"># y_train_folds[i] is the label vector for the points in X_train_folds[i].     #</span></span><br><span class="line"><span class="comment"># Hint: Look up the numpy array_split function.                                #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line">X_train_folds = np.array_split(X_train, num_folds)</span><br><span class="line">y_train_folds = np.array_split(y_train, num_folds)</span><br><span class="line"><span class="comment"># pass</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment">#                                 END OF YOUR CODE                             #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># A dictionary holding the accuracies for different values of k that we find</span></span><br><span class="line"><span class="comment"># when running cross-validation. After running cross-validation,</span></span><br><span class="line"><span class="comment"># k_to_accuracies[k] should be a list of length num_folds giving the different</span></span><br><span class="line"><span class="comment"># accuracy values that we found when using that value of k.</span></span><br><span class="line">k_to_accuracies = &#123;&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment"># <span class="doctag">TODO:</span>                                                                        #</span></span><br><span class="line"><span class="comment"># Perform k-fold cross validation to find the best value of k. For each        #</span></span><br><span class="line"><span class="comment"># possible value of k, run the k-nearest-neighbor algorithm num_folds times,   #</span></span><br><span class="line"><span class="comment"># where in each case you use all but one of the folds as training data and the #</span></span><br><span class="line"><span class="comment"># last fold as a validation set. Store the accuracies for all fold and all     #</span></span><br><span class="line"><span class="comment"># values of k in the k_to_accuracies dictionary.                               #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="keyword">for</span> k_value <span class="keyword">in</span> k_choices:</span><br><span class="line">    k_to_accuracies[k_value] = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(num_folds):</span><br><span class="line">        <span class="comment"># didn't work</span></span><br><span class="line"><span class="comment">#         X_train_cv = np.delete(X_train_folds, i).tolist()</span></span><br><span class="line"><span class="comment">#         y_train_cv = np.delete(y_train_folds, i).tolist()        </span></span><br><span class="line"><span class="comment">#         X_train_cv = np.delete(X_train_folds, i,         axis=0).reshape(X_train_folds[i].shape[0],X_train_folds[i].shape[1])</span></span><br><span class="line">        X_train_temp = np.delete(X_train_folds, i, axis=<span class="number">0</span>)</span><br><span class="line">        X_train_cv = np.concatenate(X_train_temp)</span><br><span class="line">        y_train_temp = np.delete(y_train_folds, i, axis=<span class="number">0</span>)</span><br><span class="line">        y_train_cv = np.concatenate(y_train_temp)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        classifier.train(X_train_cv, y_train_cv)</span><br><span class="line">        dist = classifier.compute_distances_no_loops(X_train_folds[i])</span><br><span class="line">        y_test_predictions = classifier.predict_labels(dist, k_value)</span><br><span class="line">        num_corrects = np.sum(y_test_predictions == y_train_folds[i])</span><br><span class="line">        accuracy = float(num_corrects) / y_train_folds[i].shape[<span class="number">0</span>]</span><br><span class="line">        k_to_accuracies[k_value].append(accuracy)</span><br><span class="line"><span class="comment"># pass</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"><span class="comment">#                                 END OF YOUR CODE                             #</span></span><br><span class="line"><span class="comment">################################################################################</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Print out the computed accuracies</span></span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> sorted(k_to_accuracies):</span><br><span class="line">    <span class="keyword">for</span> accuracy <span class="keyword">in</span> k_to_accuracies[k]:</span><br><span class="line">        print(<span class="string">'k = %d, accuracy = %f'</span> % (k, accuracy))</span><br></pre></td></tr></table></figure>
<p>You can draw the graph to see which k is the best. </p>
<p><img src="/2020/02/13/KNN/1581329141634.png" alt="1581329141634"></p>
<p><img src="/2020/02/13/KNN/1581329151487.png" alt="1581329151487"></p>
<p><img src="/2020/02/13/KNN/1581329185444.png" alt="1581329185444"></p>
<p>We can see that 8 is the best value for k and we use it to get the accuracy: 0.274.</p>
<h5 id="Conclusion"><a href="#Conclusion" class="headerlink" title="Conclusion"></a>Conclusion</h5><p>KNN is an easy algorithm to classify different categories just by calculating the distance between testing data and training data. There are three ways to calculae the distance and the no-loops way is the most efficient. But it is not a very effective way to correctly classify different categories. The accuracy is not very high as we can see from the result. </p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Python/" rel="tag"># Python</a>
              <a href="/tags/Machine-Learning/" rel="tag"># Machine Learning</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2020/02/13/Setup/" rel="prev" title="CS231n assignment1 Environment Setup">
      <i class="fa fa-chevron-left"></i> CS231n assignment1 Environment Setup
    </a></div>
      <div class="post-nav-item">
    <a href="/2020/02/14/SVM/" rel="next" title="SVM">
      SVM <i class="fa fa-chevron-right"></i>
    </a></div>
    </div>
      </footer>
    
  </article>
  
  
  

  </div>


          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let activeClass = CONFIG.comments.activeClass;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-4"><a class="nav-link" href="#KNN"><span class="nav-number">1.</span> <span class="nav-text">KNN</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#KNN-Classifier"><span class="nav-number">1.1.</span> <span class="nav-text">KNN Classifier</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Training-and-Testing"><span class="nav-number">1.2.</span> <span class="nav-text">Training and Testing</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#Conclusion"><span class="nav-number">1.3.</span> <span class="nav-text">Conclusion</span></a></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">Bania</p>
  <div class="site-description" itemprop="description">A platform for discussing programming and technology</div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">8</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">2</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2020</span>
  <span class="with-love">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Bania</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> v4.2.0
  </div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">Theme – <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.7.1
  </div>

<div>
<script async src="https://dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<span id="busuanzi_container_site_pv" style='display:none'>
  Total visits: <span id="busuanzi_value_site_pv"></span> times
  <span class="post-meta-divider">|</span>
</span>
<span id="busuanzi_container_site_uv" style='display:none'>
  <span id="busuanzi_value_site_uv"></span>people have viewed my bolg.
</span>
</div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/pangu@4/dist/browser/pangu.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>

<script src="/js/bookmark.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

  

</body>
</html>
